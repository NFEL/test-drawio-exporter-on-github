{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Timechainswap \u00b6 License \u00b6 This project is licensed under the terms of the MIT license.","title":"Timechain"},{"location":"#timechainswap","text":"","title":"Timechainswap"},{"location":"#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"Services/Development-timeline/","text":"Timeline \u00b6 Development process included in developing many services and deployment in-house developed contracts. Steps are much in detail each, but to have an over view of what went on in the past, following diagrams can help you understanding what we achieved so far. Plus there is an small section about future planes and what to expect in development. Aggregator \u00b6 Types of test we used in each step is : Stress Test Load Test Function Test Security Test Stability Test We Three major updates, which is marked by rocket icon.","title":"Timeline"},{"location":"Services/Development-timeline/#timeline","text":"Development process included in developing many services and deployment in-house developed contracts. Steps are much in detail each, but to have an over view of what went on in the past, following diagrams can help you understanding what we achieved so far. Plus there is an small section about future planes and what to expect in development.","title":"Timeline"},{"location":"Services/Development-timeline/#aggregator","text":"Types of test we used in each step is : Stress Test Load Test Function Test Security Test Stability Test We Three major updates, which is marked by rocket icon.","title":"Aggregator"},{"location":"Services/Aggregator/","text":"Aggregator \u00b6","title":"Aggregator"},{"location":"Services/Aggregator/#aggregator","text":"","title":"Aggregator"},{"location":"Services/Aggregator/Version-1/","text":"One 1 \u00b6 Here is basic diagram about what happened in aggregator so far. In this version of aggregator we only had a simple approach. It collected data via a simple script. HLA Diagram \u00b6 How the service Worked. Data Flow Diagram \u00b6 How data flow though service and handled! Extra points \u00b6 Points to Mention : This version had no load balancing or user load The script that fill the db (A simple .json file) was ran manually In file the file we stored: The addresses to uniswap pairs Tokens uniswap had Connection from pairs to tokens Later a crontab task as added","title":"One 1"},{"location":"Services/Aggregator/Version-1/#one-1","text":"Here is basic diagram about what happened in aggregator so far. In this version of aggregator we only had a simple approach. It collected data via a simple script.","title":"One 1"},{"location":"Services/Aggregator/Version-1/#hla-diagram","text":"How the service Worked.","title":"HLA Diagram"},{"location":"Services/Aggregator/Version-1/#data-flow-diagram","text":"How data flow though service and handled!","title":"Data Flow Diagram"},{"location":"Services/Aggregator/Version-1/#extra-points","text":"Points to Mention : This version had no load balancing or user load The script that fill the db (A simple .json file) was ran manually In file the file we stored: The addresses to uniswap pairs Tokens uniswap had Connection from pairs to tokens Later a crontab task as added","title":"Extra points"},{"location":"Services/Aggregator/Version-2/","text":"Two - 2 \u00b6 Here is basic diagram about what happened in aggregator so far. In this version of aggregator we only had a simple approach. It collected data via a simple script. HLA Diagram \u00b6 Setup we used to run services. List of services we used are as followed: Aggregator-api Aggregator-worker Mongo Redis Nginx RPC-node (Only For Backup) RPC-URL Data Flow Diagram \u00b6 How data flow though service and inter app connections are handled! Periodic Tasks Extra points \u00b6 Points to Mention : This version had no load balancing or user load The script that fill the db (A simple .json file) was ran manually In file the file we stored: The addresses to uniswap pairs Tokens uniswap had Connection from pairs to tokens Later a crontab task as added","title":"Two - 2"},{"location":"Services/Aggregator/Version-2/#two-2","text":"Here is basic diagram about what happened in aggregator so far. In this version of aggregator we only had a simple approach. It collected data via a simple script.","title":"Two - 2"},{"location":"Services/Aggregator/Version-2/#hla-diagram","text":"Setup we used to run services. List of services we used are as followed: Aggregator-api Aggregator-worker Mongo Redis Nginx RPC-node (Only For Backup) RPC-URL","title":"HLA Diagram"},{"location":"Services/Aggregator/Version-2/#data-flow-diagram","text":"How data flow though service and inter app connections are handled! Periodic Tasks","title":"Data Flow Diagram"},{"location":"Services/Aggregator/Version-2/#extra-points","text":"Points to Mention : This version had no load balancing or user load The script that fill the db (A simple .json file) was ran manually In file the file we stored: The addresses to uniswap pairs Tokens uniswap had Connection from pairs to tokens Later a crontab task as added","title":"Extra points"},{"location":"Services/Aggregator/BackGroundTasks/","text":"Background Tasks \u00b6 Tasks which are being periodically ran by celery workers.","title":"Background Tasks"},{"location":"Services/Aggregator/BackGroundTasks/#background-tasks","text":"Tasks which are being periodically ran by celery workers.","title":"Background Tasks"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/","text":"Dgraph Related Tasks \u00b6 Tasks which are related to insert and fetch data in dgraph database. We gather data using an address book containing networks and factory addresses of different dexes. Dexes we are currently aggregating: \u00b6 Fantom Network: \u00b6 Curve.fi FrozenYogurtSwap Solidly Beethoven-x SushiSwap SpookySwap PaintSwap WakaSwap SpiritSwap HyperSwap ELKSwap OperaSwap ZooDex Core SoulSwap fBOMB ShibaSwap TimechainSwap DefySwap ProtoFi WrathFi DarkKnightswap JetswapFi MorpheusFinance WigoswapFinance","title":"Dgraph Related Tasks"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/#dgraph-related-tasks","text":"Tasks which are related to insert and fetch data in dgraph database. We gather data using an address book containing networks and factory addresses of different dexes.","title":"Dgraph Related Tasks"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/#dexes-we-are-currently-aggregating","text":"","title":"Dexes we are currently aggregating:"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/#fantom-network","text":"Curve.fi FrozenYogurtSwap Solidly Beethoven-x SushiSwap SpookySwap PaintSwap WakaSwap SpiritSwap HyperSwap ELKSwap OperaSwap ZooDex Core SoulSwap fBOMB ShibaSwap TimechainSwap DefySwap ProtoFi WrathFi DarkKnightswap JetswapFi MorpheusFinance WigoswapFinance","title":"Fantom Network:"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/create_facets/","text":"Create Facets \u00b6 For given pairs and given amount in, calculates amount out for token0 -> token1, token1 -> token0 and so on. Generate Amount In and Edge \u00b6 Takes an input value, From 2 to the power of 0, to, 2 to the power of that value generates amount in which are assumed in FTM price amount (This will be explained in Price section). The input default value is 20. 2 ** 0: p1 2 ** 1: p2 2 ** 1: p3 2 ** 1: p4 2 ** 1: p5 2 ** 1: p6 2 ** 1: p7 2 ** 1: p8 2 ** 1: p9 2 ** 1: p10 2 ** 1: p11 2 ** 1: p12 2 ** 1: p13 2 ** 1: p14 2 ** 1: p15 2 ** 1: p16 2 ** 1: p17 2 ** 1: p18 2 ** 1: p19 def _amount_in_generator ( power ) -> Iterable [ int ]: amounts_in = [] l = list ( range ( 0 , 2 ** power + 1 )) p = int ( math . log ( len ( l ), 2 )) for i in range ( 0 , p + 1 ): amounts_in . append ( l [ 2 ** i ]) return amounts_in def _edge_name ( edge_index : int ) -> str : return f \"p { edge_index } \" Generate Amount Out \u00b6 Calculate Amount Out \u00b6 Finds pair's token addresses using pair.token_addresses and token indexes. Gets token objects from redis using their addresses. Converts amount in to none FTM amount. Calculates amount out of given tokens and obtained amount in. Converts the calculated amount out to FTM amount. def _calculate_amount_out ( chain_id : int , pair : Pair , amount_in : NoDecimal , index1 : int , index2 : int , ): address1 , address2 = pair . token_addresses [ index1 ], pair . token_addresses [ index2 ] token1 = Token . get_cache ( address1 , chain_id ) token2 = Token . get_cache ( address2 , chain_id ) a_in : NoDecimal = convert_from_ftm ( amount_in , token1 ) if a_in is None : return None a_in : WithDecimal = a_in * 10 ** token1 . decimal a_out : WithDecimal = pair . amount_out ( index1 , index2 , a_in ) amount_out : NoDecimal = convert_to_ftm ( a_out / 10 ** token2 . decimal , token2 ) return amount_out Connect Amount to Edges \u00b6 Takes: pair's uid token0's uid token1's uid amount out of token0 -> token1 amount out of token1 -> token0 edge (p0...pn) Sets facets = { \"uid\" : pair_uid , relation : { \"uid\" : token1 , \"pn| {token0} \" : amount_out0 , \"pn| {token1} \" : amount_out1 } }, { \"uid\" : token0 , relation : { \"uid\" : pair_uid , \"pn| {token1} \" : amount_out0 , \"pn| {token0} \" : amount_out1 } } async def _connect_amount_out_to_edge ( pair_uid : Uid , token_0_uid : Uid , token_1_uid : Uid , facets_0 : Dict , facets_1 : Dict , edge : str ): await dgraph_client () . connect_nodes ( uid0 = pair_uid , uid1 = token_1_uid , relation = edge , facets = facets_0 ) logging . info ( f \"pair { pair_uid } conncted to token { token_1_uid } \" ) await dgraph_client () . connect_nodes ( uid0 = token_0_uid , uid1 = pair_uid , relation = edge , facets = facets_1 ) logging . info ( f \"token { token_0_uid } conncted to pair { pair_uid } \"","title":"Create Facets"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/create_facets/#create-facets","text":"For given pairs and given amount in, calculates amount out for token0 -> token1, token1 -> token0 and so on.","title":"Create Facets"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/create_facets/#generate-amount-in-and-edge","text":"Takes an input value, From 2 to the power of 0, to, 2 to the power of that value generates amount in which are assumed in FTM price amount (This will be explained in Price section). The input default value is 20. 2 ** 0: p1 2 ** 1: p2 2 ** 1: p3 2 ** 1: p4 2 ** 1: p5 2 ** 1: p6 2 ** 1: p7 2 ** 1: p8 2 ** 1: p9 2 ** 1: p10 2 ** 1: p11 2 ** 1: p12 2 ** 1: p13 2 ** 1: p14 2 ** 1: p15 2 ** 1: p16 2 ** 1: p17 2 ** 1: p18 2 ** 1: p19 def _amount_in_generator ( power ) -> Iterable [ int ]: amounts_in = [] l = list ( range ( 0 , 2 ** power + 1 )) p = int ( math . log ( len ( l ), 2 )) for i in range ( 0 , p + 1 ): amounts_in . append ( l [ 2 ** i ]) return amounts_in def _edge_name ( edge_index : int ) -> str : return f \"p { edge_index } \"","title":"Generate Amount In and Edge"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/create_facets/#generate-amount-out","text":"","title":"Generate Amount Out"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/create_facets/#calculate-amount-out","text":"Finds pair's token addresses using pair.token_addresses and token indexes. Gets token objects from redis using their addresses. Converts amount in to none FTM amount. Calculates amount out of given tokens and obtained amount in. Converts the calculated amount out to FTM amount. def _calculate_amount_out ( chain_id : int , pair : Pair , amount_in : NoDecimal , index1 : int , index2 : int , ): address1 , address2 = pair . token_addresses [ index1 ], pair . token_addresses [ index2 ] token1 = Token . get_cache ( address1 , chain_id ) token2 = Token . get_cache ( address2 , chain_id ) a_in : NoDecimal = convert_from_ftm ( amount_in , token1 ) if a_in is None : return None a_in : WithDecimal = a_in * 10 ** token1 . decimal a_out : WithDecimal = pair . amount_out ( index1 , index2 , a_in ) amount_out : NoDecimal = convert_to_ftm ( a_out / 10 ** token2 . decimal , token2 ) return amount_out","title":"Calculate Amount Out"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/create_facets/#connect-amount-to-edges","text":"Takes: pair's uid token0's uid token1's uid amount out of token0 -> token1 amount out of token1 -> token0 edge (p0...pn) Sets facets = { \"uid\" : pair_uid , relation : { \"uid\" : token1 , \"pn| {token0} \" : amount_out0 , \"pn| {token1} \" : amount_out1 } }, { \"uid\" : token0 , relation : { \"uid\" : pair_uid , \"pn| {token1} \" : amount_out0 , \"pn| {token0} \" : amount_out1 } } async def _connect_amount_out_to_edge ( pair_uid : Uid , token_0_uid : Uid , token_1_uid : Uid , facets_0 : Dict , facets_1 : Dict , edge : str ): await dgraph_client () . connect_nodes ( uid0 = pair_uid , uid1 = token_1_uid , relation = edge , facets = facets_0 ) logging . info ( f \"pair { pair_uid } conncted to token { token_1_uid } \" ) await dgraph_client () . connect_nodes ( uid0 = token_0_uid , uid1 = pair_uid , relation = edge , facets = facets_1 ) logging . info ( f \"token { token_0_uid } conncted to pair { pair_uid } \"","title":"Connect Amount to Edges"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_facets/","text":"Filter Facets \u00b6 After setting facets (amount out) on pairs and tokens, we need to check if there is a facet with 0 value. Queries dgraph and finds all of facets. {query(func: type(Token)){ p0 @facets { uid } p1 @facets { uid } p2 @facets { uid } p3 @facets { uid } p4 @facets { uid } p5 @facets { uid } p6 @facets { uid } p7 @facets { uid } p8 @facets { uid } p9 @facets { uid } p10 @facets { uid } p11 @facets { uid } p12 @facets { uid } p13 @facets { uid } p14 @facets { uid } p15 @facets { uid } p16 @facets { uid } p17 @facets { uid } p18 @facets { uid } p19 @facets { uid } } } Checks if any facet has value of 0 and if found any, deletes it. async def delete_facets_with_zero_value (): token_facets = await dgraph_client () . find_all_facets ( \"Token\" ) if token_facets is not None and len ( token_facets ) > 0 : uids = set () for token_facet in token_facets : for facet in token_facet . values (): for p in facet : for value in p . values (): if value == 0 : for key in p . keys (): if \"p\" in key . split ( \"|\" )[ 0 ]: uids . add ( key . split ( \"|\" )[ 1 ]) deleted_facets = await delete_facets ( list ( uids )) logging . info ( f \" { len ( deleted_facets ) } facets with 0 value was deleted.\" ) async def delete_facets ( uids : List [ Uid ]): deleted_facets = [] for uid in uids : try : await dgraph_client () . delete ( uid , [ \"@facets\" ] ) deleted_facets . append ( uid ) except Exception as e : logging . exception ( f \"Couldn't delete facet on token: { uid } , { e } \" ) return deleted_facets","title":"Filter Facets"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_facets/#filter-facets","text":"After setting facets (amount out) on pairs and tokens, we need to check if there is a facet with 0 value. Queries dgraph and finds all of facets. {query(func: type(Token)){ p0 @facets { uid } p1 @facets { uid } p2 @facets { uid } p3 @facets { uid } p4 @facets { uid } p5 @facets { uid } p6 @facets { uid } p7 @facets { uid } p8 @facets { uid } p9 @facets { uid } p10 @facets { uid } p11 @facets { uid } p12 @facets { uid } p13 @facets { uid } p14 @facets { uid } p15 @facets { uid } p16 @facets { uid } p17 @facets { uid } p18 @facets { uid } p19 @facets { uid } } } Checks if any facet has value of 0 and if found any, deletes it. async def delete_facets_with_zero_value (): token_facets = await dgraph_client () . find_all_facets ( \"Token\" ) if token_facets is not None and len ( token_facets ) > 0 : uids = set () for token_facet in token_facets : for facet in token_facet . values (): for p in facet : for value in p . values (): if value == 0 : for key in p . keys (): if \"p\" in key . split ( \"|\" )[ 0 ]: uids . add ( key . split ( \"|\" )[ 1 ]) deleted_facets = await delete_facets ( list ( uids )) logging . info ( f \" { len ( deleted_facets ) } facets with 0 value was deleted.\" ) async def delete_facets ( uids : List [ Uid ]): deleted_facets = [] for uid in uids : try : await dgraph_client () . delete ( uid , [ \"@facets\" ] ) deleted_facets . append ( uid ) except Exception as e : logging . exception ( f \"Couldn't delete facet on token: { uid } , { e } \" ) return deleted_facets","title":"Filter Facets"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_pairs/","text":"Filter Pairs \u00b6 Delete low reserve pairs \u00b6 Some pairs have low reserves therefor they have no use for swaps. 1. Queries dgraph and gets all pairs (uid, reserves, decimals, tokens {uid}). async def get_all_pairs ( chain_id : ChainId ): try : all_pairs = await dgraph_client () . find_by_chain_id ( chain_id , \"Pair\" , [ \"uid\" , \"reserves\" , \"decimals\" , \"tokens { uid }\" ] ) return all_pairs . get ( \"~chain\" ) except Exception as e : logging . error ( f \"Expection { e } occured during get_all_pairs.\" ) return None Checks if pairs don't have any of needed predicates and if found any, deletes it. async def delete_pair_with_no_predicate ( pair : Dict ): if None in ( pair . get ( \"reserves\" ), pair . get ( \"decimals\" ), pair . get ( \"tokens\" )): logging . critical ( f \"pair { pair . get ( 'uid' ) } doesn't have reserve.\" ) await dgraph_client () . delete ( pair . get ( \"uid\" )) return False return True For all of pairs, divides pair's reserves into the 10 ** pair's decimals. Creates a list of dictionaries containing each pair's uid, reserves (reserve / 10 ** decimal) and tokens (uid of tokens that are connected to pair). async def split_things ( chain_id : ChainId ): pairs = await get_all_pairs ( chain_id ) if not pairs : logging . error ( \"No pair exist at split_things.\" ) made_pairs = [] for pair in pairs : if await delete_pair_with_no_predicate ( pair ): token_uids = [] decimals = [ int ( decimal ) for decimal in pair . get ( \"decimals\" ) . split ( \"#\" )] pair_reserves = [( int ( reserve ) / 10 ** decimals [ i ]) for i , reserve in enumerate ( pair . get ( \"reserves\" ) . split ( \"#\" ))] for token in pair . get ( \"tokens\" ): token_uids . append ( token . get ( \"uid\" )) made_pairs . append ({ \"uid\" : pair . get ( \"uid\" ), \"reserves\" : pair_reserves , \"tokens\" : token_uids }) return made_pairs For all of pairs (list of dictionaries), checks if pair's reserve is less than 5 and if yes, deletes it. async def delete_low_reserve_pairs ( pair : Dict ): low_reserve = False for reserve in pair . get ( \"reserves\" ): if reserve < 5 : low_reserve = True if low_reserve : await dgraph_client () . delete ( pair . get ( \"uid\" )) logging . info ( f \"deleted pair { pair . get ( 'uid' ) } for having low reserves at delete_low_reserve_pairs.\" ) return True For all of that pair's tokens, queries dgraph and finds the uid of pairs which are connected to that token. Removes the connection between token and all of it's pairs. Connect token to all of it's pairs except that one pair with low reserves. async def filter_pairs_of_tokens ( token_uid : Uid , pair_uid : Uid ): token_pairs = await dgraph_client () . find_by_uid ( token_uid , [ \"pairs { uid }\" ] ) if token_pairs is None : logging . warning ( f \"Found a pair { pair_uid } with on tokens at filter_pairs_of_tokens.\" ) else : _pairs = [] for token_pair in token_pairs . get ( \"pairs\" ): if token_pair . get ( \"uid\" ) != pair_uid : _pairs . append ({ \"uid\" : token_pair . get ( \"uid\" ), \"dgraph.type\" : dt . PAIR }) await dgraph_client () . delete ( token_uid , [ \"pairs\" ]) logging . info ( f \"deleted all of token { token_uid } pairs at filter_pairs_of_tokens.\" ) await dgraph_client () . insert ({ \"uid\" : token_uid , \"pairs\" : _pairs , \"dgraph.type\" : dt . TOKEN }) logging . info ( f \"inserted pairs of token { token_uid } to dgraph at filter_pairs_of_tokens.\" ) Check out the whole thing. async def filter_pairs ( chain_id : ChainId ): pairs = await split_things ( chain_id ) if len ( pairs ) < 1 : return None for pair in pairs : if await delete_low_reserve_pairs ( pair ): if pair . get ( \"tokens\" ) is not None : for token in pair . get ( \"tokens\" ): await filter_pairs_of_tokens ( token , pair . get ( \"uid\" )) logging . info ( \"All of pairs with low reserves are gone.\" ) Delete pairs with no token \u00b6 After setting price predicate to tokens, some tokens might end up with no price (because of not having any common pairs with base tokens), therefore they have no use for us and we cannot calculate output amount for them, so we delete them. after deleting these tokens we need to check all pairs and delete those with no tokens. 1. Queries dgraph and get tokens of all pairs pairs = await dgraph_client () . find_by_chain_id ( chain_id , \"Pair\" , [ \"uid\" , \"tokens { uid }\" ] ) pairs = pairs . get ( \"~chain\" ) 2. Checks if pairs have tokens for pair in pairs : if pair is None : return None if pair . get ( \"tokens\" ) is None : await dgraph_client () . delete ( pair . get ( \"uid\" ))","title":"Filter Pairs"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_pairs/#filter-pairs","text":"","title":"Filter Pairs"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_pairs/#delete-low-reserve-pairs","text":"Some pairs have low reserves therefor they have no use for swaps. 1. Queries dgraph and gets all pairs (uid, reserves, decimals, tokens {uid}). async def get_all_pairs ( chain_id : ChainId ): try : all_pairs = await dgraph_client () . find_by_chain_id ( chain_id , \"Pair\" , [ \"uid\" , \"reserves\" , \"decimals\" , \"tokens { uid }\" ] ) return all_pairs . get ( \"~chain\" ) except Exception as e : logging . error ( f \"Expection { e } occured during get_all_pairs.\" ) return None Checks if pairs don't have any of needed predicates and if found any, deletes it. async def delete_pair_with_no_predicate ( pair : Dict ): if None in ( pair . get ( \"reserves\" ), pair . get ( \"decimals\" ), pair . get ( \"tokens\" )): logging . critical ( f \"pair { pair . get ( 'uid' ) } doesn't have reserve.\" ) await dgraph_client () . delete ( pair . get ( \"uid\" )) return False return True For all of pairs, divides pair's reserves into the 10 ** pair's decimals. Creates a list of dictionaries containing each pair's uid, reserves (reserve / 10 ** decimal) and tokens (uid of tokens that are connected to pair). async def split_things ( chain_id : ChainId ): pairs = await get_all_pairs ( chain_id ) if not pairs : logging . error ( \"No pair exist at split_things.\" ) made_pairs = [] for pair in pairs : if await delete_pair_with_no_predicate ( pair ): token_uids = [] decimals = [ int ( decimal ) for decimal in pair . get ( \"decimals\" ) . split ( \"#\" )] pair_reserves = [( int ( reserve ) / 10 ** decimals [ i ]) for i , reserve in enumerate ( pair . get ( \"reserves\" ) . split ( \"#\" ))] for token in pair . get ( \"tokens\" ): token_uids . append ( token . get ( \"uid\" )) made_pairs . append ({ \"uid\" : pair . get ( \"uid\" ), \"reserves\" : pair_reserves , \"tokens\" : token_uids }) return made_pairs For all of pairs (list of dictionaries), checks if pair's reserve is less than 5 and if yes, deletes it. async def delete_low_reserve_pairs ( pair : Dict ): low_reserve = False for reserve in pair . get ( \"reserves\" ): if reserve < 5 : low_reserve = True if low_reserve : await dgraph_client () . delete ( pair . get ( \"uid\" )) logging . info ( f \"deleted pair { pair . get ( 'uid' ) } for having low reserves at delete_low_reserve_pairs.\" ) return True For all of that pair's tokens, queries dgraph and finds the uid of pairs which are connected to that token. Removes the connection between token and all of it's pairs. Connect token to all of it's pairs except that one pair with low reserves. async def filter_pairs_of_tokens ( token_uid : Uid , pair_uid : Uid ): token_pairs = await dgraph_client () . find_by_uid ( token_uid , [ \"pairs { uid }\" ] ) if token_pairs is None : logging . warning ( f \"Found a pair { pair_uid } with on tokens at filter_pairs_of_tokens.\" ) else : _pairs = [] for token_pair in token_pairs . get ( \"pairs\" ): if token_pair . get ( \"uid\" ) != pair_uid : _pairs . append ({ \"uid\" : token_pair . get ( \"uid\" ), \"dgraph.type\" : dt . PAIR }) await dgraph_client () . delete ( token_uid , [ \"pairs\" ]) logging . info ( f \"deleted all of token { token_uid } pairs at filter_pairs_of_tokens.\" ) await dgraph_client () . insert ({ \"uid\" : token_uid , \"pairs\" : _pairs , \"dgraph.type\" : dt . TOKEN }) logging . info ( f \"inserted pairs of token { token_uid } to dgraph at filter_pairs_of_tokens.\" ) Check out the whole thing. async def filter_pairs ( chain_id : ChainId ): pairs = await split_things ( chain_id ) if len ( pairs ) < 1 : return None for pair in pairs : if await delete_low_reserve_pairs ( pair ): if pair . get ( \"tokens\" ) is not None : for token in pair . get ( \"tokens\" ): await filter_pairs_of_tokens ( token , pair . get ( \"uid\" )) logging . info ( \"All of pairs with low reserves are gone.\" )","title":"Delete low reserve pairs"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_pairs/#delete-pairs-with-no-token","text":"After setting price predicate to tokens, some tokens might end up with no price (because of not having any common pairs with base tokens), therefore they have no use for us and we cannot calculate output amount for them, so we delete them. after deleting these tokens we need to check all pairs and delete those with no tokens. 1. Queries dgraph and get tokens of all pairs pairs = await dgraph_client () . find_by_chain_id ( chain_id , \"Pair\" , [ \"uid\" , \"tokens { uid }\" ] ) pairs = pairs . get ( \"~chain\" ) 2. Checks if pairs have tokens for pair in pairs : if pair is None : return None if pair . get ( \"tokens\" ) is None : await dgraph_client () . delete ( pair . get ( \"uid\" ))","title":"Delete pairs with no token"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_tokens/","text":"Filter Tokens \u00b6 Delete tokens with no price \u00b6 After setting price predicate to tokens, some tokens might end up with no price (because of not having any common pairs with base tokens), therefore they have no use for us and we cannot calculate output amount for them, so we delete them. 1. Queries dgraph and gets price of all tokens. tokens = await dgraph_client () . find_by_chain_id ( chain_id , \"Token\" , [ \"uid\" , \"price\" ] ) tokens = tokens . get ( \"~chain\" ) Checks if tokens have price for token in tokens : if token . get ( \"price\" ) in ( None , 0 ): await dgraph_client () . delete ( token . get ( \"uid\" )) Delete tokens with no pair \u00b6 After deleting low reserve tokens, some tokens might end up with no pairs, therefore we cannot find route for them and they have no use for us. 1. Queries dgraph and gets pairs of all tokens. tokens = await dgraph_client () . find_by_chain_id ( chain_id , \"Token\" , [ \"uid\" , \"pairs { uid }\" ] ) tokens = tokens . get ( \"~chain\" ) 2. Checks if tokens have pair for token in tokens : if token is None : return None if token . get ( \"pairs\" ) is None : await dgraph_client () . delete ( token . get ( \"uid\" )) 3. Check if token is really gone dgraph_type = await dgraph_client () . find_by_uid ( token . get ( \"uid\" ), [ \"dgraph.type\" ]) if dgraph_type : if dgraph_type . get ( \"dgraph.type\" ): logging . info ( \"The delete function doesn't work correctly!\" )","title":"Filter Tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_tokens/#filter-tokens","text":"","title":"Filter Tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_tokens/#delete-tokens-with-no-price","text":"After setting price predicate to tokens, some tokens might end up with no price (because of not having any common pairs with base tokens), therefore they have no use for us and we cannot calculate output amount for them, so we delete them. 1. Queries dgraph and gets price of all tokens. tokens = await dgraph_client () . find_by_chain_id ( chain_id , \"Token\" , [ \"uid\" , \"price\" ] ) tokens = tokens . get ( \"~chain\" ) Checks if tokens have price for token in tokens : if token . get ( \"price\" ) in ( None , 0 ): await dgraph_client () . delete ( token . get ( \"uid\" ))","title":"Delete tokens with no price"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/filter_tokens/#delete-tokens-with-no-pair","text":"After deleting low reserve tokens, some tokens might end up with no pairs, therefore we cannot find route for them and they have no use for us. 1. Queries dgraph and gets pairs of all tokens. tokens = await dgraph_client () . find_by_chain_id ( chain_id , \"Token\" , [ \"uid\" , \"pairs { uid }\" ] ) tokens = tokens . get ( \"~chain\" ) 2. Checks if tokens have pair for token in tokens : if token is None : return None if token . get ( \"pairs\" ) is None : await dgraph_client () . delete ( token . get ( \"uid\" )) 3. Check if token is really gone dgraph_type = await dgraph_client () . find_by_uid ( token . get ( \"uid\" ), [ \"dgraph.type\" ]) if dgraph_type : if dgraph_type . get ( \"dgraph.type\" ): logging . info ( \"The delete function doesn't work correctly!\" )","title":"Delete tokens with no pair"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_chains/","text":"Chain \u00b6 Inserting chains into database is done by two methods of Chain class. save_chain converts the Chain object to a dictionary that can be inserted in database. def save_chain ( self ): obj = { \"uid\" : self . uid , \"chain_id\" : self . chain_id . value , \"dgraph.type\" : dt . CHAIN } return obj check_if_chain_is_token_or_dex_or_pair Queries dgraph and checks if chain's uid already exist in database and if it exist schema type it has. async def check_if_chain_is_token_or_dex_or_pair ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type != \"Chain\" : logging . info ( f \"chain uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True","title":"Chain"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_chains/#chain","text":"Inserting chains into database is done by two methods of Chain class. save_chain converts the Chain object to a dictionary that can be inserted in database. def save_chain ( self ): obj = { \"uid\" : self . uid , \"chain_id\" : self . chain_id . value , \"dgraph.type\" : dt . CHAIN } return obj check_if_chain_is_token_or_dex_or_pair Queries dgraph and checks if chain's uid already exist in database and if it exist schema type it has. async def check_if_chain_is_token_or_dex_or_pair ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type != \"Chain\" : logging . info ( f \"chain uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True","title":"Chain"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/","text":"Insert Dexes \u00b6 In this module, we are trying to get the dex details from blockchain, and create Dex object based on those details. Only thing left here is to save them on dgraph (DB of choice). Reads address book data and finds factories on specified chain. Uses DexProtocol 's factory_class method to create Factory object for each protocol. Uses dex check_if_dex_is_token_or_pair method to check if dex already exists in database and if yes, what schema type it has. Uses dex save_dex method to convert dex object into a dictionary that can be inserted to database. Inserts dex dictionary to dgraph. Read address book data \u00b6 address book \u00b6 For each chain, we store details of dexes in an JSON object like so. { \"250\" : { \"Solidly\" : { \"name\" : \"Solidly\" , \"router\" : \"0xa38cd27185a464914D3046f0AB9d43356B34829D\" , \"factory\" : \"0x3fAaB499b519fdC5819e3D7ed0C26111904cbc28\" , \"type\" : 12 } } } read_data parses the address book and returns a dictionary containing chain_id , protocol , name , factory and router . def read_data ( chain_id : int , address_book_directory = 'Utils/address_book.json' ) -> List [ RawDex ]: chain_id = str ( chain_id ) dexs = [] parsed_data = parse_dex_file ( address_book_directory ) for dex in parsed_data . get ( chain_id ) . values (): dex_obj = { \"chain_id\" : chain_id , \"protocol\" : str ( dex . get ( \"type\" )), \"name\" : dex . get ( \"name\" ), \"factory\" : dex . get ( \"factory\" ), \"router\" : dex . get ( \"router\" ) } dexs . append ( dex_obj ) return dexs @lru_cache ( maxsize = 10000 ) def parse_dex_file ( address_book_directory ): with open ( address_book_directory ) as f : dexs = json . load ( f ) f . close () return dexs DexProtocol \u00b6 an Enum class which specifies the Dex Factory class that must be used. class DexProtocol ( Enum ): UNISWAP = '1' MDEX = '2' MOONSWAP = '3' SPARTAN = '4' ELLIPSIS = '5' Vpeg = '6' AcryptoS = '7' DODOV2 = '8' DODOV1 = '9' SOULSWAP = '10' BEETHOVEN = '11' SOLIDLY = '12' Curve = '13' FrozenYougert = '14' @classmethod def _class_map ( cls : DexProtocol ) -> List [ Dict ]: ''' selecting the factory class ''' return { cls . UNISWAP : UniSwapFactory , cls . MDEX : MDEXFactory , cls . SOLIDLY : SolidlyFactory , cls . BEETHOVEN : BeethovenFactory , cls . SPARTAN : SpartanSwapFactory , cls . MOONSWAP : MoonSwapFactory , cls . ELLIPSIS : EllipsisFactory , cls . AcryptoS : AcryptoSFactory , cls . DODOV2 : DoDoV2Factory , cls . DODOV1 : DoDoV1Factory , cls . SOULSWAP : SoulSwapFactory , cls . Vpeg : VpegFactory , cls . FrozenYougert : FrozenYogurtFactory , cls . Curve : CurveFactory } def _class_selector ( self ): ''' get pair list ''' return DexProtocol . _class_map ()[ self ] @property def factory_class ( self ): return self . _class_selector () Dex class \u00b6 check_if_dex_is_token_or_pair query dgraph and tries to find a node with dex'es uid, if found one, checks if it's schema type (dgraph.type) is equal to \"Dex\" or not. async def check_if_dex_is_token_or_pair ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type == \"Token\" or d_type == \"Pair\" : logging . info ( f \"dex uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True save_dex converts the Dex object (Factory object for each protocol) to a dictionary. connects \"Dex\" to \"Chain\" using \"chain\" edge. def save_dex ( self ): chain_id = self . chain_id . value obj = self . dict () obj [ \"uid\" ] = self . uid obj [ \"dgraph.type\" ] = dt . DEX obj [ \"chain\" ] = { \"uid\" : self . get_uid ( chain_id , chain_id ), \"chain_id\" : chain_id , \"dgraph.type\" : dt . CHAIN } obj . pop ( \"chain_id\" ) return obj Insert dexes \u00b6 insert_dexes async def insert_dexes ( chain_id , dexes : List [ RawDex ] = None ): if dexes is None : dexes = read_data ( chain_id ) for dex in dexes : try : factory = DexProtocol ( dex . get ( \"protocol\" ) ) . factory_class ( ** dex ) if await factory . check_if_dex_is_token_or_pair (): continue factory = factory . save_dex () await dgraph_client () . insert ( factory ) logging . info ( f \"factory { factory . get ( 'uid' ) } added.\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at fetch_dexes function. dex: { dex } ' ) logging . info ( f \"protocol { dex . get ( 'protocol' ) } does not exist\" )","title":"Insert Dexes"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/#insert-dexes","text":"In this module, we are trying to get the dex details from blockchain, and create Dex object based on those details. Only thing left here is to save them on dgraph (DB of choice). Reads address book data and finds factories on specified chain. Uses DexProtocol 's factory_class method to create Factory object for each protocol. Uses dex check_if_dex_is_token_or_pair method to check if dex already exists in database and if yes, what schema type it has. Uses dex save_dex method to convert dex object into a dictionary that can be inserted to database. Inserts dex dictionary to dgraph.","title":"Insert Dexes"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/#read-address-book-data","text":"","title":"Read address book data"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/#address-book","text":"For each chain, we store details of dexes in an JSON object like so. { \"250\" : { \"Solidly\" : { \"name\" : \"Solidly\" , \"router\" : \"0xa38cd27185a464914D3046f0AB9d43356B34829D\" , \"factory\" : \"0x3fAaB499b519fdC5819e3D7ed0C26111904cbc28\" , \"type\" : 12 } } } read_data parses the address book and returns a dictionary containing chain_id , protocol , name , factory and router . def read_data ( chain_id : int , address_book_directory = 'Utils/address_book.json' ) -> List [ RawDex ]: chain_id = str ( chain_id ) dexs = [] parsed_data = parse_dex_file ( address_book_directory ) for dex in parsed_data . get ( chain_id ) . values (): dex_obj = { \"chain_id\" : chain_id , \"protocol\" : str ( dex . get ( \"type\" )), \"name\" : dex . get ( \"name\" ), \"factory\" : dex . get ( \"factory\" ), \"router\" : dex . get ( \"router\" ) } dexs . append ( dex_obj ) return dexs @lru_cache ( maxsize = 10000 ) def parse_dex_file ( address_book_directory ): with open ( address_book_directory ) as f : dexs = json . load ( f ) f . close () return dexs","title":"address book"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/#dexprotocol","text":"an Enum class which specifies the Dex Factory class that must be used. class DexProtocol ( Enum ): UNISWAP = '1' MDEX = '2' MOONSWAP = '3' SPARTAN = '4' ELLIPSIS = '5' Vpeg = '6' AcryptoS = '7' DODOV2 = '8' DODOV1 = '9' SOULSWAP = '10' BEETHOVEN = '11' SOLIDLY = '12' Curve = '13' FrozenYougert = '14' @classmethod def _class_map ( cls : DexProtocol ) -> List [ Dict ]: ''' selecting the factory class ''' return { cls . UNISWAP : UniSwapFactory , cls . MDEX : MDEXFactory , cls . SOLIDLY : SolidlyFactory , cls . BEETHOVEN : BeethovenFactory , cls . SPARTAN : SpartanSwapFactory , cls . MOONSWAP : MoonSwapFactory , cls . ELLIPSIS : EllipsisFactory , cls . AcryptoS : AcryptoSFactory , cls . DODOV2 : DoDoV2Factory , cls . DODOV1 : DoDoV1Factory , cls . SOULSWAP : SoulSwapFactory , cls . Vpeg : VpegFactory , cls . FrozenYougert : FrozenYogurtFactory , cls . Curve : CurveFactory } def _class_selector ( self ): ''' get pair list ''' return DexProtocol . _class_map ()[ self ] @property def factory_class ( self ): return self . _class_selector ()","title":"DexProtocol"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/#dex-class","text":"check_if_dex_is_token_or_pair query dgraph and tries to find a node with dex'es uid, if found one, checks if it's schema type (dgraph.type) is equal to \"Dex\" or not. async def check_if_dex_is_token_or_pair ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type == \"Token\" or d_type == \"Pair\" : logging . info ( f \"dex uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True save_dex converts the Dex object (Factory object for each protocol) to a dictionary. connects \"Dex\" to \"Chain\" using \"chain\" edge. def save_dex ( self ): chain_id = self . chain_id . value obj = self . dict () obj [ \"uid\" ] = self . uid obj [ \"dgraph.type\" ] = dt . DEX obj [ \"chain\" ] = { \"uid\" : self . get_uid ( chain_id , chain_id ), \"chain_id\" : chain_id , \"dgraph.type\" : dt . CHAIN } obj . pop ( \"chain_id\" ) return obj","title":"Dex class"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_dexes/#insert-dexes_1","text":"insert_dexes async def insert_dexes ( chain_id , dexes : List [ RawDex ] = None ): if dexes is None : dexes = read_data ( chain_id ) for dex in dexes : try : factory = DexProtocol ( dex . get ( \"protocol\" ) ) . factory_class ( ** dex ) if await factory . check_if_dex_is_token_or_pair (): continue factory = factory . save_dex () await dgraph_client () . insert ( factory ) logging . info ( f \"factory { factory . get ( 'uid' ) } added.\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at fetch_dexes function. dex: { dex } ' ) logging . info ( f \"protocol { dex . get ( 'protocol' ) } does not exist\" )","title":"Insert dexes"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/","text":"Insert Pairs \u00b6 In this module, we are trying to get the pair details from blockchain, and create Pair object based on those details. Only thing left here is to save them on dgraph (DB of choice). Steps to achive this: Reads address book data and finds factories on specified chain. Uses DexProtocol 's factory_class method to create Factory object for each protocol. Uses retrieve_pair method in dex factory class to create Pair objects. Uses dex check_if_pair_is_token_or_dex method in dex factory to check if pair already exists in database and if yes, what schema type it has. Uses dex save_dex_pair method in pair class to convert pair object into a dictionary that can be inserted to database. Inserts pair dictionary to dgraph. Read address book data \u00b6 address book \u00b6 For each chain, we store details of dexes in an JSON object like so. { \"250\" : { \"Solidly\" : { \"name\" : \"Solidly\" , \"router\" : \"0xa38cd27185a464914D3046f0AB9d43356B34829D\" , \"factory\" : \"0x3fAaB499b519fdC5819e3D7ed0C26111904cbc28\" , \"type\" : 12 } } } read_data Parses the address book and returns a dictionary containing chain_id , protocol , name , factory and router . def read_data ( chain_id : int , address_book_directory = 'Utils/address_book.json' ) -> List [ RawDex ]: chain_id = str ( chain_id ) dexs = [] parsed_data = parse_dex_file ( address_book_directory ) for dex in parsed_data . get ( chain_id ) . values (): dex_obj = { \"chain_id\" : chain_id , \"protocol\" : str ( dex . get ( \"type\" )), \"name\" : dex . get ( \"name\" ), \"factory\" : dex . get ( \"factory\" ), \"router\" : dex . get ( \"router\" ) } dexs . append ( dex_obj ) return dexs @lru_cache ( maxsize = 10000 ) def parse_dex_file ( address_book_directory ): with open ( address_book_directory ) as f : dexs = json . load ( f ) f . close () return dexs DexProtocol \u00b6 an Enum class which specifies the Dex Factory class that must be used. class DexProtocol ( Enum ): UNISWAP = '1' MDEX = '2' MOONSWAP = '3' SPARTAN = '4' ELLIPSIS = '5' Vpeg = '6' AcryptoS = '7' DODOV2 = '8' DODOV1 = '9' SOULSWAP = '10' BEETHOVEN = '11' SOLIDLY = '12' Curve = '13' FrozenYougert = '14' @classmethod def _class_map ( cls : DexProtocol ) -> List [ Dict ]: ''' selecting the factory class ''' return { cls . UNISWAP : UniSwapFactory , cls . MDEX : MDEXFactory , cls . SOLIDLY : SolidlyFactory , cls . BEETHOVEN : BeethovenFactory , cls . SPARTAN : SpartanSwapFactory , cls . MOONSWAP : MoonSwapFactory , cls . ELLIPSIS : EllipsisFactory , cls . AcryptoS : AcryptoSFactory , cls . DODOV2 : DoDoV2Factory , cls . DODOV1 : DoDoV1Factory , cls . SOULSWAP : SoulSwapFactory , cls . Vpeg : VpegFactory , cls . FrozenYougert : FrozenYogurtFactory , cls . Curve : CurveFactory } def _class_selector ( self ): ''' get pair list ''' return DexProtocol . _class_map ()[ self ] @property def factory_class ( self ): return self . _class_selector () Dex class \u00b6 check_if_decheck_if_pair_is_token_or_dex Queries dgraph and tries to find a node with pair's uid, if found one, checks if it's schema type (dgraph.type) is equal to \"Pair\" or not. async def check_if_pair_is_token_or_dex ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type != \"Pair\" : logging . info ( f \"pair uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True retrieve_pair Uses factory to find all of it's pairs and make Pair objects. will be explained in Protocol section. Pair calss \u00b6 save_dex_pair Converts Pair object to a dictionary that can be inserted to dgraph. will be explained in Protocol section. ## Insert pairs insert_pairs async def insert_pairs ( chain_id ): dexs = read_data ( chain_id ) return await save_all_pairs ( dexs ) async def save_all_pairs ( dexs ): return await asyncio . gather ( * [ asyncio . ensure_future ( _retrive_dex_pairs ( dex )) for dex in dexs ], return_exceptions = True ) async def _retrive_dex_pairs ( dex ): try : logging . info ( f \"Adding Dex { dex . get ( 'name' ) } \" ) for pair in DexProtocol ( dex . get ( \"protocol\" ) ) . factory_class ( ** dex ) . retrive_pair (): try : if await pair . check_if_pair_is_token_or_dex (): continue made_pair = pair . save_dex_pair () await dgraph_client () . insert ( made_pair ) logging . info ( f \"pair: { pair . uid } added\" ) print ( f \"pair: { pair . uid } added\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at save_all_pairs. Pair: { pair } ' ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at save_all_pairs. Dex: { dex } .' )","title":"Insert Pairs"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/#insert-pairs","text":"In this module, we are trying to get the pair details from blockchain, and create Pair object based on those details. Only thing left here is to save them on dgraph (DB of choice). Steps to achive this: Reads address book data and finds factories on specified chain. Uses DexProtocol 's factory_class method to create Factory object for each protocol. Uses retrieve_pair method in dex factory class to create Pair objects. Uses dex check_if_pair_is_token_or_dex method in dex factory to check if pair already exists in database and if yes, what schema type it has. Uses dex save_dex_pair method in pair class to convert pair object into a dictionary that can be inserted to database. Inserts pair dictionary to dgraph.","title":"Insert Pairs"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/#read-address-book-data","text":"","title":"Read address book data"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/#address-book","text":"For each chain, we store details of dexes in an JSON object like so. { \"250\" : { \"Solidly\" : { \"name\" : \"Solidly\" , \"router\" : \"0xa38cd27185a464914D3046f0AB9d43356B34829D\" , \"factory\" : \"0x3fAaB499b519fdC5819e3D7ed0C26111904cbc28\" , \"type\" : 12 } } } read_data Parses the address book and returns a dictionary containing chain_id , protocol , name , factory and router . def read_data ( chain_id : int , address_book_directory = 'Utils/address_book.json' ) -> List [ RawDex ]: chain_id = str ( chain_id ) dexs = [] parsed_data = parse_dex_file ( address_book_directory ) for dex in parsed_data . get ( chain_id ) . values (): dex_obj = { \"chain_id\" : chain_id , \"protocol\" : str ( dex . get ( \"type\" )), \"name\" : dex . get ( \"name\" ), \"factory\" : dex . get ( \"factory\" ), \"router\" : dex . get ( \"router\" ) } dexs . append ( dex_obj ) return dexs @lru_cache ( maxsize = 10000 ) def parse_dex_file ( address_book_directory ): with open ( address_book_directory ) as f : dexs = json . load ( f ) f . close () return dexs","title":"address book"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/#dexprotocol","text":"an Enum class which specifies the Dex Factory class that must be used. class DexProtocol ( Enum ): UNISWAP = '1' MDEX = '2' MOONSWAP = '3' SPARTAN = '4' ELLIPSIS = '5' Vpeg = '6' AcryptoS = '7' DODOV2 = '8' DODOV1 = '9' SOULSWAP = '10' BEETHOVEN = '11' SOLIDLY = '12' Curve = '13' FrozenYougert = '14' @classmethod def _class_map ( cls : DexProtocol ) -> List [ Dict ]: ''' selecting the factory class ''' return { cls . UNISWAP : UniSwapFactory , cls . MDEX : MDEXFactory , cls . SOLIDLY : SolidlyFactory , cls . BEETHOVEN : BeethovenFactory , cls . SPARTAN : SpartanSwapFactory , cls . MOONSWAP : MoonSwapFactory , cls . ELLIPSIS : EllipsisFactory , cls . AcryptoS : AcryptoSFactory , cls . DODOV2 : DoDoV2Factory , cls . DODOV1 : DoDoV1Factory , cls . SOULSWAP : SoulSwapFactory , cls . Vpeg : VpegFactory , cls . FrozenYougert : FrozenYogurtFactory , cls . Curve : CurveFactory } def _class_selector ( self ): ''' get pair list ''' return DexProtocol . _class_map ()[ self ] @property def factory_class ( self ): return self . _class_selector ()","title":"DexProtocol"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/#dex-class","text":"check_if_decheck_if_pair_is_token_or_dex Queries dgraph and tries to find a node with pair's uid, if found one, checks if it's schema type (dgraph.type) is equal to \"Pair\" or not. async def check_if_pair_is_token_or_dex ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type != \"Pair\" : logging . info ( f \"pair uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True retrieve_pair Uses factory to find all of it's pairs and make Pair objects. will be explained in Protocol section.","title":"Dex class"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_pairs/#pair-calss","text":"save_dex_pair Converts Pair object to a dictionary that can be inserted to dgraph. will be explained in Protocol section. ## Insert pairs insert_pairs async def insert_pairs ( chain_id ): dexs = read_data ( chain_id ) return await save_all_pairs ( dexs ) async def save_all_pairs ( dexs ): return await asyncio . gather ( * [ asyncio . ensure_future ( _retrive_dex_pairs ( dex )) for dex in dexs ], return_exceptions = True ) async def _retrive_dex_pairs ( dex ): try : logging . info ( f \"Adding Dex { dex . get ( 'name' ) } \" ) for pair in DexProtocol ( dex . get ( \"protocol\" ) ) . factory_class ( ** dex ) . retrive_pair (): try : if await pair . check_if_pair_is_token_or_dex (): continue made_pair = pair . save_dex_pair () await dgraph_client () . insert ( made_pair ) logging . info ( f \"pair: { pair . uid } added\" ) print ( f \"pair: { pair . uid } added\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at save_all_pairs. Pair: { pair } ' ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at save_all_pairs. Dex: { dex } .' )","title":"Pair calss"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_prices/","text":"Insert and Update Token Prices \u00b6 Calculate price of tokens comparing to network's value token. 1. Get network's base token We choose the network's wrapped token (wrapped of network's value) as the base token and we compare other tokens to this token. async def _get_chain_base_token ( chain_id : ChainId ): try : chain_token_address = Chain ( ** { \"chain_id\" : chain_id }) . network_wrapped_token_address chain_token_uid = Token . get_uid ( chain_id , chain_token_address ) if not chain_token_uid : logging . critical ( \"There is no uid for network value wrapped token! at updating_token_prices.\" ) return None dgraph_token = await dgraph_client () . find_by_uid ( chain_token_uid , [ \"address\" ] ) if not dgraph_token : logging . critical ( f \"Network wrapped token: { chain_token_uid } doesn't have an address in dgraph!\" ) return None , None return chain_token_address , chain_token_uid except Exception as e : logging . error ( f \"Exception { e } occurred at _get_chain_base_token.\" ) return None , None Insert base token's price (price = 1) to database. async def insert_base_token_price ( chain_id : ChainId , chain_token_address : Address ): \"\"\" This function takes chain_token_uid and inserts price = 1 for it's object\"\"\" try : token = Token . get_cache ( key = chain_token_address , chain_id = chain_id ) token . price = 1 token . cache () chain_token_price_mutation = token . save_token () await dgraph_client () . insert ( chain_token_price_mutation ) except Exception as e : logging . error ( f \"Exception { e } occurred at insert_base_token_price.\" ) Get pairs of base token async def get_base_token_pairs ( base_token_uid : Uid ) -> List [ Node ]: \"\"\" this function takes base_token_uid and returns all pairs connected to chain token \"\"\" try : res = await dgraph_client () . find_by_uid ( base_token_uid , [ \"pairs {uid expand(_all_)}\" ] ) return res . get ( \"pairs\" ) except Exception as e : logging . error ( f \"Exception { e } occurred at get_base_token_pairs.\" ) return None Make Pair object of base token's pairs Calculate price of each pair's tokens, based on pair's protocol. _protocol = _pair . protocol if _protocol in ( PairProtocol . UNISWAP . value , PairProtocol . SOULSWAP . value , PairProtocol . SOLIDLY . value ): try : other_token_index = 1 - chain_token_index price , token_reserve = calculate_amm_price ( other_token_index , chain_token_index , _pair ) else : for address , index in tokens_indexes . items (): if index == chain_token_index : continue reserves , price = caculate_not_amm_price ( index , chain_token_index , _pair ) calculate AMM prices Takes: base token's index in pair toToken's index in pair the object of Pair Returns: price of toToken pair's reserves Divides reserve of base token to reserve of toToken def calculate_amm_price ( token_index : int , chain_token_index : int , pair : Pair ): \"\"\" this function calculates prices of amm pairs by deviding their reserves to each other \"\"\" try : _res = ( pair . reserves [ chain_token_index ] / ( 10 ** pair . decimals [ chain_token_index ])) / \\ ( pair . reserves [ token_index ] / ( 10 ** pair . decimals [ token_index ])) except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_amm_price.\" ) return None , None return _res , pair . reserves [ token_index ] Calculate Not-AMM prices Takes: base token's index in pair toToken's index in pair the object of Pair Returns: price of toToken pair's reserves For 1 as amount_in, base token as fromToken and other token as toToken, calls the amount_out method of each Pair class. def calculate_not_amm_price ( token_index : int , chain_token_index : int , pair : Pair ): try : amount_in : WithDecimal = int ( 0.01 * 10 ** pair . decimals [ token_index ]) price : WithDecimal = pair . amount_out ( token_index , chain_token_index , amount_in ) if price == 0 : return None , None price : NoDecimal = price / \\ ( 10 ** pair . decimals [ chain_token_index ]) price *= 100 logging . info ( f \"NOT amm: { price } { pair . token_symbols } { pair . protocol } \" ) return pair . reserves [ token_index ], price except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_not_amm_price.\" ) return None , None Calculate the average price Lots of tokens have more than one common pairs with base token, we set the average price as the final price. async def update_price_predicate ( uid_price_map : Dict [ Uid , Tuple [ int , float ]], chain_id : ChainId ): \"\"\" gets all of tokens prices and calculates a weighted average to set price predicate on it's node and in redis \"\"\" for uid , value in uid_price_map . items (): try : numerator , denominator = [ res * price for res , price in value ], [ res for res , price in value ] final_price = sum ( numerator ) / sum ( denominator ) token_address = Token . get_identifier ( chain_id , uid ) token = Token . get_cache ( token_address , chain_id ) token . price = final_price token . cache () await dgraph_client () . insert ( token . save_token ()) logging . info ( \"*************UPDATING PRICE************\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at update_price_predicate.' ) Update price of remaining tokens. Get remaining tokens Some tokens don't have any common pair with base token, so we need to find another solution to calculate their prices. First we query dgraph and find tokens with no prices async def get_remaining_tokens ( chain_id : int , updated_token_uids : Set [ Uid ]) -> Set [ Uid ]: try : _tokens = await dgraph_client () . find_by_chain_id ( chain_id = chain_id , schema_type = \"Token\" ) tokens = { _ . get ( \"uid\" ) for _ in _tokens . get ( \"~chain\" )} return tokens - updated_token_uids except Exception as e : logging . error ( f \"Exception { e } occurred at get_remaining_tokens.\" ) return None Get base tokens Then we find base tokens (first 15 tokens that have most pairs). we try to get their uid from redis and if couldn't find them there, we query dgraph. async def find_base_tokens ( chain_id : ChainId ): tokens = await dgraph_client () . find_base_tokens () base_tokens = [] for token in tokens : if token . get ( \"address\" ) is None : logging . critical ( f \"token { token . get ( 'uid' ) } doesn't have address in dgraph.\" ) continue if token . get ( \"chain\" ) . get ( \"chain_id\" ) == chain_id : base_tokens . append ( token . get ( \"uid\" )) save_base_tokens ( chain_id , base_tokens ) return base_tokens Find common pairs between remaining tokens and base tokens In order to calculate prices, we need to find some pairs that have both remaining token and base token as tokens. After finding the first pair containing both remaining token and base token, we calculate the price and insert it to both redis and dgraph. for token_uid in remaining_tokens : price = None token_address = Token . get_identifier ( chain_id = chain_id , uid = token_uid ) for _bt_uid in base_tokens : pairs = await dgraph_client () . common_pairs ( from_token = token_uid , to_token = _bt_uid ) if pairs is None : continue pairs = { _ . get ( \"uid\" ) for _ in pairs . get ( \"pairs\" )} token = Token . get_cache ( token_address , chain_id ) price = await _calculate_price ( chain_id = chain_id , pairs = pairs , base_token_uid = _bt_uid , token_address = token_address ) if price is None : continue token . price = price token . cache () await dgraph_client () . insert ( token . save_token ()) break Calculate remaining price For obtained pair as the common pair of remaining token and base token, we calculate price of remaining based on Pair object's protocol (AMM/Not-Amm). Then we call _final_price function which calculates the average price. async def _calculate_price ( chain_id : int , pairs : Set [ Uid ], base_token_uid : Uid , token_address : Address ): _base_token_address = Token . get_identifier ( chain_id = chain_id , uid = base_token_uid ) numerator , dominator = 0 , 0 for _pair_uid in pairs : _reserves , _price = None , None _pair = await dgraph_client () . find_by_uid ( uid = _pair_uid , predicates = [ \"expand(_all_)\" ]) _pair = await make_pair_obj ( pair = _pair , chain_id = chain_id ) _protocol = _pair . protocol if _protocol == PairProtocol . UNISWAP . value or \\ _protocol == PairProtocol . SOULSWAP . value or \\ _protocol == PairProtocol . SOLIDLY . value : _reserves , _price = await _calculate_amm_price ( pair = _pair , base_token_address = _base_token_address ) else : _reserves , _price = await _calculate_non_amm_price ( pair = _pair , base_token_address = _base_token_address , token_address = token_address ) if _reserves is None or _price is None : continue numerator += _reserves * _price dominator += _reserves return await _final_price ( numerator = numerator , dominator = dominator , base_token_address = _base_token_address , chain_id = chain_id ) async def _final_price ( numerator , dominator , base_token_address : Address , chain_id : int ): try : _price = numerator / dominator token = Token . get_cache ( key = base_token_address , chain_id = chain_id ) return _price * token . price except Exception as e : logging . error ( f \" { e } happened at _final_prices\" ) return None","title":"Insert and Update Token Prices"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_prices/#insert-and-update-token-prices","text":"Calculate price of tokens comparing to network's value token. 1. Get network's base token We choose the network's wrapped token (wrapped of network's value) as the base token and we compare other tokens to this token. async def _get_chain_base_token ( chain_id : ChainId ): try : chain_token_address = Chain ( ** { \"chain_id\" : chain_id }) . network_wrapped_token_address chain_token_uid = Token . get_uid ( chain_id , chain_token_address ) if not chain_token_uid : logging . critical ( \"There is no uid for network value wrapped token! at updating_token_prices.\" ) return None dgraph_token = await dgraph_client () . find_by_uid ( chain_token_uid , [ \"address\" ] ) if not dgraph_token : logging . critical ( f \"Network wrapped token: { chain_token_uid } doesn't have an address in dgraph!\" ) return None , None return chain_token_address , chain_token_uid except Exception as e : logging . error ( f \"Exception { e } occurred at _get_chain_base_token.\" ) return None , None Insert base token's price (price = 1) to database. async def insert_base_token_price ( chain_id : ChainId , chain_token_address : Address ): \"\"\" This function takes chain_token_uid and inserts price = 1 for it's object\"\"\" try : token = Token . get_cache ( key = chain_token_address , chain_id = chain_id ) token . price = 1 token . cache () chain_token_price_mutation = token . save_token () await dgraph_client () . insert ( chain_token_price_mutation ) except Exception as e : logging . error ( f \"Exception { e } occurred at insert_base_token_price.\" ) Get pairs of base token async def get_base_token_pairs ( base_token_uid : Uid ) -> List [ Node ]: \"\"\" this function takes base_token_uid and returns all pairs connected to chain token \"\"\" try : res = await dgraph_client () . find_by_uid ( base_token_uid , [ \"pairs {uid expand(_all_)}\" ] ) return res . get ( \"pairs\" ) except Exception as e : logging . error ( f \"Exception { e } occurred at get_base_token_pairs.\" ) return None Make Pair object of base token's pairs Calculate price of each pair's tokens, based on pair's protocol. _protocol = _pair . protocol if _protocol in ( PairProtocol . UNISWAP . value , PairProtocol . SOULSWAP . value , PairProtocol . SOLIDLY . value ): try : other_token_index = 1 - chain_token_index price , token_reserve = calculate_amm_price ( other_token_index , chain_token_index , _pair ) else : for address , index in tokens_indexes . items (): if index == chain_token_index : continue reserves , price = caculate_not_amm_price ( index , chain_token_index , _pair ) calculate AMM prices Takes: base token's index in pair toToken's index in pair the object of Pair Returns: price of toToken pair's reserves Divides reserve of base token to reserve of toToken def calculate_amm_price ( token_index : int , chain_token_index : int , pair : Pair ): \"\"\" this function calculates prices of amm pairs by deviding their reserves to each other \"\"\" try : _res = ( pair . reserves [ chain_token_index ] / ( 10 ** pair . decimals [ chain_token_index ])) / \\ ( pair . reserves [ token_index ] / ( 10 ** pair . decimals [ token_index ])) except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_amm_price.\" ) return None , None return _res , pair . reserves [ token_index ] Calculate Not-AMM prices Takes: base token's index in pair toToken's index in pair the object of Pair Returns: price of toToken pair's reserves For 1 as amount_in, base token as fromToken and other token as toToken, calls the amount_out method of each Pair class. def calculate_not_amm_price ( token_index : int , chain_token_index : int , pair : Pair ): try : amount_in : WithDecimal = int ( 0.01 * 10 ** pair . decimals [ token_index ]) price : WithDecimal = pair . amount_out ( token_index , chain_token_index , amount_in ) if price == 0 : return None , None price : NoDecimal = price / \\ ( 10 ** pair . decimals [ chain_token_index ]) price *= 100 logging . info ( f \"NOT amm: { price } { pair . token_symbols } { pair . protocol } \" ) return pair . reserves [ token_index ], price except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_not_amm_price.\" ) return None , None Calculate the average price Lots of tokens have more than one common pairs with base token, we set the average price as the final price. async def update_price_predicate ( uid_price_map : Dict [ Uid , Tuple [ int , float ]], chain_id : ChainId ): \"\"\" gets all of tokens prices and calculates a weighted average to set price predicate on it's node and in redis \"\"\" for uid , value in uid_price_map . items (): try : numerator , denominator = [ res * price for res , price in value ], [ res for res , price in value ] final_price = sum ( numerator ) / sum ( denominator ) token_address = Token . get_identifier ( chain_id , uid ) token = Token . get_cache ( token_address , chain_id ) token . price = final_price token . cache () await dgraph_client () . insert ( token . save_token ()) logging . info ( \"*************UPDATING PRICE************\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at update_price_predicate.' ) Update price of remaining tokens. Get remaining tokens Some tokens don't have any common pair with base token, so we need to find another solution to calculate their prices. First we query dgraph and find tokens with no prices async def get_remaining_tokens ( chain_id : int , updated_token_uids : Set [ Uid ]) -> Set [ Uid ]: try : _tokens = await dgraph_client () . find_by_chain_id ( chain_id = chain_id , schema_type = \"Token\" ) tokens = { _ . get ( \"uid\" ) for _ in _tokens . get ( \"~chain\" )} return tokens - updated_token_uids except Exception as e : logging . error ( f \"Exception { e } occurred at get_remaining_tokens.\" ) return None Get base tokens Then we find base tokens (first 15 tokens that have most pairs). we try to get their uid from redis and if couldn't find them there, we query dgraph. async def find_base_tokens ( chain_id : ChainId ): tokens = await dgraph_client () . find_base_tokens () base_tokens = [] for token in tokens : if token . get ( \"address\" ) is None : logging . critical ( f \"token { token . get ( 'uid' ) } doesn't have address in dgraph.\" ) continue if token . get ( \"chain\" ) . get ( \"chain_id\" ) == chain_id : base_tokens . append ( token . get ( \"uid\" )) save_base_tokens ( chain_id , base_tokens ) return base_tokens Find common pairs between remaining tokens and base tokens In order to calculate prices, we need to find some pairs that have both remaining token and base token as tokens. After finding the first pair containing both remaining token and base token, we calculate the price and insert it to both redis and dgraph. for token_uid in remaining_tokens : price = None token_address = Token . get_identifier ( chain_id = chain_id , uid = token_uid ) for _bt_uid in base_tokens : pairs = await dgraph_client () . common_pairs ( from_token = token_uid , to_token = _bt_uid ) if pairs is None : continue pairs = { _ . get ( \"uid\" ) for _ in pairs . get ( \"pairs\" )} token = Token . get_cache ( token_address , chain_id ) price = await _calculate_price ( chain_id = chain_id , pairs = pairs , base_token_uid = _bt_uid , token_address = token_address ) if price is None : continue token . price = price token . cache () await dgraph_client () . insert ( token . save_token ()) break Calculate remaining price For obtained pair as the common pair of remaining token and base token, we calculate price of remaining based on Pair object's protocol (AMM/Not-Amm). Then we call _final_price function which calculates the average price. async def _calculate_price ( chain_id : int , pairs : Set [ Uid ], base_token_uid : Uid , token_address : Address ): _base_token_address = Token . get_identifier ( chain_id = chain_id , uid = base_token_uid ) numerator , dominator = 0 , 0 for _pair_uid in pairs : _reserves , _price = None , None _pair = await dgraph_client () . find_by_uid ( uid = _pair_uid , predicates = [ \"expand(_all_)\" ]) _pair = await make_pair_obj ( pair = _pair , chain_id = chain_id ) _protocol = _pair . protocol if _protocol == PairProtocol . UNISWAP . value or \\ _protocol == PairProtocol . SOULSWAP . value or \\ _protocol == PairProtocol . SOLIDLY . value : _reserves , _price = await _calculate_amm_price ( pair = _pair , base_token_address = _base_token_address ) else : _reserves , _price = await _calculate_non_amm_price ( pair = _pair , base_token_address = _base_token_address , token_address = token_address ) if _reserves is None or _price is None : continue numerator += _reserves * _price dominator += _reserves return await _final_price ( numerator = numerator , dominator = dominator , base_token_address = _base_token_address , chain_id = chain_id ) async def _final_price ( numerator , dominator , base_token_address : Address , chain_id : int ): try : _price = numerator / dominator token = Token . get_cache ( key = base_token_address , chain_id = chain_id ) return _price * token . price except Exception as e : logging . error ( f \" { e } happened at _final_prices\" ) return None","title":"Insert and Update Token Prices"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/","text":"Insert Tokens \u00b6 In this module, we are trying to get the token details of stored pairs from blockchain, and create Token object based on those details. Only thing left here is to save them on dgraph (DB of choice). 1. Query dgraph and finds all token addresses of existing pairs. 2. Uses Token 's detail method to create token object of specific addresses. 3. Uses token check_if_token_is_pair_or_dex method to check if token already exists in database and if yes, what schema type it has. 4. Uses token save_token method to convert token object into a dictionary that can be inserted to database. 5. Inserts token dictionary to dgraph. Find token addresses \u00b6 get_all_pairs_token async def get_all_pairs_tokens ( chain_id : ChainId ) -> List [ List [ Address ]]: pairs = await dgraph_client () . find_by_type ( \"Pair\" , [ \"token_addresses\" , \"chain { chain_id }\" ] ) tokens_addresses = [] for pair in pairs : token_addresses = split_pairs ( chain_id , pair ) if token_addresses is not None : tokens_addresses . append ( token_addresses ) return tokens_addresses def split_pairs ( chain_id : ChainId , pair : Dict ): try : if pair . get ( \"chain\" ) . get ( \"chain_id\" ) == chain_id : token_addresses = pair . get ( \"token_addresses\" ) . split ( \"#\" ) return token_addresses return None except Exception as e : logging . error ( e ) return None Get and save tokens \u00b6 async def get_and_save_tokens ( chain_id : ChainId , tokens_addresses = List [ List [ Address ]] ) -> List [ Token ]: added_tokens = [] for token_addresses in tokens_addresses : for token_address in token_addresses : try : token = Token . detail ( token_address , chain_id , Network ( chain_id ) . web3_client , redis_client ( chain_id ) ) if token is None : logging . error ( f \"Couldn't make token { token_address } but it's address exist in a pair, at get_and_save_tokens.\" ) continue if await token . check_if_token_is_pair_or_dex (): logging . warning ( \"Repeated dgraph.type at get_and_save_tokens.\" ) continue token_uid = token . uid token = token . save_token () if token_uid not in added_tokens : await dgraph_client () . insert ( token ) logging . info ( f \"token: { token_uid } added\" ) print ( f \"token: { token_uid } added\" ) added_tokens . append ( token_uid ) except Exception as e : logging . exception ( f \"Expection { e } occured at get_and_save_tokens.\" ) Token class \u00b6 detail Checks if token is cached in redis as a bad token. Check if token is cached in redis as a good token. Checks if token is amm pair or amm factory and if yes, caches it as a bad token and returns None. Gets token's details (name, symbol and decimal) by calling cls._token_network_detail. Makes the token object and caches it in redis. @classmethod def detail ( cls , address , chain , w3 , _redis_client ) -> Token : if cls . _is_bad_token ( address , _redis_client ): return None token = Token . get_cache ( address , chain ) if token is not None : return token _c = w3 . eth . contract ( address , abi = ABI . TOKEN ) time . sleep ( Redis . rpc_request ( chain = chain , count = 3 ) ) if ( not cls . _is_addres_amm_pair ( address , w3 ) and not cls . _is_address_amm_factory ( address , w3 ) ): token_details = cls . _token_network_detail ( _c ) if token_details : token = cls ( ** { \"chain_id\" : chain , \"address\" : address , ** token_details }) token . cache () return token cls . _cache_bad_token_address ( address , _redis_client ) return None token_network_detail Uses batch contract's \"callContractsWithStruct\" function to find name, symbol and decimals of token. Contract's logic will be explained in Contract section. @classmethod def _token_network_detail ( cls , address : Address , batch_contract : Contract , _c : Contract ) -> RawToken : try : result = batch_contract . functions . callContractsWithStruct ( cls . token_entries ( _c = _c , address = address )) . call () token_details = cls . decode_batch_output ( _c , result ) return token_details except Exception as e : logging . exception ( e ) return None token_entries Uses \"contract.functions._encode_traction_data()\" to find batch contract's callContractsWithStruct function entries. @classmethod def token_entries ( cls , _c : Contract , address : Address ): try : # name = _c.functions.name()._encode_transaction_data() # symbol = _c.functions.symbol()._encode_transaction_data() # decimal = _c.functions.decimals()._encode_transaction_data() # return [[address, name], [address, symbol], [address, decimal]] return [[ address , '0x06fdde03' ], [ address , '0x95d89b41' ], [ address , '0x313ce567' ]] except Exception as e : logging . exception ( f \"Exception { e } occured at Token.token_entries.\" ) decode_batch_output Decodes callContractsWithStruct's output witch is a list of bytes. @classmethod def decode_batch_output ( cls , _c : Contract , batch_output : List ): try : try : name_len = int ( batch_output [ 0 ] . hex ()[ 64 : 128 ], 16 ) name = bytes . fromhex ( batch_output [ 0 ] . hex ()[ 128 : 192 ]) . decode ( 'ascii' )[: name_len ] except Exception as e : logging . exception ( f \"Exception { e } occured at Token.decode_batch_output.\" ) name = _c . functions . name () . call try : symbol_len = int ( batch_output [ 1 ] . hex ()[ 64 : 128 ], 16 ) symbol = bytes . fromhex ( batch_output [ 1 ] . hex ()[ 128 : 192 ]) . decode ( 'ascii' )[: symbol_len ] except Exception as e : logging . exception ( f \"Exception { e } occured at Token.decode_batch_output.\" ) decimal = int ( batch_output [ 2 ] . hex ()[: 64 ], 16 ) return { 'name' : name , 'symbol' : symbol , 'decimal' : decimal } except Exception as e : logging . exception ( f \"Exception { e } occured at Token.decode_batch_output.\" ) check_if_token_is_pair_or_dex Queries dgraph and tries to find a node with token's uid, if found one, checks if it's schema type (dgraph.type) is equal to \"Token\" or not. async def check_if_token_is_pair_or_dex ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type != \"Token\" : logging . info ( f \"token uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True return False return False save_token Converts the Token object to a dictionary. connects \"Token\" to \"Chain\" using \"chain\" edge. def save_token ( self ): obj = self . dict () obj [ \"uid\" ] = self . uid obj [ \"dgraph.type\" ] = dt . TOKEN obj [ \"burn_rate\" ] = self . burn_rate if self . price is not None : obj [ \"price\" ] = str ( self . price ) obj [ \"chain\" ] = { \"uid\" : self . get_and_set_uid ( self . chain_id . value , self . chain_id . value ), \"chain_id\" : self . chain_id . value , \"dgraph.type\" : dt . CHAIN } obj . pop ( \"chain_id\" ) return obj Insert tokens \u00b6 insert_tokens async def insert_dexes ( chain_id , dexes : List [ RawDex ] = None ): if dexes is None : dexes = read_data ( chain_id ) for dex in dexes : try : factory = DexProtocol ( dex . get ( \"protocol\" ) ) . factory_class ( ** dex ) if await factory . check_if_dex_is_token_or_pair (): continue factory = factory . save_dex () await dgraph_client () . insert ( factory ) logging . info ( f \"factory { factory . get ( 'uid' ) } added.\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at fetch_dexes function. dex: { dex } ' ) logging . info ( f \"protocol { dex . get ( 'protocol' ) } does not exist\" ) Insert tokens \u00b6 async def insert_tokens ( chain_id : ChainId , addresses : List [ Address ] = None ) -> List [ Token ]: if addresses is None : addresses = await get_all_pairs_tokens ( chain_id ) return await get_and_save_tokens ( chain_id , addresses )","title":"Insert Tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/#insert-tokens","text":"In this module, we are trying to get the token details of stored pairs from blockchain, and create Token object based on those details. Only thing left here is to save them on dgraph (DB of choice). 1. Query dgraph and finds all token addresses of existing pairs. 2. Uses Token 's detail method to create token object of specific addresses. 3. Uses token check_if_token_is_pair_or_dex method to check if token already exists in database and if yes, what schema type it has. 4. Uses token save_token method to convert token object into a dictionary that can be inserted to database. 5. Inserts token dictionary to dgraph.","title":"Insert Tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/#find-token-addresses","text":"get_all_pairs_token async def get_all_pairs_tokens ( chain_id : ChainId ) -> List [ List [ Address ]]: pairs = await dgraph_client () . find_by_type ( \"Pair\" , [ \"token_addresses\" , \"chain { chain_id }\" ] ) tokens_addresses = [] for pair in pairs : token_addresses = split_pairs ( chain_id , pair ) if token_addresses is not None : tokens_addresses . append ( token_addresses ) return tokens_addresses def split_pairs ( chain_id : ChainId , pair : Dict ): try : if pair . get ( \"chain\" ) . get ( \"chain_id\" ) == chain_id : token_addresses = pair . get ( \"token_addresses\" ) . split ( \"#\" ) return token_addresses return None except Exception as e : logging . error ( e ) return None","title":"Find token addresses"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/#get-and-save-tokens","text":"async def get_and_save_tokens ( chain_id : ChainId , tokens_addresses = List [ List [ Address ]] ) -> List [ Token ]: added_tokens = [] for token_addresses in tokens_addresses : for token_address in token_addresses : try : token = Token . detail ( token_address , chain_id , Network ( chain_id ) . web3_client , redis_client ( chain_id ) ) if token is None : logging . error ( f \"Couldn't make token { token_address } but it's address exist in a pair, at get_and_save_tokens.\" ) continue if await token . check_if_token_is_pair_or_dex (): logging . warning ( \"Repeated dgraph.type at get_and_save_tokens.\" ) continue token_uid = token . uid token = token . save_token () if token_uid not in added_tokens : await dgraph_client () . insert ( token ) logging . info ( f \"token: { token_uid } added\" ) print ( f \"token: { token_uid } added\" ) added_tokens . append ( token_uid ) except Exception as e : logging . exception ( f \"Expection { e } occured at get_and_save_tokens.\" )","title":"Get and save tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/#token-class","text":"detail Checks if token is cached in redis as a bad token. Check if token is cached in redis as a good token. Checks if token is amm pair or amm factory and if yes, caches it as a bad token and returns None. Gets token's details (name, symbol and decimal) by calling cls._token_network_detail. Makes the token object and caches it in redis. @classmethod def detail ( cls , address , chain , w3 , _redis_client ) -> Token : if cls . _is_bad_token ( address , _redis_client ): return None token = Token . get_cache ( address , chain ) if token is not None : return token _c = w3 . eth . contract ( address , abi = ABI . TOKEN ) time . sleep ( Redis . rpc_request ( chain = chain , count = 3 ) ) if ( not cls . _is_addres_amm_pair ( address , w3 ) and not cls . _is_address_amm_factory ( address , w3 ) ): token_details = cls . _token_network_detail ( _c ) if token_details : token = cls ( ** { \"chain_id\" : chain , \"address\" : address , ** token_details }) token . cache () return token cls . _cache_bad_token_address ( address , _redis_client ) return None token_network_detail Uses batch contract's \"callContractsWithStruct\" function to find name, symbol and decimals of token. Contract's logic will be explained in Contract section. @classmethod def _token_network_detail ( cls , address : Address , batch_contract : Contract , _c : Contract ) -> RawToken : try : result = batch_contract . functions . callContractsWithStruct ( cls . token_entries ( _c = _c , address = address )) . call () token_details = cls . decode_batch_output ( _c , result ) return token_details except Exception as e : logging . exception ( e ) return None token_entries Uses \"contract.functions._encode_traction_data()\" to find batch contract's callContractsWithStruct function entries. @classmethod def token_entries ( cls , _c : Contract , address : Address ): try : # name = _c.functions.name()._encode_transaction_data() # symbol = _c.functions.symbol()._encode_transaction_data() # decimal = _c.functions.decimals()._encode_transaction_data() # return [[address, name], [address, symbol], [address, decimal]] return [[ address , '0x06fdde03' ], [ address , '0x95d89b41' ], [ address , '0x313ce567' ]] except Exception as e : logging . exception ( f \"Exception { e } occured at Token.token_entries.\" ) decode_batch_output Decodes callContractsWithStruct's output witch is a list of bytes. @classmethod def decode_batch_output ( cls , _c : Contract , batch_output : List ): try : try : name_len = int ( batch_output [ 0 ] . hex ()[ 64 : 128 ], 16 ) name = bytes . fromhex ( batch_output [ 0 ] . hex ()[ 128 : 192 ]) . decode ( 'ascii' )[: name_len ] except Exception as e : logging . exception ( f \"Exception { e } occured at Token.decode_batch_output.\" ) name = _c . functions . name () . call try : symbol_len = int ( batch_output [ 1 ] . hex ()[ 64 : 128 ], 16 ) symbol = bytes . fromhex ( batch_output [ 1 ] . hex ()[ 128 : 192 ]) . decode ( 'ascii' )[: symbol_len ] except Exception as e : logging . exception ( f \"Exception { e } occured at Token.decode_batch_output.\" ) decimal = int ( batch_output [ 2 ] . hex ()[: 64 ], 16 ) return { 'name' : name , 'symbol' : symbol , 'decimal' : decimal } except Exception as e : logging . exception ( f \"Exception { e } occured at Token.decode_batch_output.\" ) check_if_token_is_pair_or_dex Queries dgraph and tries to find a node with token's uid, if found one, checks if it's schema type (dgraph.type) is equal to \"Token\" or not. async def check_if_token_is_pair_or_dex ( self ): obj = await dgraph_client () . find_by_uid ( self . uid , [ \"dgraph.type\" ]) if obj is not None : for d_type in obj . get ( \"dgraph.type\" ): if d_type != \"Token\" : logging . info ( f \"token uid was used before { self . uid } : { obj . get ( 'dgraph.type' ) } \" ) return True return False return False save_token Converts the Token object to a dictionary. connects \"Token\" to \"Chain\" using \"chain\" edge. def save_token ( self ): obj = self . dict () obj [ \"uid\" ] = self . uid obj [ \"dgraph.type\" ] = dt . TOKEN obj [ \"burn_rate\" ] = self . burn_rate if self . price is not None : obj [ \"price\" ] = str ( self . price ) obj [ \"chain\" ] = { \"uid\" : self . get_and_set_uid ( self . chain_id . value , self . chain_id . value ), \"chain_id\" : self . chain_id . value , \"dgraph.type\" : dt . CHAIN } obj . pop ( \"chain_id\" ) return obj","title":"Token class"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/#insert-tokens_1","text":"insert_tokens async def insert_dexes ( chain_id , dexes : List [ RawDex ] = None ): if dexes is None : dexes = read_data ( chain_id ) for dex in dexes : try : factory = DexProtocol ( dex . get ( \"protocol\" ) ) . factory_class ( ** dex ) if await factory . check_if_dex_is_token_or_pair (): continue factory = factory . save_dex () await dgraph_client () . insert ( factory ) logging . info ( f \"factory { factory . get ( 'uid' ) } added.\" ) except Exception as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } at fetch_dexes function. dex: { dex } ' ) logging . info ( f \"protocol { dex . get ( 'protocol' ) } does not exist\" )","title":"Insert tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/insert_tokens/#insert-tokens_2","text":"async def insert_tokens ( chain_id : ChainId , addresses : List [ Address ] = None ) -> List [ Token ]: if addresses is None : addresses = await get_all_pairs_tokens ( chain_id ) return await get_and_save_tokens ( chain_id , addresses )","title":"Insert tokens"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/update_pairs/","text":"Update Pairs \u00b6 Because of new transactions, reserves of pairs are constantly changing nad to calculate the right amount output we need to have updated reserves. 1. Deletes all of changed_pairs uids from redis. (For each pair, if it's reserves change, it will be cached again, using decode_batch_output method.) 2. Queries dgraph and gets all pairs. 3. Makes Pair object of pairs. async def get_all_kind_of_pairs ( chain_id : ChainId ) -> List [ Pair ]: try : all_pairs = await dgraph_client () . find_by_chain_id ( chain_id , \"Pair\" , [ \"uid\" , \"expand(_all_)\" ] ) pairs = [] all_pairs = all_pairs . get ( \"~chain\" ) for pair in all_pairs : pair_obj = await make_pair_obj ( pair , chain_id ) pairs . append ( pair_obj ) return pairs except Exception as e : logging . error ( f \"Exception { e } occured at get_all_kinda_pairs.\" ) 4. For each pair, calls the pair_entries method in it's class and return the data that batch contract's \"callContractsWithStruct\" need. def get_all_kind_of_pairs_entries ( pairs : List [ Pair ]) -> List [ Tuple ]: enteries = [] for pair in pairs : try : enteries . append ( pair . pair_entries ()) except Exception as e : print ( e ) logging . exception ( f \"protocol { pair . protocol } does not have pair_entries function\" ) return enteries Some pairs might have more than one entry, sort_entries sorts entries and return a list of tuples. def sort_entries ( entries ): new_entires = [] for entry in entries : for item in entry : new_entires . append ( tuple ( item )) return new_entires Calls batch contract's callContractsWithStruct function which returns a list of bytes. For each pair, decodes batch's output using pair's decode_pair_output method. async def update_pairs ( chain_id : ChainId ): delete_changed_pairs ( chain_id ) chain = Chain ( chain_id = chain_id ) pairs = await get_all_kind_of_pairs ( chain_id ) entries = get_all_kind_of_pairs_entries ( pairs ) new_entries = sort_entries ( entries ) c = chain . batch_everything_contract results = c . functions . callContractsWithStruct ( new_entries ) . call () counter = 0 for pair , _pair_entriez in zip ( pairs , entries ): try : _r_c = len ( _pair_entriez ) new_pair = pair . decode_batch_output ( results [ counter : counter + _r_c ]) await dgraph_client () . insert ( new_pair ) logging . info ( f \"Updated pair { pair . uid } .\" ) print ( f \"Updated pair { pair . uid } .\" ) updated_pairs_num += 1 counter += _r_c except Exception as e : logging . exception ( f \"Exception { e } occured in protocol { pair . protocol } decode_batch_output function.\" )","title":"Update Pairs"},{"location":"Services/Aggregator/BackGroundTasks/DgraphRelated/update_pairs/#update-pairs","text":"Because of new transactions, reserves of pairs are constantly changing nad to calculate the right amount output we need to have updated reserves. 1. Deletes all of changed_pairs uids from redis. (For each pair, if it's reserves change, it will be cached again, using decode_batch_output method.) 2. Queries dgraph and gets all pairs. 3. Makes Pair object of pairs. async def get_all_kind_of_pairs ( chain_id : ChainId ) -> List [ Pair ]: try : all_pairs = await dgraph_client () . find_by_chain_id ( chain_id , \"Pair\" , [ \"uid\" , \"expand(_all_)\" ] ) pairs = [] all_pairs = all_pairs . get ( \"~chain\" ) for pair in all_pairs : pair_obj = await make_pair_obj ( pair , chain_id ) pairs . append ( pair_obj ) return pairs except Exception as e : logging . error ( f \"Exception { e } occured at get_all_kinda_pairs.\" ) 4. For each pair, calls the pair_entries method in it's class and return the data that batch contract's \"callContractsWithStruct\" need. def get_all_kind_of_pairs_entries ( pairs : List [ Pair ]) -> List [ Tuple ]: enteries = [] for pair in pairs : try : enteries . append ( pair . pair_entries ()) except Exception as e : print ( e ) logging . exception ( f \"protocol { pair . protocol } does not have pair_entries function\" ) return enteries Some pairs might have more than one entry, sort_entries sorts entries and return a list of tuples. def sort_entries ( entries ): new_entires = [] for entry in entries : for item in entry : new_entires . append ( tuple ( item )) return new_entires Calls batch contract's callContractsWithStruct function which returns a list of bytes. For each pair, decodes batch's output using pair's decode_pair_output method. async def update_pairs ( chain_id : ChainId ): delete_changed_pairs ( chain_id ) chain = Chain ( chain_id = chain_id ) pairs = await get_all_kind_of_pairs ( chain_id ) entries = get_all_kind_of_pairs_entries ( pairs ) new_entries = sort_entries ( entries ) c = chain . batch_everything_contract results = c . functions . callContractsWithStruct ( new_entries ) . call () counter = 0 for pair , _pair_entriez in zip ( pairs , entries ): try : _r_c = len ( _pair_entriez ) new_pair = pair . decode_batch_output ( results [ counter : counter + _r_c ]) await dgraph_client () . insert ( new_pair ) logging . info ( f \"Updated pair { pair . uid } .\" ) print ( f \"Updated pair { pair . uid } .\" ) updated_pairs_num += 1 counter += _r_c except Exception as e : logging . exception ( f \"Exception { e } occured in protocol { pair . protocol } decode_batch_output function.\" )","title":"Update Pairs"},{"location":"Services/Aggregator/Configs/dgraph_config/","text":"","title":"Dgraph config"},{"location":"Services/Aggregator/Databases/","text":"Databases \u00b6","title":"Databases"},{"location":"Services/Aggregator/Databases/#databases","text":"","title":"Databases"},{"location":"Services/Aggregator/Databases/Dgraph/","text":"Dgraph \u00b6 The native GraphQL database with a graph backend. We use it to store the main part of our data. Read More.","title":"Dgraph"},{"location":"Services/Aggregator/Databases/Dgraph/#dgraph","text":"The native GraphQL database with a graph backend. We use it to store the main part of our data. Read More.","title":"Dgraph"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/","text":"More on Dgraph \u00b6 GraphQL \u00b6 Dgraph's Generated API. DQL \u00b6 Dgraph's Query Language. Pydgraph \u00b6 The official Dgraph database client implementation for Python. More about Pydgaph DQL Schema and Type System \u00b6 Schema Types \u00b6 Scalar Types \u00b6 default int float string bool dateTime geo password UID Type \u00b6 uid Type System \u00b6 Dgraph supports a type system that can be used to categorize nodes and query them based on their type. The type system is also used during expand queries. example: type Student { name dob home_address year friends } Types are declared along with the schema using the Alter endpoint. In order to properly support the above type, a predicate for each of the attributes in the type is also needed, such as: name: string @index(term) . dob: datetime . home_address: string . year: int . friends: [uid] . Query \u00b6 A DQL query finds nodes based on search criteria, matches patterns in a graph and returns a graph as a result. A query is composed of nested blocks, starting with a query root. The root finds the initial set of nodes against which the following graph matching and filtering is applied. Facets and Edge Attributes in DQL \u00b6 Dgraph supports facets \u2014 key value pairs on edges \u2014 as an extension to RDF triples. That is, facets add properties to edges, rather than to nodes. Facets can also be used as weights for edges. More about facets. Functions with DQL \u00b6 Dgraph Query Language (DQL) functions allow filtering based on properties of nodes or variables. Functions can be applied in the query root or in filters. More about fucntions. Comparison functions: - le: less than or equal to - lt: less than - ge: greater than or equal to - gt: greater than","title":"More on Dgraph"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#more-on-dgraph","text":"","title":"More on Dgraph"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#graphql","text":"Dgraph's Generated API.","title":"GraphQL"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#dql","text":"Dgraph's Query Language.","title":"DQL"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#pydgraph","text":"The official Dgraph database client implementation for Python. More about Pydgaph","title":"Pydgraph"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#dql-schema-and-type-system","text":"","title":"DQL Schema and Type System"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#schema-types","text":"","title":"Schema Types"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#scalar-types","text":"default int float string bool dateTime geo password","title":"Scalar Types"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#uid-type","text":"uid","title":"UID Type"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#type-system","text":"Dgraph supports a type system that can be used to categorize nodes and query them based on their type. The type system is also used during expand queries. example: type Student { name dob home_address year friends } Types are declared along with the schema using the Alter endpoint. In order to properly support the above type, a predicate for each of the attributes in the type is also needed, such as: name: string @index(term) . dob: datetime . home_address: string . year: int . friends: [uid] .","title":"Type System"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#query","text":"A DQL query finds nodes based on search criteria, matches patterns in a graph and returns a graph as a result. A query is composed of nested blocks, starting with a query root. The root finds the initial set of nodes against which the following graph matching and filtering is applied.","title":"Query"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#facets-and-edge-attributes-in-dql","text":"Dgraph supports facets \u2014 key value pairs on edges \u2014 as an extension to RDF triples. That is, facets add properties to edges, rather than to nodes. Facets can also be used as weights for edges. More about facets.","title":"Facets and Edge Attributes in DQL"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_built_in/#functions-with-dql","text":"Dgraph Query Language (DQL) functions allow filtering based on properties of nodes or variables. Functions can be applied in the query root or in filters. More about fucntions. Comparison functions: - le: less than or equal to - lt: less than - ge: greater than or equal to - gt: greater than","title":"Functions with DQL"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/","text":"Dgraph Client \u00b6 Schema \u00b6 We store chains, dexes, pairs, tokens and error messages in dgraph. Predicates: \u00b6 chain_id: int @index(int) . address: string @index(hash) . name: string @index(exact) . balance: string . allowance: string . is_verified: bool @index(bool). default: string . symbol: string . price: string . burn_rate: int . popularity: int . factory: string . enabled: bool . pair_count: int . protocol: string @index(exact) . factory_uid: string . token_addresses: string . decimals: string . decimal: int . token_symbols: string . reserves: string @index(exact) . reserves_init: string @index(exact) . lp_fee: string . tr_fee: string . hash: string . A: string . N_COINS: string . rates: string . swap_fee: string . swap_enabled: bool . weights: string . mid_price: string . oracle_price: string . K: string . R: int . lpFeeRate: string . mtFeeRate: string . slip_fee: string . msg : string . status_code : int . time: string . ftomToken : string . toToken : string . chainId: int . walletAddress: string . slippage: int . amount_out: string . amount_in: string . with_draw: bool . timestamp: dateTime . Edges: \u00b6 chain: uid @reverse . tokens: [uid] @reverse . pairs: [uid] @reverse . Facets: \u00b6 p0: [uid] @reverse . p1: [uid] @reverse . p2: [uid] @reverse . p3: [uid] @reverse . p4: [uid] @reverse . p5: [uid] @reverse . p6: [uid] @reverse . p7: [uid] @reverse . p8: [uid] @reverse . p9: [uid] @reverse . p10: [uid] @reverse . p11: [uid] @reverse . p12: [uid] @reverse . p13: [uid] @reverse . p14: [uid] @reverse . p15: [uid] @reverse . p16: [uid] @reverse . p17: [uid] @reverse . p18: [uid] @reverse . p19: [uid] @reverse . Types: \u00b6 type Chain { chain_id } type Token { chain address name balance allowance is_verified decimal default symbol pairs price prices burn_rate popularity p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 } type Dex { chain factory enabled name pair_count protocol } type Pair { chain address balance allowance is_verified default enabled symbol protocol factory factory_uid tokens token_addresses decimals token_symbols reserves reserves_init lp_fee tr_fee hash A N_COINS rates swap_fee swap_enabled weights mid_price oracle_price K R lpFeeRate mtFeeRate slip_fee p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 } type Error { time msg status_code ftomToken toToken chainId walletAddress slippage amount_out amount_in with_draw } Client \u00b6 class GraphqlClient : \"\"\"A class inspired by MongoClient, connects to Dgraph Database via given Url, port and authentication credentials \"\"\" def __init__ ( self , host : str , alpha_port : str , ratel_port : str , username : str = None , password : str = None , namespace : str = None ) -> None : self . alpha_port = alpha_port self . ratel_port = ratel_port self . host = host self . username = username self . password = password self . namespace = namespace client_stub = pydgraph . DgraphClientStub ( f ' { host } : { ratel_port } ' ) self . client = pydgraph . DgraphClient ( client_stub ) if username and password and namespace : self . client . login_into_namespace ( username , password , namespace ) Mutate \u00b6 To insert or delete data we must use pydgraph's mutate function. async def mutate_dgraph ( self , data : Union [ Dict , List , str ], delete : bool = False ): res = None try : txn = self . client . txn () if delete : txn . mutate ( del_nquads = data ) else : res = txn . mutate ( set_obj = data ) txn . commit () except _InactiveRpcError as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } ' ) finally : txn . discard () if res : return res Query \u00b6 To query data we must use pydgraph's query function. async def query_dgraph ( self , query_text : str ): try : txn = self . client . txn ( read_only = True ) res = txn . query ( query_text ) data = json . loads ( res . json ) . get ( \"query\" ) return data except Exception as e : logging . exception ( f ' { datetime . now () : %H:%M:%S } - { e } at query_dgraph' )","title":"Dgraph Client"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#dgraph-client","text":"","title":"Dgraph Client"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#schema","text":"We store chains, dexes, pairs, tokens and error messages in dgraph.","title":"Schema"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#predicates","text":"chain_id: int @index(int) . address: string @index(hash) . name: string @index(exact) . balance: string . allowance: string . is_verified: bool @index(bool). default: string . symbol: string . price: string . burn_rate: int . popularity: int . factory: string . enabled: bool . pair_count: int . protocol: string @index(exact) . factory_uid: string . token_addresses: string . decimals: string . decimal: int . token_symbols: string . reserves: string @index(exact) . reserves_init: string @index(exact) . lp_fee: string . tr_fee: string . hash: string . A: string . N_COINS: string . rates: string . swap_fee: string . swap_enabled: bool . weights: string . mid_price: string . oracle_price: string . K: string . R: int . lpFeeRate: string . mtFeeRate: string . slip_fee: string . msg : string . status_code : int . time: string . ftomToken : string . toToken : string . chainId: int . walletAddress: string . slippage: int . amount_out: string . amount_in: string . with_draw: bool . timestamp: dateTime .","title":"Predicates:"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#edges","text":"chain: uid @reverse . tokens: [uid] @reverse . pairs: [uid] @reverse .","title":"Edges:"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#facets","text":"p0: [uid] @reverse . p1: [uid] @reverse . p2: [uid] @reverse . p3: [uid] @reverse . p4: [uid] @reverse . p5: [uid] @reverse . p6: [uid] @reverse . p7: [uid] @reverse . p8: [uid] @reverse . p9: [uid] @reverse . p10: [uid] @reverse . p11: [uid] @reverse . p12: [uid] @reverse . p13: [uid] @reverse . p14: [uid] @reverse . p15: [uid] @reverse . p16: [uid] @reverse . p17: [uid] @reverse . p18: [uid] @reverse . p19: [uid] @reverse .","title":"Facets:"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#types","text":"type Chain { chain_id } type Token { chain address name balance allowance is_verified decimal default symbol pairs price prices burn_rate popularity p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 } type Dex { chain factory enabled name pair_count protocol } type Pair { chain address balance allowance is_verified default enabled symbol protocol factory factory_uid tokens token_addresses decimals token_symbols reserves reserves_init lp_fee tr_fee hash A N_COINS rates swap_fee swap_enabled weights mid_price oracle_price K R lpFeeRate mtFeeRate slip_fee p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16 p17 p18 p19 } type Error { time msg status_code ftomToken toToken chainId walletAddress slippage amount_out amount_in with_draw }","title":"Types:"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#client","text":"class GraphqlClient : \"\"\"A class inspired by MongoClient, connects to Dgraph Database via given Url, port and authentication credentials \"\"\" def __init__ ( self , host : str , alpha_port : str , ratel_port : str , username : str = None , password : str = None , namespace : str = None ) -> None : self . alpha_port = alpha_port self . ratel_port = ratel_port self . host = host self . username = username self . password = password self . namespace = namespace client_stub = pydgraph . DgraphClientStub ( f ' { host } : { ratel_port } ' ) self . client = pydgraph . DgraphClient ( client_stub ) if username and password and namespace : self . client . login_into_namespace ( username , password , namespace )","title":"Client"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#mutate","text":"To insert or delete data we must use pydgraph's mutate function. async def mutate_dgraph ( self , data : Union [ Dict , List , str ], delete : bool = False ): res = None try : txn = self . client . txn () if delete : txn . mutate ( del_nquads = data ) else : res = txn . mutate ( set_obj = data ) txn . commit () except _InactiveRpcError as e : logging . error ( f ' { datetime . now () : %H:%M:%S } - { e } ' ) finally : txn . discard () if res : return res","title":"Mutate"},{"location":"Services/Aggregator/Databases/Dgraph/dgraph_client/#query","text":"To query data we must use pydgraph's query function. async def query_dgraph ( self , query_text : str ): try : txn = self . client . txn ( read_only = True ) res = txn . query ( query_text ) data = json . loads ( res . json ) . get ( \"query\" ) return data except Exception as e : logging . exception ( f ' { datetime . now () : %H:%M:%S } - { e } at query_dgraph' )","title":"Query"},{"location":"Services/Aggregator/Databases/Redis/","text":"Redis \u00b6 An open source (BSD licensed), in-memory data structure store used as a database, cache, message broker, and streaming engine. Read more.","title":"Redis"},{"location":"Services/Aggregator/Databases/Redis/#redis","text":"An open source (BSD licensed), in-memory data structure store used as a database, cache, message broker, and streaming engine. Read more.","title":"Redis"},{"location":"Services/Aggregator/Databases/Redis/redis_client/","text":"Redis Client \u00b6","title":"Redis Client"},{"location":"Services/Aggregator/Databases/Redis/redis_client/#redis-client","text":"","title":"Redis Client"},{"location":"Services/Aggregator/Logic/best_path/","text":"Best Path \u00b6 This module's purpose is to find the best pair with the highest output amount and calculates amount out for it. Find pair and amount out \u00b6 Finds the best pair for tokenA -> tokenB and calculates amount out for it. async def find_pair_and_amount_out ( chain_id : ChainId , token_in : Token , token_out : Token , amount_in : WithDecimal , ): pair = await find_and_cache_the_right_pair ( chain_id , token_in , token_out , amount_in ) if not pair : return None , None amount_out = calculate_amount_out_for_pair ( pair_obj = pair , token_in = token_in , token_out = token_out , amount_in = amount_in ) return amount_out , pair Find and cache the best pair \u00b6 Using tokenA and amount in, calculates the number of edge (pn). def calculate_number_of_edge ( token_in : Token , amount_in : WithDecimal ): try : amount_in = convert_to_ftm ( amount_in , token_in ) amount_in : NoDecimal = amount_in / 10 ** token_in . decimal power = round ( math . log ( amount_in , 2 )) power = max ( power , 0 ) edge = f \"p { power } \" return edge except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_number_of_edge.\" ) Finds and caches the best pair. Finding pair is implemented in 3 different ways: 1. Redis 2. Facets 3. Common pairs Find the best pair from redis \u00b6 After finding the best pair for exchanging tokenA -> tokenB for a specified amount, we cache the pair uid in redis (with a deadline), so while finding the best pair, first thing to do is to check redis and search for it. Gets pair's uid from redis. Makes the Pair object. Syncs pair. Returns the updated pair. In each function, after finding the best pair, we cache it's uid in redis. async def get_the_best_pair_from_redis ( chain_id : ChainId , token_in : Token , token_out : Token , edge : str ): try : pair_uid = get_chosen_pair ( chain_id , token_in . uid , token_out . uid , edge ) if pair_uid not in ( None , \"0\" ): logging . debug ( \"Couldn't find the best pair in redis at get_the_best_pair_from_redis.\" ) pair_obj = await dgraph_client () . find_by_uid ( pair_uid , [ \"uid\" , \"expand(_all_)\" ] ) pair_obj = await make_pair_obj ( pair_obj , chain_id ) pair_obj = await sync_pairs ( chain_id , [ pair_obj ]) return pair_obj [ 0 ] except Exception as e : logging . error ( f \"Exception { e } occurred at get_the_best_pair_from_redis.\" ) return None Find the best pair by facets \u00b6 While updating pairs, we check the \"hash\" predicate of them and compare it to old hashes. If there was a difference between the old hash and the new hash (it means pairs' reserves have been changed), we save their uid as \"changed pairs\" in redis and we set facets on them. In order to find the best pair to exchange tokenA -> tokenB, if we couldn't find the pair in redis, we query dgraph and check facets, and choose the pair with the highest one. Queries dgraph and finds the pair with highest facets . Makes the Pair object. Syncs pair. Returns the updated pair. async def get_the_best_pair_by_facets ( chain_id : ChainId , token_in : Token , token_out : Token , edge ): try : token_in_uid = token_in . uid token_out_uid = token_out . uid response = await dgraph_client () . depth_one_best_path ( uid0 = token_in_uid , uid1 = token_out_uid , relation = edge , pair_predicates = [ \"uid\" , \"expand(_all_)\" ], ) if not response : logging . debug ( f \"There are no facests between token_in: { token_in_uid } and token_out: { token_out_uid } at get_the_best_pair_by_facets.\" ) return None pair = response [ 0 ] . get ( edge )[ 0 ] pair_obj = await make_pair_obj ( pair , chain_id ) pair_obj = await sync_pairs ( chain_id , [ pair_obj ]) return pair_obj [ 0 ] Dgraph query {query(func: uid(uid0)){ {edge} @facets(orderdesc: uid1) { uid } } } Find the best pair by common pairs \u00b6 Finds common pairs between tokenA and tokenB. Sync all of found pairs. Calculates amount out for all of updated pairs. Returns the pairs with the highest amount out. async def get_the_best_pair_by_common_pairs ( chain_id : ChainId , token_in : Token , token_out : Token , amount_in : WithDecimal ): try : pair_uids = await dgraph_client () . common_pairs ( token_in . uid , token_out . uid ) if pair_uids is None : logging . debug ( f \"There are no common pairs between token_in: { token_in . uid } and token_out: { token_out . uid } at get_the_best_pair_by_common_pairs.\" ) return None pair_uids = pair_uids . get ( \"pairs\" ) uids = [ uid . get ( \"uid\" ) for uid in pair_uids ] updated_pairs = await sync_pairs ( chain_id , uids ) logging . debug ( f \"Synced { len ( updated_pairs ) } pairs at get_the_best_pair_by_common_pairs.\" ) pairs = [] for pair in updated_pairs : amount_out = calculate_amount_out_for_pair ( pair_obj = pair , token_in = token_in , token_out = token_out , amount_in = amount_in ) if amount_out is None : continue pairs . append ({ \"amount_out\" : amount_out , \"pair\" : pair }) if len ( pairs ) > 0 : pairs = sorted ( pairs , key = lambda d : d [ 'amount_out' ]) return pairs [ 0 ] . get ( \"pair\" ) except Exception as e : logging . error ( f \"Exception { e } occurred at get_the_best_pair_by_common_pairs.\" ) return None Calculates the amount out for pair. Calls Pair's amount_out method. def calculate_amount_out_for_pair ( token_in : Token , token_out : Token , amount_in : WithDecimal , pair_obj : Pair ): try : for i , token in enumerate ( pair_obj . token_addresses ): if token == token_in . address : index1 = i for i , token in enumerate ( pair_obj . token_addresses ): if token == token_in . address : index1 = i if token == token_out . address : index2 = i amount_out = pair_obj . amount_out ( index1 , index2 , amount_in ) if amount_out == 0 : return None return amount_out except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_amount_out_for_pair.\" )","title":"Best Path"},{"location":"Services/Aggregator/Logic/best_path/#best-path","text":"This module's purpose is to find the best pair with the highest output amount and calculates amount out for it.","title":"Best Path"},{"location":"Services/Aggregator/Logic/best_path/#find-pair-and-amount-out","text":"Finds the best pair for tokenA -> tokenB and calculates amount out for it. async def find_pair_and_amount_out ( chain_id : ChainId , token_in : Token , token_out : Token , amount_in : WithDecimal , ): pair = await find_and_cache_the_right_pair ( chain_id , token_in , token_out , amount_in ) if not pair : return None , None amount_out = calculate_amount_out_for_pair ( pair_obj = pair , token_in = token_in , token_out = token_out , amount_in = amount_in ) return amount_out , pair","title":"Find pair and amount out"},{"location":"Services/Aggregator/Logic/best_path/#find-and-cache-the-best-pair","text":"Using tokenA and amount in, calculates the number of edge (pn). def calculate_number_of_edge ( token_in : Token , amount_in : WithDecimal ): try : amount_in = convert_to_ftm ( amount_in , token_in ) amount_in : NoDecimal = amount_in / 10 ** token_in . decimal power = round ( math . log ( amount_in , 2 )) power = max ( power , 0 ) edge = f \"p { power } \" return edge except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_number_of_edge.\" ) Finds and caches the best pair. Finding pair is implemented in 3 different ways: 1. Redis 2. Facets 3. Common pairs","title":"Find and cache the best pair"},{"location":"Services/Aggregator/Logic/best_path/#find-the-best-pair-from-redis","text":"After finding the best pair for exchanging tokenA -> tokenB for a specified amount, we cache the pair uid in redis (with a deadline), so while finding the best pair, first thing to do is to check redis and search for it. Gets pair's uid from redis. Makes the Pair object. Syncs pair. Returns the updated pair. In each function, after finding the best pair, we cache it's uid in redis. async def get_the_best_pair_from_redis ( chain_id : ChainId , token_in : Token , token_out : Token , edge : str ): try : pair_uid = get_chosen_pair ( chain_id , token_in . uid , token_out . uid , edge ) if pair_uid not in ( None , \"0\" ): logging . debug ( \"Couldn't find the best pair in redis at get_the_best_pair_from_redis.\" ) pair_obj = await dgraph_client () . find_by_uid ( pair_uid , [ \"uid\" , \"expand(_all_)\" ] ) pair_obj = await make_pair_obj ( pair_obj , chain_id ) pair_obj = await sync_pairs ( chain_id , [ pair_obj ]) return pair_obj [ 0 ] except Exception as e : logging . error ( f \"Exception { e } occurred at get_the_best_pair_from_redis.\" ) return None","title":"Find the best pair from redis"},{"location":"Services/Aggregator/Logic/best_path/#find-the-best-pair-by-facets","text":"While updating pairs, we check the \"hash\" predicate of them and compare it to old hashes. If there was a difference between the old hash and the new hash (it means pairs' reserves have been changed), we save their uid as \"changed pairs\" in redis and we set facets on them. In order to find the best pair to exchange tokenA -> tokenB, if we couldn't find the pair in redis, we query dgraph and check facets, and choose the pair with the highest one. Queries dgraph and finds the pair with highest facets . Makes the Pair object. Syncs pair. Returns the updated pair. async def get_the_best_pair_by_facets ( chain_id : ChainId , token_in : Token , token_out : Token , edge ): try : token_in_uid = token_in . uid token_out_uid = token_out . uid response = await dgraph_client () . depth_one_best_path ( uid0 = token_in_uid , uid1 = token_out_uid , relation = edge , pair_predicates = [ \"uid\" , \"expand(_all_)\" ], ) if not response : logging . debug ( f \"There are no facests between token_in: { token_in_uid } and token_out: { token_out_uid } at get_the_best_pair_by_facets.\" ) return None pair = response [ 0 ] . get ( edge )[ 0 ] pair_obj = await make_pair_obj ( pair , chain_id ) pair_obj = await sync_pairs ( chain_id , [ pair_obj ]) return pair_obj [ 0 ] Dgraph query {query(func: uid(uid0)){ {edge} @facets(orderdesc: uid1) { uid } } }","title":"Find the best pair by facets"},{"location":"Services/Aggregator/Logic/best_path/#find-the-best-pair-by-common-pairs","text":"Finds common pairs between tokenA and tokenB. Sync all of found pairs. Calculates amount out for all of updated pairs. Returns the pairs with the highest amount out. async def get_the_best_pair_by_common_pairs ( chain_id : ChainId , token_in : Token , token_out : Token , amount_in : WithDecimal ): try : pair_uids = await dgraph_client () . common_pairs ( token_in . uid , token_out . uid ) if pair_uids is None : logging . debug ( f \"There are no common pairs between token_in: { token_in . uid } and token_out: { token_out . uid } at get_the_best_pair_by_common_pairs.\" ) return None pair_uids = pair_uids . get ( \"pairs\" ) uids = [ uid . get ( \"uid\" ) for uid in pair_uids ] updated_pairs = await sync_pairs ( chain_id , uids ) logging . debug ( f \"Synced { len ( updated_pairs ) } pairs at get_the_best_pair_by_common_pairs.\" ) pairs = [] for pair in updated_pairs : amount_out = calculate_amount_out_for_pair ( pair_obj = pair , token_in = token_in , token_out = token_out , amount_in = amount_in ) if amount_out is None : continue pairs . append ({ \"amount_out\" : amount_out , \"pair\" : pair }) if len ( pairs ) > 0 : pairs = sorted ( pairs , key = lambda d : d [ 'amount_out' ]) return pairs [ 0 ] . get ( \"pair\" ) except Exception as e : logging . error ( f \"Exception { e } occurred at get_the_best_pair_by_common_pairs.\" ) return None Calculates the amount out for pair. Calls Pair's amount_out method. def calculate_amount_out_for_pair ( token_in : Token , token_out : Token , amount_in : WithDecimal , pair_obj : Pair ): try : for i , token in enumerate ( pair_obj . token_addresses ): if token == token_in . address : index1 = i for i , token in enumerate ( pair_obj . token_addresses ): if token == token_in . address : index1 = i if token == token_out . address : index2 = i amount_out = pair_obj . amount_out ( index1 , index2 , amount_in ) if amount_out == 0 : return None return amount_out except Exception as e : logging . error ( f \"Exception { e } occurred at calculate_amount_out_for_pair.\" )","title":"Find the best pair by common pairs"},{"location":"Services/Aggregator/Logic/find_and_swap/","text":"Find and Swap \u00b6 Finds the best route between fromToken and toToken and returns the path and amount out, then builds the transaction and caches it. Returns find's result, transaction raw data and proxy's inputs (swap struct.) Checks if fromToken or toToken are network's value token (FTM in Fantom Network) if fromToken ( should_wrap ) == network's value token: calls find_native_to_wrap if toToken ( should_unwrap ) == network's value token: calls find_wrap_to_native else: calls _init_find Builds transaction Saves transaction in redis async def find_and_swap ( request : PathRequest ): chain , _from , _to = ( request . _chain , request . fromToken , request . toToken ) swap_struct , res = None , None if should_wrap ( chain , _from , _to ): swap_struct , res = await find_native_to_wrap ( request . _chain , request . _amount_in , request . deadline ) elif should_unwrap ( chain , _from , _to ): swap_struct , res = await find_wrap_to_native ( request . _chain , request . _amount_in , request . deadline ) else : swap_struct , res = await _init_find ( request ) trx_result = build_transaction ( request , swap_struct ) save_transaction ( request . chainId , request . walletAddress , request . _from_token . symbol , request . _to_token . symbol , request . amount_in , trx_result , ) return res , trx_result , swap_struct","title":"Find and Swap"},{"location":"Services/Aggregator/Logic/find_and_swap/#find-and-swap","text":"Finds the best route between fromToken and toToken and returns the path and amount out, then builds the transaction and caches it. Returns find's result, transaction raw data and proxy's inputs (swap struct.) Checks if fromToken or toToken are network's value token (FTM in Fantom Network) if fromToken ( should_wrap ) == network's value token: calls find_native_to_wrap if toToken ( should_unwrap ) == network's value token: calls find_wrap_to_native else: calls _init_find Builds transaction Saves transaction in redis async def find_and_swap ( request : PathRequest ): chain , _from , _to = ( request . _chain , request . fromToken , request . toToken ) swap_struct , res = None , None if should_wrap ( chain , _from , _to ): swap_struct , res = await find_native_to_wrap ( request . _chain , request . _amount_in , request . deadline ) elif should_unwrap ( chain , _from , _to ): swap_struct , res = await find_wrap_to_native ( request . _chain , request . _amount_in , request . deadline ) else : swap_struct , res = await _init_find ( request ) trx_result = build_transaction ( request , swap_struct ) save_transaction ( request . chainId , request . walletAddress , request . _from_token . symbol , request . _to_token . symbol , request . amount_in , trx_result , ) return res , trx_result , swap_struct","title":"Find and Swap"},{"location":"Services/Aggregator/Logic/init_find/","text":"Init Find \u00b6 In order to find the best possible route between two tokens, we try 3 different ways by calling Path.best_path : 1. Depth one: Searches for pairs containing both tokens in dgraph. 2. Depth two: Searches for pairs containing fromToken and base tokens, if found any, tries to find common pairs between base tokens and toToken. 3. Depth three: Searches for pairs containing fromToken and base tokens, if found any, tries to find common pairs between those pairs and other base tokens, then tries to find common pairs between those tokens and toToken. 4. For each depth, after finding routes, calculates amount outs and returns the path with the higher amount out. 5. Compares amount out of depth one, depth two, and depth three and returns the path with the higher one. 6. Checks if the final amount out is higher than the minimum amount and raises NotExpectableAmountOut error. 7. Checks the slippage rate. 8. Depends on the depth of output path, calls find_one , find_two or find_three and returns proxy's inputs (swap struct) and find response. async def _init_find ( request : PathRequest ) -> Tuple [ SwapStruct , dict ]: res = await Path ( request . chainId , request . _from_token , request . _to_token , request . _amount_in ) . best_path () amounts_in_1 , amount_out_1 , pairs_1 = res [ 0 ] amounts_in_2 , amount_out_2 , paths_2 = res [ 1 ] amounts_in_3 , amount_out_3 , paths_3 , first_step , second_step = res [ 2 ] amount_outs = [] if amount_out_1 : amount_outs . append ( amount_out_1 ) if amount_out_2 : amount_outs . append ( amount_out_2 ) if amount_out_3 : amount_outs . append ( amount_out_3 ) if amount_outs : amount_out = max ( amount_outs [:]) flags = make_flags ( request = request ) else : raise Errors . NoPathFound () if amount_out + cc . AMOUNT_OUT_THRESHOLD < cc . AMOUNT_OUT_MINIMUM : raise Errors . NotExpectableAmountOut ( request = request , amount_out = amount_out ) if int ( amount_out - ( amount_out * request . slippage * 0.01 )) < 0 : raise Errors . LowSlippageRate ( request = request ) if amount_out == amount_out_1 : res , swap_struct = await find_d1 ( request , amounts_in_1 , amount_out , pairs_1 , flags ) if amount_out == amount_out_2 : res , swap_struct = await find_d2 ( request , amount_out , paths_2 , flags ) if amount_out == amount_out_3 : res , swap_struct = await find_d3 ( request , amount_out , paths_3 , first_step , second_step , flags ) return swap_struct , res","title":"Init Find"},{"location":"Services/Aggregator/Logic/init_find/#init-find","text":"In order to find the best possible route between two tokens, we try 3 different ways by calling Path.best_path : 1. Depth one: Searches for pairs containing both tokens in dgraph. 2. Depth two: Searches for pairs containing fromToken and base tokens, if found any, tries to find common pairs between base tokens and toToken. 3. Depth three: Searches for pairs containing fromToken and base tokens, if found any, tries to find common pairs between those pairs and other base tokens, then tries to find common pairs between those tokens and toToken. 4. For each depth, after finding routes, calculates amount outs and returns the path with the higher amount out. 5. Compares amount out of depth one, depth two, and depth three and returns the path with the higher one. 6. Checks if the final amount out is higher than the minimum amount and raises NotExpectableAmountOut error. 7. Checks the slippage rate. 8. Depends on the depth of output path, calls find_one , find_two or find_three and returns proxy's inputs (swap struct) and find response. async def _init_find ( request : PathRequest ) -> Tuple [ SwapStruct , dict ]: res = await Path ( request . chainId , request . _from_token , request . _to_token , request . _amount_in ) . best_path () amounts_in_1 , amount_out_1 , pairs_1 = res [ 0 ] amounts_in_2 , amount_out_2 , paths_2 = res [ 1 ] amounts_in_3 , amount_out_3 , paths_3 , first_step , second_step = res [ 2 ] amount_outs = [] if amount_out_1 : amount_outs . append ( amount_out_1 ) if amount_out_2 : amount_outs . append ( amount_out_2 ) if amount_out_3 : amount_outs . append ( amount_out_3 ) if amount_outs : amount_out = max ( amount_outs [:]) flags = make_flags ( request = request ) else : raise Errors . NoPathFound () if amount_out + cc . AMOUNT_OUT_THRESHOLD < cc . AMOUNT_OUT_MINIMUM : raise Errors . NotExpectableAmountOut ( request = request , amount_out = amount_out ) if int ( amount_out - ( amount_out * request . slippage * 0.01 )) < 0 : raise Errors . LowSlippageRate ( request = request ) if amount_out == amount_out_1 : res , swap_struct = await find_d1 ( request , amounts_in_1 , amount_out , pairs_1 , flags ) if amount_out == amount_out_2 : res , swap_struct = await find_d2 ( request , amount_out , paths_2 , flags ) if amount_out == amount_out_3 : res , swap_struct = await find_d3 ( request , amount_out , paths_3 , first_step , second_step , flags ) return swap_struct , res","title":"Init Find"},{"location":"Services/Aggregator/Logic/middleware/","text":"Middleware \u00b6 In order to check if request \u200chas the proper elements it has to pass to through the middleware. Populate chain Middleware \u00b6 Wallet Address Checks if request.walletAddress exist and is type of Address. def check_wallet_address ( request : PathRequest ): if request . walletAddress is not None : try : request . walletAddress = Address ( request . walletAddress ) except ValueError : raise Errors . WalletConnection ( request = request ) Chain Checks if request.chain_id is in supported chains and if yes,creates request._chain(Chain object). def check_and_make_chain ( chain_id : ChainId ): if chain_id not in Network . _SUPPORTED_CHAINES (): raise Errors . ChainIdNotSupported chain = Chain ( ** { \"chain_id\" : chain_id }) return chain Checks if request.fromToken and request.toToken are network_value_token(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE\") or not. network = Network ( chain_id ) if token_address == network . network_value_address : return network . network_value_wrapped_address return token_address Token object Tries to find token in redis using it's identifier(address), if it couldn't find token, tries to find it in dgraph, then makes the Token object. raises Errors.TokenNotFound if couldn't find token async def make_token_obj ( token_address : Address , chain_id : ChainId ) -> Token : token = Token . get_cache ( key = token_address , chain_id = chain_id ) if token is not None : return token token = await dgraph_client () . find_by_predicate ( ( \"address\" , token_address ), [ \"uid\" , \"expand(_all_)\" ]) if len ( token ) >= 1 : token = token [ 0 ] logging . warning ( f \"Token { token_address } doesn't exist in redis db at make_token_obj.\" ) return make_token ( token , chain_id ) raise Errors . TokenNotFound () Amount Creates request._amount_in and request.amount_out by multiplying amount and 10 ** token.decimal. raises Errors.LowAmountIn if couldn't find any amount. def check_amounts ( request : PathRequest ): if request . amount_in : request . _amount_in = int ( request . amount_in * 10 ** request . _from_token . decimal ) if request . amount_out : request . _amount_out = int ( request . amount_out * 10 ** request . _to_token . decimal ) if not request . _amount_in and not request . _amount_out : raise Errors . LowAmountIn ( request = request ) return request Value Checks if request.fromToken is network_value_address. If yes, changes request. def network_value_middleware ( request : PathRequest ) -> PathRequest : network = Network ( request . chainId ) if request . fromToken == network . network_value_address : request . value = request . _amount_in return request Burn rate Checks if from_token or to_token has burn_rate def set_burn_rate_flag ( request : PathRequest ) -> bool : if request . _burning_detail : if request . _burning_detail . from_token_burn_rate not in ( None , 0 ) or \\ request . _burning_detail . to_token_burn_rate not in ( None , 0 ): return True if request . _from_token . burn_rate not in ( None , 0 ) or \\ request . _to_token . burn_rate not in ( None , 0 ): return True if request . _has_burn_rate : return True return False Balance Checks if user has enough balance in their wallet. async def check_balance ( request : PathRequest ) -> PathRequest chain = request . _chain try : _token_in , _native_token = ( chain . batch_reader_contract . functions . multiGetInfo ( [ request . _from_token . address ], chain . proxy_address , request . walletAddress ) . call () ) has_balance = (( _token_in [ 1 ] >= request . _amount_in ) and ( _native_token [ 1 ] >= 0.2 * 10 ** 18 )) except Exception as e : logging . error ( f \"at check balance middleware: { e } \" ) print ( f \"at check balance middleware: { e } \" ) return return has_balance","title":"Middleware"},{"location":"Services/Aggregator/Logic/middleware/#middleware","text":"In order to check if request \u200chas the proper elements it has to pass to through the middleware.","title":"Middleware"},{"location":"Services/Aggregator/Logic/middleware/#populate-chain-middleware","text":"Wallet Address Checks if request.walletAddress exist and is type of Address. def check_wallet_address ( request : PathRequest ): if request . walletAddress is not None : try : request . walletAddress = Address ( request . walletAddress ) except ValueError : raise Errors . WalletConnection ( request = request ) Chain Checks if request.chain_id is in supported chains and if yes,creates request._chain(Chain object). def check_and_make_chain ( chain_id : ChainId ): if chain_id not in Network . _SUPPORTED_CHAINES (): raise Errors . ChainIdNotSupported chain = Chain ( ** { \"chain_id\" : chain_id }) return chain Checks if request.fromToken and request.toToken are network_value_token(0xEeeeeEeeeEeEeeEeEeEeeEEEeeeeEeeeeeeeEEeE\") or not. network = Network ( chain_id ) if token_address == network . network_value_address : return network . network_value_wrapped_address return token_address Token object Tries to find token in redis using it's identifier(address), if it couldn't find token, tries to find it in dgraph, then makes the Token object. raises Errors.TokenNotFound if couldn't find token async def make_token_obj ( token_address : Address , chain_id : ChainId ) -> Token : token = Token . get_cache ( key = token_address , chain_id = chain_id ) if token is not None : return token token = await dgraph_client () . find_by_predicate ( ( \"address\" , token_address ), [ \"uid\" , \"expand(_all_)\" ]) if len ( token ) >= 1 : token = token [ 0 ] logging . warning ( f \"Token { token_address } doesn't exist in redis db at make_token_obj.\" ) return make_token ( token , chain_id ) raise Errors . TokenNotFound () Amount Creates request._amount_in and request.amount_out by multiplying amount and 10 ** token.decimal. raises Errors.LowAmountIn if couldn't find any amount. def check_amounts ( request : PathRequest ): if request . amount_in : request . _amount_in = int ( request . amount_in * 10 ** request . _from_token . decimal ) if request . amount_out : request . _amount_out = int ( request . amount_out * 10 ** request . _to_token . decimal ) if not request . _amount_in and not request . _amount_out : raise Errors . LowAmountIn ( request = request ) return request Value Checks if request.fromToken is network_value_address. If yes, changes request. def network_value_middleware ( request : PathRequest ) -> PathRequest : network = Network ( request . chainId ) if request . fromToken == network . network_value_address : request . value = request . _amount_in return request Burn rate Checks if from_token or to_token has burn_rate def set_burn_rate_flag ( request : PathRequest ) -> bool : if request . _burning_detail : if request . _burning_detail . from_token_burn_rate not in ( None , 0 ) or \\ request . _burning_detail . to_token_burn_rate not in ( None , 0 ): return True if request . _from_token . burn_rate not in ( None , 0 ) or \\ request . _to_token . burn_rate not in ( None , 0 ): return True if request . _has_burn_rate : return True return False Balance Checks if user has enough balance in their wallet. async def check_balance ( request : PathRequest ) -> PathRequest chain = request . _chain try : _token_in , _native_token = ( chain . batch_reader_contract . functions . multiGetInfo ( [ request . _from_token . address ], chain . proxy_address , request . walletAddress ) . call () ) has_balance = (( _token_in [ 1 ] >= request . _amount_in ) and ( _native_token [ 1 ] >= 0.2 * 10 ** 18 )) except Exception as e : logging . error ( f \"at check balance middleware: { e } \" ) print ( f \"at check balance middleware: { e } \" ) return return has_balance","title":"Populate chain Middleware"},{"location":"Services/Aggregator/Logic/path/","text":"Path \u00b6 A class that help us to find paths with depth of one, two and three between fromToken and toToken. Attributes: \u00b6 chain_id: ChainId token_in: Token token_out: Token amount_in: WithDecimal Methods: \u00b6 is_common common_tokens find_common_tokens depth_one depth_two depth_three best_path init \u00b6 def __init__ ( self , chain_id : ChainId , token_in : Token , token_out : Token , amount_in : WithDecimal ) -> None : self . chain_id = chain_id self . token_in = token_in self . token_out = token_out self . amount_in = amount_in self . client : GraphqlClient = dgraph_client () 1. is_common \u00b6 Takes to 2 tokens uid and checks if there are any common pairs between them. async def is_common ( self , token_uid : Uid , base_token : Uid ) -> bool : from_token_to_base = await self . client . common_pairs ( from_token = token_uid , to_token = base_token , pair_counts = 1 ) if from_token_to_base : return True return False Dgraph query for token_uid: \"0x1\" and base_token: \"0x2\": {query(func: uid(0x1)){ pairs _first @cascade { uid tokens @filter(uid(0x2)){ uid } } } } 2. common_tokens \u00b6 Checks if token_in has any common pairs with base tokens and returns a list of them. async def common_tokens ( self , token_uid : Uid , base_tokens : List [ Uid ] ) -> List [ Token ]: common = set () for uid in base_tokens : if uid == token_uid : continue if await self . is_common ( token_uid , base_token = uid ): common . add ( uid ) return list ( common ) 3. find_common_tokens \u00b6 Finds base tokens Tries to get base_tokens from redis and if couldn't find them, queries dgraph and gets fist 15 tokens which have the most pairs. Finds depth one common tokens Returns a list of base token which have common pairs with token_in. Finds depth two common tokens For each token in depth one tokens list, checks if that token and base tokens have any common pairs and returns a list of lists, containing depth_one_tokens[i] and base tokens which have common pairs with that. Example: token_in_uid = \"0x5\" base_tokens = [ \"0x1\" \"0x2\" , \"0x3\" , \"0x4\" , \"0x6\" ] first_tokens = [ \"0x1\" , \"0x3\" , \"0x6\" ] second_tokens = [ [ \"0x1\" , \"0x2\" ], [ \"0x3\" , \"0x4\" , \"0x5\" ], [ \"0x6\" , \"0x2\" ] ] async def find_common_tokens ( self ): BASE_TOKENS = get_base_tokens ( self . chain_id ) if not BASE_TOKENS : BASE_TOKENS = await find_base_tokens ( self . chain_id ) if not BASE_TOKENS : logging . error ( \"Couldn't find any base token at find_common_tokens.\" ) return [], [[]] depth_one_tokens = await self . common_tokens ( self . token_in . uid , BASE_TOKENS ) if len ( depth_one_tokens ) < 1 : logging . debug ( \"There are no common tokens between token_in and base tokens at find_common_tokens.\" ) return [], [[]] depth_two_common_tokens = [] for uid in depth_one_tokens : depth_two_common_tokens . append ( await self . common_tokens ( uid , BASE_TOKENS )) return depth_one_tokens , depth_two_common_tokens 4. depth_one \u00b6 Checks if token_in has any common pairs with token_out, returns None, None, None if couldn't found any. Calls Portioning.portion_best_path and returns amounts_in, amount_out and pairs. async def depth_one ( self ): try : if await self . is_common ( self . token_in . uid , self . token_out . uid ): first_amounts_in , first_amount_out , first_pairs = await Portioning ( self . chain_id , self . amount_in , self . token_in , self . token_out ) . portion_best_path () return first_amounts_in , first_amount_out , first_pairs logging . debug ( f \"Couldn't find any common pairs between token_in: { self . token_in . uid } and token_out: { self . token_out . uid } at Path.depth_one.\" ) return ( None ,) * 3 except Exception as e : logging . error ( f \"Exception { e } occurred at Path.depth_on\" ) return ( None ,) * 3 5. depth_two \u00b6 Takes: depth_one_tokens Returns: amounts_in amount_out pairs For depth_one_token in depth_one_tokens; checks if depth_one_token has common pairs with token_out, if yes, makes the depth_one_token's object and calls Portioning.portion_best_path() for: token_in depth_one_token amount_in Returns: first_amounts_in _amount_out first_pairs Calls Portioning.calculate_real_amount_out() for: depth_one_token token_out _amount_out Returns: last_amount_out last_pair Returns: (first_amounts_in, last_amounts_in) last_amount_out [{\"pairs\": last_pair, \"portion\": 100}] async def depth_two ( self , common_tokens : List [ Uid ] ): try : for uid in common_tokens : if await self . is_common ( uid , self . token_out . uid ): base_token = await make_token_obj ( Token . get_identifier ( self . chain_id , uid ), self . chain_id ) first_portioning = Portioning ( self . chain_id , self . amount_in , self . token_in , base_token ) first_amounts_in , _amount_out , first_pairs = await first_portioning . portion_best_path () if not _amount_out : continue last_portioning = Portioning ( self . chain_id , _amount_out , base_token , self . token_out ) last_amounts_in = [ _amount_out ] last_amount_out , last_pair = await last_portioning . calculate_real_amount_out ( _amount_out ) if last_amount_out in ( None , maximum ): continue last_pair = [{ \"pairs\" : last_pair , \"portion\" : 100 }] maximum = last_amount_out pairs = [ first_pairs , last_pair ] return ( first_amounts_in , last_amounts_in ), maximum , pairs if maximum == 0 : logging . debug ( f \"Couldn't find any path with depth two between token_in: { self . token_in . uid } and token_out: { self . token_out . uid } at Path.depth_two.\" ) return ( None ,) * 3 except Exception as e : logging . error ( f \"Exception { e } occurred at Path.depth_two\" ) return ( None ,) * 3 6. depth_three \u00b6 Takes: depth_one_tokens depth_two_tokens Returns: amounts_in amount_out pairs first_middle_token second_middle_token For each first_uid in depth_one_tokens, finds the common tokens between first_uid and depth_two_tokens[i]. For each second_uid in second_uids, checks if second_uid has common pairs with token_out and if couldn't find any continues. for i , first_uid in enumerate ( first_common_tokens ): second_uids = await self . common_tokens ( first_uid , second_common_tokens [ i ], ) if not second_uids : continue Makes first_uid's Token object. Makes second_uid's Token object. base_token_1 = await make_token_obj ( Token . get_identifier ( self . chain_id , first_uid ), self . chain_id ) base_token_2 = await make_token_obj ( Token . get_identifier ( self . chain_id , second_uid ), self . chain_id ) 5. Calls Portioning.portion_best_path() for: token_in base_token_1 amount_in Returns: - first_amounts_in - first_amount_out - first_pairs first_portioning = Portioning ( self . chain_id , self . amount_in , self . token_in , base_token_1 ) first_amounts_in , first_amount_out , first_pairs = await first_portioning . portion_best_path () first_step = base_token_1 if not first_amount_out : continue Calls **Portioning.calculate_real_amount_out() for: base_token_1 base_token_2 first_amount_out Returns: - middle_amount_out - middle_pair middle_portioning = Portioning ( self . chain_id , first_amount_out , base_token_1 , base_token_2 ) middle_amount_out , middle_pair = await middle_portioning . calculate_real_amount_out ( first_amount_out ) if not middle_amount_out : continue middle_amounts_in = [ first_amount_out ] middle_pairs = [{ \"pairs\" : middle_pair , \"portion\" : 100 }] second_step = base_token_2 Calls Portioning.calculate_real_amount_out() for: base_token_2 token_out middle_amount_out Returns: - last_amount_out - last_pair last_portioning = Portioning ( self . chain_id , middle_amount_out , base_token_2 , self . token_out ) last_amount_out , last_pair = await last_portioning . calculate_real_amount_out ( middle_amount_out ) if last_amount_out in ( None , maximum ): continue last_amounts_in = [ middle_amount_out ] last_pairs = [{ \"pairs\" : last_pair , \"portion\" : 100 }] if maximum > last_amount_out : continue maximum = last_amount_out amount_ins = [ first_amounts_in , middle_amounts_in , last_amounts_in ] pairs = [ first_pairs , middle_pairs , last_pairs ] Returns: - [first_amounts_in, middle_amounts_in, last_amounts_in] - last_amount_out - first_pairs, middle_pairs, last_pairs] - first_middle_token - second_middle_token return amount_ins , maximum , pairs , first_step , second_step 7. Best path \u00b6 Return a list containing: first amounts in, first amount out and first pairs for depth one. second amounts in, second amount out and second pairs for depth two. third amounts in, third amount out, third pairs, first middle token and second middle token for depth three. Raises Errors.NoPathFound() if couldn't find any path. async def best_path ( self ): depth_one_common_tokens , depth_two_common_tokens = await self . find_common_tokens () first_amounts_in , first_amount_out , first_pairs = await self . depth_one () second_amounts_in , second_amount_out , second_pairs = await self . depth_two ( depth_one_common_tokens ) third_amounts_in , third_amount_out , third_pairs , first_step , second_step = await self . depth_three ( depth_one_common_tokens , depth_two_common_tokens ) if first_pairs is None and second_pairs is None and third_pairs is None : raise Errors . NoPathFound () return [ ( first_amounts_in , first_amount_out , first_pairs ), ( second_amounts_in , second_amount_out , second_pairs ), ( third_amounts_in , third_amount_out , third_pairs , first_step , second_step ) ]","title":"Path"},{"location":"Services/Aggregator/Logic/path/#path","text":"A class that help us to find paths with depth of one, two and three between fromToken and toToken.","title":"Path"},{"location":"Services/Aggregator/Logic/path/#attributes","text":"chain_id: ChainId token_in: Token token_out: Token amount_in: WithDecimal","title":"Attributes:"},{"location":"Services/Aggregator/Logic/path/#methods","text":"is_common common_tokens find_common_tokens depth_one depth_two depth_three best_path","title":"Methods:"},{"location":"Services/Aggregator/Logic/path/#init","text":"def __init__ ( self , chain_id : ChainId , token_in : Token , token_out : Token , amount_in : WithDecimal ) -> None : self . chain_id = chain_id self . token_in = token_in self . token_out = token_out self . amount_in = amount_in self . client : GraphqlClient = dgraph_client ()","title":"init"},{"location":"Services/Aggregator/Logic/path/#1-is_common","text":"Takes to 2 tokens uid and checks if there are any common pairs between them. async def is_common ( self , token_uid : Uid , base_token : Uid ) -> bool : from_token_to_base = await self . client . common_pairs ( from_token = token_uid , to_token = base_token , pair_counts = 1 ) if from_token_to_base : return True return False Dgraph query for token_uid: \"0x1\" and base_token: \"0x2\": {query(func: uid(0x1)){ pairs _first @cascade { uid tokens @filter(uid(0x2)){ uid } } } }","title":"1. is_common"},{"location":"Services/Aggregator/Logic/path/#2-common_tokens","text":"Checks if token_in has any common pairs with base tokens and returns a list of them. async def common_tokens ( self , token_uid : Uid , base_tokens : List [ Uid ] ) -> List [ Token ]: common = set () for uid in base_tokens : if uid == token_uid : continue if await self . is_common ( token_uid , base_token = uid ): common . add ( uid ) return list ( common )","title":"2. common_tokens"},{"location":"Services/Aggregator/Logic/path/#3-find_common_tokens","text":"Finds base tokens Tries to get base_tokens from redis and if couldn't find them, queries dgraph and gets fist 15 tokens which have the most pairs. Finds depth one common tokens Returns a list of base token which have common pairs with token_in. Finds depth two common tokens For each token in depth one tokens list, checks if that token and base tokens have any common pairs and returns a list of lists, containing depth_one_tokens[i] and base tokens which have common pairs with that. Example: token_in_uid = \"0x5\" base_tokens = [ \"0x1\" \"0x2\" , \"0x3\" , \"0x4\" , \"0x6\" ] first_tokens = [ \"0x1\" , \"0x3\" , \"0x6\" ] second_tokens = [ [ \"0x1\" , \"0x2\" ], [ \"0x3\" , \"0x4\" , \"0x5\" ], [ \"0x6\" , \"0x2\" ] ] async def find_common_tokens ( self ): BASE_TOKENS = get_base_tokens ( self . chain_id ) if not BASE_TOKENS : BASE_TOKENS = await find_base_tokens ( self . chain_id ) if not BASE_TOKENS : logging . error ( \"Couldn't find any base token at find_common_tokens.\" ) return [], [[]] depth_one_tokens = await self . common_tokens ( self . token_in . uid , BASE_TOKENS ) if len ( depth_one_tokens ) < 1 : logging . debug ( \"There are no common tokens between token_in and base tokens at find_common_tokens.\" ) return [], [[]] depth_two_common_tokens = [] for uid in depth_one_tokens : depth_two_common_tokens . append ( await self . common_tokens ( uid , BASE_TOKENS )) return depth_one_tokens , depth_two_common_tokens","title":"3. find_common_tokens"},{"location":"Services/Aggregator/Logic/path/#4-depth_one","text":"Checks if token_in has any common pairs with token_out, returns None, None, None if couldn't found any. Calls Portioning.portion_best_path and returns amounts_in, amount_out and pairs. async def depth_one ( self ): try : if await self . is_common ( self . token_in . uid , self . token_out . uid ): first_amounts_in , first_amount_out , first_pairs = await Portioning ( self . chain_id , self . amount_in , self . token_in , self . token_out ) . portion_best_path () return first_amounts_in , first_amount_out , first_pairs logging . debug ( f \"Couldn't find any common pairs between token_in: { self . token_in . uid } and token_out: { self . token_out . uid } at Path.depth_one.\" ) return ( None ,) * 3 except Exception as e : logging . error ( f \"Exception { e } occurred at Path.depth_on\" ) return ( None ,) * 3","title":"4. depth_one"},{"location":"Services/Aggregator/Logic/path/#5-depth_two","text":"Takes: depth_one_tokens Returns: amounts_in amount_out pairs For depth_one_token in depth_one_tokens; checks if depth_one_token has common pairs with token_out, if yes, makes the depth_one_token's object and calls Portioning.portion_best_path() for: token_in depth_one_token amount_in Returns: first_amounts_in _amount_out first_pairs Calls Portioning.calculate_real_amount_out() for: depth_one_token token_out _amount_out Returns: last_amount_out last_pair Returns: (first_amounts_in, last_amounts_in) last_amount_out [{\"pairs\": last_pair, \"portion\": 100}] async def depth_two ( self , common_tokens : List [ Uid ] ): try : for uid in common_tokens : if await self . is_common ( uid , self . token_out . uid ): base_token = await make_token_obj ( Token . get_identifier ( self . chain_id , uid ), self . chain_id ) first_portioning = Portioning ( self . chain_id , self . amount_in , self . token_in , base_token ) first_amounts_in , _amount_out , first_pairs = await first_portioning . portion_best_path () if not _amount_out : continue last_portioning = Portioning ( self . chain_id , _amount_out , base_token , self . token_out ) last_amounts_in = [ _amount_out ] last_amount_out , last_pair = await last_portioning . calculate_real_amount_out ( _amount_out ) if last_amount_out in ( None , maximum ): continue last_pair = [{ \"pairs\" : last_pair , \"portion\" : 100 }] maximum = last_amount_out pairs = [ first_pairs , last_pair ] return ( first_amounts_in , last_amounts_in ), maximum , pairs if maximum == 0 : logging . debug ( f \"Couldn't find any path with depth two between token_in: { self . token_in . uid } and token_out: { self . token_out . uid } at Path.depth_two.\" ) return ( None ,) * 3 except Exception as e : logging . error ( f \"Exception { e } occurred at Path.depth_two\" ) return ( None ,) * 3","title":"5. depth_two"},{"location":"Services/Aggregator/Logic/path/#6-depth_three","text":"Takes: depth_one_tokens depth_two_tokens Returns: amounts_in amount_out pairs first_middle_token second_middle_token For each first_uid in depth_one_tokens, finds the common tokens between first_uid and depth_two_tokens[i]. For each second_uid in second_uids, checks if second_uid has common pairs with token_out and if couldn't find any continues. for i , first_uid in enumerate ( first_common_tokens ): second_uids = await self . common_tokens ( first_uid , second_common_tokens [ i ], ) if not second_uids : continue Makes first_uid's Token object. Makes second_uid's Token object. base_token_1 = await make_token_obj ( Token . get_identifier ( self . chain_id , first_uid ), self . chain_id ) base_token_2 = await make_token_obj ( Token . get_identifier ( self . chain_id , second_uid ), self . chain_id ) 5. Calls Portioning.portion_best_path() for: token_in base_token_1 amount_in Returns: - first_amounts_in - first_amount_out - first_pairs first_portioning = Portioning ( self . chain_id , self . amount_in , self . token_in , base_token_1 ) first_amounts_in , first_amount_out , first_pairs = await first_portioning . portion_best_path () first_step = base_token_1 if not first_amount_out : continue Calls **Portioning.calculate_real_amount_out() for: base_token_1 base_token_2 first_amount_out Returns: - middle_amount_out - middle_pair middle_portioning = Portioning ( self . chain_id , first_amount_out , base_token_1 , base_token_2 ) middle_amount_out , middle_pair = await middle_portioning . calculate_real_amount_out ( first_amount_out ) if not middle_amount_out : continue middle_amounts_in = [ first_amount_out ] middle_pairs = [{ \"pairs\" : middle_pair , \"portion\" : 100 }] second_step = base_token_2 Calls Portioning.calculate_real_amount_out() for: base_token_2 token_out middle_amount_out Returns: - last_amount_out - last_pair last_portioning = Portioning ( self . chain_id , middle_amount_out , base_token_2 , self . token_out ) last_amount_out , last_pair = await last_portioning . calculate_real_amount_out ( middle_amount_out ) if last_amount_out in ( None , maximum ): continue last_amounts_in = [ middle_amount_out ] last_pairs = [{ \"pairs\" : last_pair , \"portion\" : 100 }] if maximum > last_amount_out : continue maximum = last_amount_out amount_ins = [ first_amounts_in , middle_amounts_in , last_amounts_in ] pairs = [ first_pairs , middle_pairs , last_pairs ] Returns: - [first_amounts_in, middle_amounts_in, last_amounts_in] - last_amount_out - first_pairs, middle_pairs, last_pairs] - first_middle_token - second_middle_token return amount_ins , maximum , pairs , first_step , second_step","title":"6. depth_three"},{"location":"Services/Aggregator/Logic/path/#7-best-path","text":"Return a list containing: first amounts in, first amount out and first pairs for depth one. second amounts in, second amount out and second pairs for depth two. third amounts in, third amount out, third pairs, first middle token and second middle token for depth three. Raises Errors.NoPathFound() if couldn't find any path. async def best_path ( self ): depth_one_common_tokens , depth_two_common_tokens = await self . find_common_tokens () first_amounts_in , first_amount_out , first_pairs = await self . depth_one () second_amounts_in , second_amount_out , second_pairs = await self . depth_two ( depth_one_common_tokens ) third_amounts_in , third_amount_out , third_pairs , first_step , second_step = await self . depth_three ( depth_one_common_tokens , depth_two_common_tokens ) if first_pairs is None and second_pairs is None and third_pairs is None : raise Errors . NoPathFound () return [ ( first_amounts_in , first_amount_out , first_pairs ), ( second_amounts_in , second_amount_out , second_pairs ), ( third_amounts_in , third_amount_out , third_pairs , first_step , second_step ) ]","title":"7. Best path"},{"location":"Services/Aggregator/Logic/portioning/","text":"Portioning \u00b6 A class that help us to portion the input amount, find the best pair for each portion and calculate amount out for them. How it works \u00b6 While exchanging tokens, in order to avoid price impact and gaining more output, we divide the input amount to different portions. Assume that we have a pair with tokenA and tokenB like the image below: And there are 4 types of requests: 0 - 50 $ 50 - 1500 $ 1500 - 10k $ 10k < Assume that some user requests 17 $ tokenA -> tokenB; We divide the input amount into 16 and 1 . And we calculate amount out of tokenA -> tokenB for following amounts: 16 $: 2**4 8 $: 2**3 4 $: 2**2 2 $: 2**1 1 $: 2**0 Then we set amount in: amount out facets on pair. The pair's chart would be something like: Binary portioning \u00b6 As you observed, portioning works based on binary numbers. The first portion is the closest base 2 (equal or less) number to the input amount, the second portion is the closest base 2 number of the remaining amount and so on. The follow the below pattern: 2**0 = 1 2**1 = 2 2**2 = 4 2**3 = 8 2**4 = 16 2**5 = 32 2**6 = 64 ... Example \u00b6 Assume that some user wants to exchange 1600 $ of tokenA to tokenB; The input amounts would be: 160 = 2 ** n 128 < 160 > 256 160 = 128 + 32 32 = 2 ** n 32 = 2 ** 5 portioning = 128 + 32 Attributes: \u00b6 chain_id: ChainId token_in: Token token_out: Token amount_in_with_decimal: WithDecimal Methods: \u00b6 find_best_pair common_tokens burn_rate_amount_in burn_rate_amount_out calculate_real_amount_out portion_same_pair portion_best_path init \u00b6 Sets: chain_id token_in token_out amount_in_with_decimal amount_in_without_decimal client Sets ftm_amount_in_with_decimal by converting amount_in to FTM (network's value token) price. Sets binary_string by converting amount_in_with_decimal to a binary number. def __init__ ( self , chain_id : ChainId , amount_in_with_decimal : WithDecimal , token_in : Token , token_out : Token ) -> None : self . chain_id = chain_id self . has_been_populated = False self . token_in = token_in self . token_out = token_out self . amount_in_with_decimal = amount_in_with_decimal self . amount_in_without_decimal : NoDecimal = amount_in_with_decimal / \\ 10 ** token_in . decimal self . pair = {} self . pair_amount_out = 0 self . amount_out = 0 async def populate_obj (): if self . has_been_populated : return self . has_been_populated = True self . client = dgraph_client () self . ftm_amount_in_with_decimal = convert_to_ftm ( self . amount_in_with_decimal , self . token_in ) if not self . ftm_amount_in_with_decimal : logging . error ( f \"Price of token_in: { self . token_in . uid } is { self . token_in . price } at Portioning.populate_obj.\" ) self . binary_string = \"0\" else : self . binary_string = format ( round ( self . ftm_amount_in_with_decimal / ( 10 ** self . token_in . decimal )), \"b\" ) self . binary_string = max ( self . binary_string , '1' ) self . populate_obj = populate_obj 1. find_best_pair \u00b6 Calls find_pair_and_amount_out function and returns pair and amount_out. async def find_best_pair ( self ): amount_out , pair = await find_pair_and_amount_out ( self . chain_id , self . token_in , self . token_out , self . amount_in_with_decimal ) return amount_out , pair 2. burn_rate_amount_in \u00b6 Some tokens are burn tokens, meaning some of it will be sent to an account that can only receive it. This function checks if the input token is a burn token and if yes, calculates the amount of token that will burn and removes it from the input amount. def burn_rate_amount_in ( self , amount_in_with_decimal ): in_burn_rate = self . token_in . burn_rate if in_burn_rate not in ( None , 0 ): burning_amount_in = ( amount_in_with_decimal * in_burn_rate ) // cc . BURN_RATE_PERSISSION return burning_amount_in return amount_in_with_decimal 3. burn_rate_amount_out \u00b6 This function checks if the output token is a burn token and if yes, calculates the amount of token that will burn and removes it from the output amount. def burn_rate_amount_out ( self , amount_out ): out_burn_rate = self . token_out . burn_rate if out_burn_rate not in ( None , 0 ): amount_out = ( amount_out * out_burn_rate ) // cc . BURN_RATE_PERSISSION return amount_out 4. calculate_real_amount_out \u00b6 Checks if token_in is a burn token and if yes, calculates the new amount_in. Calls find_pair_and_amount_out function that returns amount_out and pair. Checks if token_out is a burn token and if yes, calculates the new amount_out. async def calculate_real_amount_out ( self , amount_in_with_decimal ): try : amount_in_with_decimal = self . burn_rate_amount_in ( amount_in_with_decimal ) amount_out , pair = await find_pair_and_amount_out ( self . chain_id , self . token_in , self . token_out , amount_in_with_decimal ) if amount_out : amount_out = self . burn_rate_amount_out ( amount_out ) return amount_out , pair except Exception as e : logging . error ( f \"Exception { e } occurred at Portioning.calculate_real_amount_out.\" ) return None , None 5. portion_same_pair \u00b6 While finding path, sometimes we pass through the same pair more than one time. if we use one pair for exchanging tokenA to tokenB more than once, we are practically paying extra fee without gaining any benefits. This functions searches for repeated pairs and sums their input amount so we can pass through it just one time. def portion_same_pairs ( self , pairs : List , pair_amounts : Dict , pair : Pair , ): if pair_amounts . get ( pair . address ) is not None : portion = pairs [ len ( pair_amounts ) - 1 ] . get ( \"portion\" ) + \\ (( amount_0 / self . amount_in_with_decimal ) * 100 ) pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : portion }) if pairs [ len ( pair_amounts ) - 1 ] . get ( \"portion\" ) > 100 : pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : 100 }) amount_0 += pair_amounts . get ( pair . address ) pair_amounts . update ({ pair . address : amount_0 }) else : pair_amounts [ pair . address ] = amount_0 pairs . append ({ \"pairs\" : pair }) pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : ( amount_0 / self . amount_in_with_decimal ) * 100 }) if pairs [ len ( pair_amounts ) - 1 ] . get ( \"portion\" ) > 100 : pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : 100 }) 6. portion_best_path \u00b6 Loops through binary_string (portions) and calculates amount_in , pair and amount_out for each portion and returns amounts_in , total_amount_out and pairs . async def portion_best_path ( self ) -> Tuple [ List [ WithDecimal ], WithDecimal , List [ Dict [ str , Pair ]]]: await self . populate_obj () pairs = [] total_amount_out = 0 pair_amounts = {} try : for index , item in enumerate ( self . binary_string ): if int ( item ) == 0 : continue power = len ( self . binary_string ) - index - 1 amount_0_in_ftm_with_decimal = ( 2 ** power ) * ( 10 ** self . token_in . decimal ) amount_0 : WithDecimal = int ( convert_from_ftm ( amount_0_in_ftm_with_decimal , self . token_in )) if amount_0 > self . amount_in_with_decimal : amount_0 = self . amount_in_with_decimal real_amount_out , pair_obj = await self . calculate_real_amount_out ( amount_0 ) end = time . time_ns () logging . info ( f \"Elapsed time at calculate_real_amount_out in line 108 of portioning is: { end - s } \" ) if pair_obj is None : return ( None ,) * 3 self . portion_same_pairs ( pairs , pair_amounts , pair_obj ) total_amount_out += real_amount_out amounts_in = list ( pair_amounts . values ()) return amounts_in , total_amount_out , pairs except Exception as e : logging . error ( f \"Exception { e } occurred at Portioning.portion_best_path.\" ) return ( None ,) * 3","title":"Portioning"},{"location":"Services/Aggregator/Logic/portioning/#portioning","text":"A class that help us to portion the input amount, find the best pair for each portion and calculate amount out for them.","title":"Portioning"},{"location":"Services/Aggregator/Logic/portioning/#how-it-works","text":"While exchanging tokens, in order to avoid price impact and gaining more output, we divide the input amount to different portions. Assume that we have a pair with tokenA and tokenB like the image below: And there are 4 types of requests: 0 - 50 $ 50 - 1500 $ 1500 - 10k $ 10k < Assume that some user requests 17 $ tokenA -> tokenB; We divide the input amount into 16 and 1 . And we calculate amount out of tokenA -> tokenB for following amounts: 16 $: 2**4 8 $: 2**3 4 $: 2**2 2 $: 2**1 1 $: 2**0 Then we set amount in: amount out facets on pair. The pair's chart would be something like:","title":"How it works"},{"location":"Services/Aggregator/Logic/portioning/#binary-portioning","text":"As you observed, portioning works based on binary numbers. The first portion is the closest base 2 (equal or less) number to the input amount, the second portion is the closest base 2 number of the remaining amount and so on. The follow the below pattern: 2**0 = 1 2**1 = 2 2**2 = 4 2**3 = 8 2**4 = 16 2**5 = 32 2**6 = 64 ...","title":"Binary portioning"},{"location":"Services/Aggregator/Logic/portioning/#example","text":"Assume that some user wants to exchange 1600 $ of tokenA to tokenB; The input amounts would be: 160 = 2 ** n 128 < 160 > 256 160 = 128 + 32 32 = 2 ** n 32 = 2 ** 5 portioning = 128 + 32","title":"Example"},{"location":"Services/Aggregator/Logic/portioning/#attributes","text":"chain_id: ChainId token_in: Token token_out: Token amount_in_with_decimal: WithDecimal","title":"Attributes:"},{"location":"Services/Aggregator/Logic/portioning/#methods","text":"find_best_pair common_tokens burn_rate_amount_in burn_rate_amount_out calculate_real_amount_out portion_same_pair portion_best_path","title":"Methods:"},{"location":"Services/Aggregator/Logic/portioning/#init","text":"Sets: chain_id token_in token_out amount_in_with_decimal amount_in_without_decimal client Sets ftm_amount_in_with_decimal by converting amount_in to FTM (network's value token) price. Sets binary_string by converting amount_in_with_decimal to a binary number. def __init__ ( self , chain_id : ChainId , amount_in_with_decimal : WithDecimal , token_in : Token , token_out : Token ) -> None : self . chain_id = chain_id self . has_been_populated = False self . token_in = token_in self . token_out = token_out self . amount_in_with_decimal = amount_in_with_decimal self . amount_in_without_decimal : NoDecimal = amount_in_with_decimal / \\ 10 ** token_in . decimal self . pair = {} self . pair_amount_out = 0 self . amount_out = 0 async def populate_obj (): if self . has_been_populated : return self . has_been_populated = True self . client = dgraph_client () self . ftm_amount_in_with_decimal = convert_to_ftm ( self . amount_in_with_decimal , self . token_in ) if not self . ftm_amount_in_with_decimal : logging . error ( f \"Price of token_in: { self . token_in . uid } is { self . token_in . price } at Portioning.populate_obj.\" ) self . binary_string = \"0\" else : self . binary_string = format ( round ( self . ftm_amount_in_with_decimal / ( 10 ** self . token_in . decimal )), \"b\" ) self . binary_string = max ( self . binary_string , '1' ) self . populate_obj = populate_obj","title":"init"},{"location":"Services/Aggregator/Logic/portioning/#1-find_best_pair","text":"Calls find_pair_and_amount_out function and returns pair and amount_out. async def find_best_pair ( self ): amount_out , pair = await find_pair_and_amount_out ( self . chain_id , self . token_in , self . token_out , self . amount_in_with_decimal ) return amount_out , pair","title":"1. find_best_pair"},{"location":"Services/Aggregator/Logic/portioning/#2-burn_rate_amount_in","text":"Some tokens are burn tokens, meaning some of it will be sent to an account that can only receive it. This function checks if the input token is a burn token and if yes, calculates the amount of token that will burn and removes it from the input amount. def burn_rate_amount_in ( self , amount_in_with_decimal ): in_burn_rate = self . token_in . burn_rate if in_burn_rate not in ( None , 0 ): burning_amount_in = ( amount_in_with_decimal * in_burn_rate ) // cc . BURN_RATE_PERSISSION return burning_amount_in return amount_in_with_decimal","title":"2. burn_rate_amount_in"},{"location":"Services/Aggregator/Logic/portioning/#3-burn_rate_amount_out","text":"This function checks if the output token is a burn token and if yes, calculates the amount of token that will burn and removes it from the output amount. def burn_rate_amount_out ( self , amount_out ): out_burn_rate = self . token_out . burn_rate if out_burn_rate not in ( None , 0 ): amount_out = ( amount_out * out_burn_rate ) // cc . BURN_RATE_PERSISSION return amount_out","title":"3. burn_rate_amount_out"},{"location":"Services/Aggregator/Logic/portioning/#4-calculate_real_amount_out","text":"Checks if token_in is a burn token and if yes, calculates the new amount_in. Calls find_pair_and_amount_out function that returns amount_out and pair. Checks if token_out is a burn token and if yes, calculates the new amount_out. async def calculate_real_amount_out ( self , amount_in_with_decimal ): try : amount_in_with_decimal = self . burn_rate_amount_in ( amount_in_with_decimal ) amount_out , pair = await find_pair_and_amount_out ( self . chain_id , self . token_in , self . token_out , amount_in_with_decimal ) if amount_out : amount_out = self . burn_rate_amount_out ( amount_out ) return amount_out , pair except Exception as e : logging . error ( f \"Exception { e } occurred at Portioning.calculate_real_amount_out.\" ) return None , None","title":"4. calculate_real_amount_out"},{"location":"Services/Aggregator/Logic/portioning/#5-portion_same_pair","text":"While finding path, sometimes we pass through the same pair more than one time. if we use one pair for exchanging tokenA to tokenB more than once, we are practically paying extra fee without gaining any benefits. This functions searches for repeated pairs and sums their input amount so we can pass through it just one time. def portion_same_pairs ( self , pairs : List , pair_amounts : Dict , pair : Pair , ): if pair_amounts . get ( pair . address ) is not None : portion = pairs [ len ( pair_amounts ) - 1 ] . get ( \"portion\" ) + \\ (( amount_0 / self . amount_in_with_decimal ) * 100 ) pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : portion }) if pairs [ len ( pair_amounts ) - 1 ] . get ( \"portion\" ) > 100 : pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : 100 }) amount_0 += pair_amounts . get ( pair . address ) pair_amounts . update ({ pair . address : amount_0 }) else : pair_amounts [ pair . address ] = amount_0 pairs . append ({ \"pairs\" : pair }) pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : ( amount_0 / self . amount_in_with_decimal ) * 100 }) if pairs [ len ( pair_amounts ) - 1 ] . get ( \"portion\" ) > 100 : pairs [ len ( pair_amounts ) - 1 ] . update ({ \"portion\" : 100 })","title":"5. portion_same_pair"},{"location":"Services/Aggregator/Logic/portioning/#6-portion_best_path","text":"Loops through binary_string (portions) and calculates amount_in , pair and amount_out for each portion and returns amounts_in , total_amount_out and pairs . async def portion_best_path ( self ) -> Tuple [ List [ WithDecimal ], WithDecimal , List [ Dict [ str , Pair ]]]: await self . populate_obj () pairs = [] total_amount_out = 0 pair_amounts = {} try : for index , item in enumerate ( self . binary_string ): if int ( item ) == 0 : continue power = len ( self . binary_string ) - index - 1 amount_0_in_ftm_with_decimal = ( 2 ** power ) * ( 10 ** self . token_in . decimal ) amount_0 : WithDecimal = int ( convert_from_ftm ( amount_0_in_ftm_with_decimal , self . token_in )) if amount_0 > self . amount_in_with_decimal : amount_0 = self . amount_in_with_decimal real_amount_out , pair_obj = await self . calculate_real_amount_out ( amount_0 ) end = time . time_ns () logging . info ( f \"Elapsed time at calculate_real_amount_out in line 108 of portioning is: { end - s } \" ) if pair_obj is None : return ( None ,) * 3 self . portion_same_pairs ( pairs , pair_amounts , pair_obj ) total_amount_out += real_amount_out amounts_in = list ( pair_amounts . values ()) return amounts_in , total_amount_out , pairs except Exception as e : logging . error ( f \"Exception { e } occurred at Portioning.portion_best_path.\" ) return ( None ,) * 3","title":"6. portion_best_path"},{"location":"Services/Aggregator/Logic/should_unwrap/","text":"","title":"Should unwrap"},{"location":"Services/Aggregator/Logic/should_wrap/","text":"","title":"Should wrap"},{"location":"Services/Aggregator/Models/","text":"Models \u00b6 Models in aggregator inherit from pydantic's basemodel class.","title":"Models"},{"location":"Services/Aggregator/Models/#models","text":"Models in aggregator inherit from pydantic's basemodel class.","title":"Models"},{"location":"Services/Aggregator/Protocols/","text":"Swap - Protocols \u00b6","title":"Swap - Protocols"},{"location":"Services/Aggregator/Protocols/#swap-protocols","text":"","title":"Swap - Protocols"},{"location":"Services/Aggregator/Protocols/Amm/","text":"","title":"Amm"},{"location":"Services/Aggregator/Protocols/Beethoven/","text":"","title":"Beethoven"},{"location":"Services/Aggregator/Protocols/Curve/","text":"","title":"Curve"},{"location":"Services/AggregatorOfAggregators/","text":"Aggregator Of Aggregators - intro \u00b6 By calling other aggregator's api just as a normal user does in a browser, we can find the best quote between different aggregators. This helps users to chose the best possible result at any given time. It's just like as if user, enter the desired tokens to the aggregator and then after confirm the price; swapped on the recommended aggregator. Normal Flow \u00b6 Let's say user want to test out if is it good enough to use 1inch aggregator or not. And if not, check the quotes on another aggregator . graph LR A[User] -->|Quote| B([Aggregator-UI]) B --> C{Is quote good enough} C -->|Yes| D[Swap] C -->|No| E[Check another aggregator] E -.->|Goes Back|B Agg Of Agg Flow \u00b6 However, in this service user will experience a flow as shown in following chart: flowchart LR A[User] -->|Quote| B([Timechainswap-UI]) B --- C([Receive Best Quote]) C -->|Chosen Aggregator| D[Swap request] D --> |Send swap object| A And What Happens in the Back-end side is one: Quoting for user request flowchart LR C([Receive Best Quote]) -.- Server([Server]) Server --> Z([Call network's aggregators quote endpoint]) Z -.-> OneInch(1inch) Z -.-> OC(OpenOcean) Z -.-> Ky(KyberSwap) Ky ---> |Result| Res(Aggregator Quote) OC --> |Result| Res OneInch --> |Result| Res Res -.-> |Best Quote|Server Creating a swap object for user flowchart TB C([Create Swap object]) -.- Server([Server]) Server --> Z([Call network's aggregators swap endopint]) Z ==> Co(Chosen aggregator) Co --> AggRes(parse response) AggRes --> X{Estimate Gas} X --> |Success| Res(Aggregator Swap Response) X --> |Failed| G(Next Best Aggregator) G --> |Try Next Aggregator| AggRes Res -.-> |Swap Object|Server G --> |Tried all network aggregator| No(Error) No -.->|Error Response| Server","title":"Aggregator Of Aggregators - intro"},{"location":"Services/AggregatorOfAggregators/#aggregator-of-aggregators-intro","text":"By calling other aggregator's api just as a normal user does in a browser, we can find the best quote between different aggregators. This helps users to chose the best possible result at any given time. It's just like as if user, enter the desired tokens to the aggregator and then after confirm the price; swapped on the recommended aggregator.","title":"Aggregator Of Aggregators - intro"},{"location":"Services/AggregatorOfAggregators/#normal-flow","text":"Let's say user want to test out if is it good enough to use 1inch aggregator or not. And if not, check the quotes on another aggregator . graph LR A[User] -->|Quote| B([Aggregator-UI]) B --> C{Is quote good enough} C -->|Yes| D[Swap] C -->|No| E[Check another aggregator] E -.->|Goes Back|B","title":"Normal Flow"},{"location":"Services/AggregatorOfAggregators/#agg-of-agg-flow","text":"However, in this service user will experience a flow as shown in following chart: flowchart LR A[User] -->|Quote| B([Timechainswap-UI]) B --- C([Receive Best Quote]) C -->|Chosen Aggregator| D[Swap request] D --> |Send swap object| A And What Happens in the Back-end side is one: Quoting for user request flowchart LR C([Receive Best Quote]) -.- Server([Server]) Server --> Z([Call network's aggregators quote endpoint]) Z -.-> OneInch(1inch) Z -.-> OC(OpenOcean) Z -.-> Ky(KyberSwap) Ky ---> |Result| Res(Aggregator Quote) OC --> |Result| Res OneInch --> |Result| Res Res -.-> |Best Quote|Server Creating a swap object for user flowchart TB C([Create Swap object]) -.- Server([Server]) Server --> Z([Call network's aggregators swap endopint]) Z ==> Co(Chosen aggregator) Co --> AggRes(parse response) AggRes --> X{Estimate Gas} X --> |Success| Res(Aggregator Swap Response) X --> |Failed| G(Next Best Aggregator) G --> |Try Next Aggregator| AggRes Res -.-> |Swap Object|Server G --> |Tried all network aggregator| No(Error) No -.->|Error Response| Server","title":"Agg Of Agg Flow"},{"location":"Services/AggregatorOfAggregators/api/","text":"","title":"Api"},{"location":"Services/AggregatorOfAggregators/ui/","text":"AOA - Data Flow \u00b6 Goal is to guide user to swap on the best possible quote. sequenceDiagram User->>UI: Chose Chain UI->>AGG: Desired Chain AGG->>UI: Chain Token List User->>UI: Chose tokens User->>UI: Enter AmountIn note left of AGG: Should use agg of agg for this chain ? [YES] UI->>AGG: Send user req to Agg AGG->>AOA: internal call user data note right of AOA: Gather Aggregators quotes! note right of AOA: Parse Aggregators Responses! AOA->>AGG: respond with best agg result note right of AGG: Complete Responses with prices! AGG->>UI:Send chosen Agg info UI->>User:Show Find response User->>UI:Confirms Swap UI->>AGG:Request for swap object note left of AGG: Check User's Allowance to chosen Aggregator!","title":"AOA - Data Flow"},{"location":"Services/AggregatorOfAggregators/ui/#aoa-data-flow","text":"Goal is to guide user to swap on the best possible quote. sequenceDiagram User->>UI: Chose Chain UI->>AGG: Desired Chain AGG->>UI: Chain Token List User->>UI: Chose tokens User->>UI: Enter AmountIn note left of AGG: Should use agg of agg for this chain ? [YES] UI->>AGG: Send user req to Agg AGG->>AOA: internal call user data note right of AOA: Gather Aggregators quotes! note right of AOA: Parse Aggregators Responses! AOA->>AGG: respond with best agg result note right of AGG: Complete Responses with prices! AGG->>UI:Send chosen Agg info UI->>User:Show Find response User->>UI:Confirms Swap UI->>AGG:Request for swap object note left of AGG: Check User's Allowance to chosen Aggregator!","title":"AOA - Data Flow"},{"location":"Services/Bridge/api/","text":"","title":"Api"},{"location":"Services/Bridge/event/","text":"Event crawler service - intro \u00b6 This microservice is working every 30 seconds for getting all events from our contracts in chains. Why need events? \u00b6 we need events for validating our bridge input transactions with events data. Quick lunch \u00b6 Install requirements pip install - r requirements . txt create a database from mongo and named event . If you need more info about mongo db use mongo documentation . initialize base datas python db_initializer . py run service python crawler_startup . py - m EventCrawler - n NETWORK Functions logic \u00b6 Starting function \u00b6 this function decision maker desids about how the service should be run with how many process def run_concurrent_process ( method , workers = 3 , * args ): if DEBUG : method () else : with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = [] i = 0 while i < 1500 : futures . append ( executor . submit ( method , * args , )) i += 1 for future in concurrent . futures . as_completed ( futures ): try : message = future . result () if message is not None : pass except Exception as e : print ( e ) Base configs \u00b6 .env MONGO_CONNECTION_URL=\"mongodb://localhost\" BASE_PATH=api/bridgev3/ DEBUG=True Add this configs in you .env file MONGO_CONNECTION_URL is your mongo connection string \"mongodb://username:password\" for more info about mongo connection use mongo connection documentation Code Logic \u00b6 EventCrawlerManager class \u00b6 get_ready_chain method \u00b6 def get_ready_chain ( self ): while True : self . chain_info = self . chain_mongo_db . get_ready_chain () if self . chain_info is None : BaseLogger . log_warn ( \"can not find free chain for crawl, sleep 10\" ) sleep ( 10 ) if self . chain_info is not None : self . last_block = None BaseLogger . log_info ( f \"get chain { self . chain_info [ 'chain_name' ] } \" ) break Get chains for get events from network the condition of free chains is busy = False and next_crawl_date_time < now chains crawl every 40 second start_store_events method \u00b6 def start_store_events ( self , block_number , steps = 2000 ): to_block = block_number while True : from_block = to_block - steps if to_block < self . chain_info [ \"last_block\" ]: from_block = block_number event_data = self . get_event ( from_block = from_block , to_block = to_block ) if len ( event_data ) > 0 : self . export_and_save ( event_data ) to_block -= steps sleep ( 1 ) if self . is_not_valid_to_continue ( block_number , from_block ): self . chain_mongo_db . update_last_block ( self . chain_info . get ( \"_id\" ), block_number ) self . chain_mongo_db . update_busy_false ( self . chain_info . get ( \"_id\" )) return Get the last block number and last crawled number and try to get blocks with 2000 steps by default and if can find the new events from contract trying store event to event mongo collection get_event method \u00b6 def get_event ( self , from_block , to_block ): while True : try : event_data = self . event_method . getLogs ( fromBlock = from_block , toBlock = to_block ) return event_data except Exception as e : BaseLogger . log_error ( f \" { self . chain_info . get ( 'chain_name' ) } can not get event error message = { e } \" ) Trying ro get events form network contract is_not_valid_to_continue method \u00b6 def is_not_valid_to_continue ( self , last_block , block_number ): if self . last_block is None : print ( last_block ) self . last_block = last_block if block_number <= self . chain_info [ \"last_block\" ]: return True if block_number == 0 : return True return False if getting block number == last block number event crawler is sleep for 40 seconds for new blocks and function return True else function return False and event service while getting new blocks with 2000 steps clean_data method \u00b6 Before storing new event into mongo event collection cleaning data and create a data structure and after that store data into mongo db def clean_data ( self , input_data , network = \"ftm\" ): salt = first_search_key ( input_data , \"salt\" ) transaction_hash = first_search_key ( input_data , \"transactionHash\" ) amount = first_search_key ( input_data , \"amount\" ) if salt is not None and isinstance ( salt , bytes ): salt = salt . hex () if transaction_hash is not None : input_data . pop ( \"transactionHash\" ) transaction_hash = transaction_hash . hex () input_data [ 'args' ] . pop ( 'amount' ) if amount is not None : amount = Decimal128 ( Decimal ( amount )) context = { \"network\" : self . chain_info . get ( \"chain_name\" ), \"test_chain\" : self . chain_info . get ( \"test_chain\" ), \"date_time\" : first_search_key ( input_data , \"date_time\" ), \"salt\" : salt , \"transaction_hash\" : transaction_hash , \"amount\" : amount , ** input_data } return context","title":"Event crawler service - intro"},{"location":"Services/Bridge/event/#event-crawler-service-intro","text":"This microservice is working every 30 seconds for getting all events from our contracts in chains.","title":"Event crawler service - intro"},{"location":"Services/Bridge/event/#why-need-events","text":"we need events for validating our bridge input transactions with events data.","title":"Why need events?"},{"location":"Services/Bridge/event/#quick-lunch","text":"Install requirements pip install - r requirements . txt create a database from mongo and named event . If you need more info about mongo db use mongo documentation . initialize base datas python db_initializer . py run service python crawler_startup . py - m EventCrawler - n NETWORK","title":"Quick lunch"},{"location":"Services/Bridge/event/#functions-logic","text":"","title":"Functions logic"},{"location":"Services/Bridge/event/#starting-function","text":"this function decision maker desids about how the service should be run with how many process def run_concurrent_process ( method , workers = 3 , * args ): if DEBUG : method () else : with ProcessPoolExecutor ( max_workers = workers ) as executor : futures = [] i = 0 while i < 1500 : futures . append ( executor . submit ( method , * args , )) i += 1 for future in concurrent . futures . as_completed ( futures ): try : message = future . result () if message is not None : pass except Exception as e : print ( e )","title":"Starting function"},{"location":"Services/Bridge/event/#base-configs","text":".env MONGO_CONNECTION_URL=\"mongodb://localhost\" BASE_PATH=api/bridgev3/ DEBUG=True Add this configs in you .env file MONGO_CONNECTION_URL is your mongo connection string \"mongodb://username:password\" for more info about mongo connection use mongo connection documentation","title":"Base configs"},{"location":"Services/Bridge/event/#code-logic","text":"","title":"Code Logic"},{"location":"Services/Bridge/event/#eventcrawlermanager-class","text":"","title":"EventCrawlerManager class"},{"location":"Services/Bridge/event/#get_ready_chain-method","text":"def get_ready_chain ( self ): while True : self . chain_info = self . chain_mongo_db . get_ready_chain () if self . chain_info is None : BaseLogger . log_warn ( \"can not find free chain for crawl, sleep 10\" ) sleep ( 10 ) if self . chain_info is not None : self . last_block = None BaseLogger . log_info ( f \"get chain { self . chain_info [ 'chain_name' ] } \" ) break Get chains for get events from network the condition of free chains is busy = False and next_crawl_date_time < now chains crawl every 40 second","title":"get_ready_chain method"},{"location":"Services/Bridge/event/#start_store_events-method","text":"def start_store_events ( self , block_number , steps = 2000 ): to_block = block_number while True : from_block = to_block - steps if to_block < self . chain_info [ \"last_block\" ]: from_block = block_number event_data = self . get_event ( from_block = from_block , to_block = to_block ) if len ( event_data ) > 0 : self . export_and_save ( event_data ) to_block -= steps sleep ( 1 ) if self . is_not_valid_to_continue ( block_number , from_block ): self . chain_mongo_db . update_last_block ( self . chain_info . get ( \"_id\" ), block_number ) self . chain_mongo_db . update_busy_false ( self . chain_info . get ( \"_id\" )) return Get the last block number and last crawled number and try to get blocks with 2000 steps by default and if can find the new events from contract trying store event to event mongo collection","title":"start_store_events method"},{"location":"Services/Bridge/event/#get_event-method","text":"def get_event ( self , from_block , to_block ): while True : try : event_data = self . event_method . getLogs ( fromBlock = from_block , toBlock = to_block ) return event_data except Exception as e : BaseLogger . log_error ( f \" { self . chain_info . get ( 'chain_name' ) } can not get event error message = { e } \" ) Trying ro get events form network contract","title":"get_event method"},{"location":"Services/Bridge/event/#is_not_valid_to_continue-method","text":"def is_not_valid_to_continue ( self , last_block , block_number ): if self . last_block is None : print ( last_block ) self . last_block = last_block if block_number <= self . chain_info [ \"last_block\" ]: return True if block_number == 0 : return True return False if getting block number == last block number event crawler is sleep for 40 seconds for new blocks and function return True else function return False and event service while getting new blocks with 2000 steps","title":"is_not_valid_to_continue method"},{"location":"Services/Bridge/event/#clean_data-method","text":"Before storing new event into mongo event collection cleaning data and create a data structure and after that store data into mongo db def clean_data ( self , input_data , network = \"ftm\" ): salt = first_search_key ( input_data , \"salt\" ) transaction_hash = first_search_key ( input_data , \"transactionHash\" ) amount = first_search_key ( input_data , \"amount\" ) if salt is not None and isinstance ( salt , bytes ): salt = salt . hex () if transaction_hash is not None : input_data . pop ( \"transactionHash\" ) transaction_hash = transaction_hash . hex () input_data [ 'args' ] . pop ( 'amount' ) if amount is not None : amount = Decimal128 ( Decimal ( amount )) context = { \"network\" : self . chain_info . get ( \"chain_name\" ), \"test_chain\" : self . chain_info . get ( \"test_chain\" ), \"date_time\" : first_search_key ( input_data , \"date_time\" ), \"salt\" : salt , \"transaction_hash\" : transaction_hash , \"amount\" : amount , ** input_data } return context","title":"clean_data method"},{"location":"Services/Bridge/PaymentsService/base/","text":"Base diagram \u00b6 Data structure \u00b6 Base abstract model \u00b6 Base abstract model inherit all models from this model model field: created_time = models.DateTimeField(verbose_name=_('created time'), auto_now_add=True) modified_time = models.DateTimeField(verbose_name=_('modified time'), auto_now=True) deleted_time = models.DateTimeField(verbose_name=_('deleted time'), null=True, blank=True, editable=False) deleted = models.BooleanField(verbose_name=_('deleted'), default=False, editable=False) deleted for safe delete data Django apps \u00b6 This project contain tree apps and four celery tasks Apps \u00b6 Chain app \u00b6 It contains chains info, connections, tokens, tokens fee Models \u00b6 Network model fields: name = models . CharField ( max_length = 255 , verbose_name = \"name\" ) symbol = models . CharField ( max_length = 255 , verbose_name = \"symbol\" ) chain_id = models . IntegerField ( verbose_name = \"chain_id\" ) bridge_contract_address = models . CharField ( max_length = 255 , verbose_name = \"contract_address\" ) bridge_contract_abi = models . CharField ( max_length = 256 , default = \"./abi/contract_abi.json\" , verbose_name = \"bridge_contract_abi\" ) decimal_digits = models . IntegerField ( verbose_name = \"decimal_digits\" , default = 18 ) active = models . BooleanField ( default = True , verbose_name = \"active\" ) for example: { name:fantom, symbl:ftm, chain_id:560, bridge_contract_address:our bridge contract address, bridge_contract_abi: abi address(defult=./abi/contract_abi.json), decimal_digits: native token decimal digits, active: True/False, } Connection model This model store networks connections and can store many rpc connections from network '''' model fields : name, path, network, type '''' example: {name: any name , path: rpc connection, network: network relation, type: connection type} Abstract base services \u00b6 BaseContract(ABC) class \u00b6 set_contract method \u00b6 This method get a payment transaction and trying to connect network and set contract @staticmethod def set_contract ( payment_transaction ): try : contract = ContractTasks ( network_connections_list = payment_transaction . token . network . connections . all () ) contract . set_contract ( contract_address = payment_transaction . token . network . bridge_contract_address , contract_abi_path = payment_transaction . token . network . bridge_contract_abi ) return contract except Exception as e : print ( e ) failed_transaction method \u00b6 Update payment status to reverted @staticmethod def failed_transaction ( payment ): try : with transaction . atomic (): payment . transaction_des . status = TRANSACTION_TYPE_FAILED payment . transaction_des . save () payment . status = PAYMENT_TYPE_REVERTED payment . save () return payment except Exception as e : BaseLogger . log_error ( e ) failed_service_fee method \u00b6 If calculated service fee and estimated gas is more than destnation transaction amount payment update status to failed service fee (fail_service_fee) and service can not build transaction @staticmethod def failed_service_fee ( payment ): payment . status = PAYMENT_TYPE_FAIL_SERVICE_FEE payment . save () done_payment method \u00b6 Update payment to done and stor contract transaction gas into payment info @staticmethod def done_payment ( payment , transaction_result , gas_price ): try : with transaction . atomic (): payment . transaction_des . status = TRANSACTION_TYPE_DONE token_gas = ( transaction_result . gasUsed * gas_price ) / 10 ** payment . transaction_des . token . network . decimal_digits real_gas = token_to_usd_convert ( token_gas , payment . transaction_des . token . network . symbol ) payment . transaction_des . gas_price = gas_price payment . transaction_des . real_gas = real_gas payment . transaction_des . save () payment . status = PAYMENT_TYPE_DONE payment . save () BaseLogger . log_info ( f \"payment_key: { payment . payment_key } ,\" f \"update payment status to done\" , ) return payment except Exception as e : BaseLogger . log_error ( e ) save_gas_info_to_model method \u00b6 Get gas info form des transaction and store in payment info @staticmethod def save_gas_info_to_model ( transaction_data , gas_price , estimate_gas , token_usdc_price , calculated_gas_fee_dollar , final_amount ): with transaction . atomic (): transaction_data . gas_price = gas_price transaction_data . estimate_gas = estimate_gas transaction_data . token_usdc_price = token_usdc_price transaction_data . calculated_gas_fee_dollar = calculated_gas_fee_dollar transaction_data . amount = final_amount transaction_data . save () is_valid_payment_and_transaction_info method \u00b6 Validate open payment with transaction info and evnet crawler @staticmethod def is_valid_payment_and_transaction_info ( payment , event ): from mongo.utiles import first_search_key if payment . salt [ 2 :] == event . get ( \"salt\" ): event_amount = first_search_key ( event , \"amount\" ) if event_amount is not None : event_amount = event_amount . to_decimal () if payment . amount == event_amount : if Web3 . toChecksumAddress ( payment . wallet . wallet_address ) == Web3 . toChecksumAddress ( first_search_key ( event , \"sender\" ) ): if Web3 . toChecksumAddress ( payment . transaction_src . token . token_contract_address ) == Web3 . toChecksumAddress ( first_search_key ( event , \"token\" ) ): return True if payment . transaction_src . token . is_native_token or payment . transaction_des . token . is_native_token : return True return False Token fee calculator \u00b6","title":"Base diagram"},{"location":"Services/Bridge/PaymentsService/base/#base-diagram","text":"","title":"Base diagram"},{"location":"Services/Bridge/PaymentsService/base/#data-structure","text":"","title":"Data structure"},{"location":"Services/Bridge/PaymentsService/base/#base-abstract-model","text":"Base abstract model inherit all models from this model model field: created_time = models.DateTimeField(verbose_name=_('created time'), auto_now_add=True) modified_time = models.DateTimeField(verbose_name=_('modified time'), auto_now=True) deleted_time = models.DateTimeField(verbose_name=_('deleted time'), null=True, blank=True, editable=False) deleted = models.BooleanField(verbose_name=_('deleted'), default=False, editable=False) deleted for safe delete data","title":"Base abstract model"},{"location":"Services/Bridge/PaymentsService/base/#django-apps","text":"This project contain tree apps and four celery tasks","title":"Django apps"},{"location":"Services/Bridge/PaymentsService/base/#apps","text":"","title":"Apps"},{"location":"Services/Bridge/PaymentsService/base/#chain-app","text":"It contains chains info, connections, tokens, tokens fee","title":"Chain app"},{"location":"Services/Bridge/PaymentsService/base/#models","text":"Network model fields: name = models . CharField ( max_length = 255 , verbose_name = \"name\" ) symbol = models . CharField ( max_length = 255 , verbose_name = \"symbol\" ) chain_id = models . IntegerField ( verbose_name = \"chain_id\" ) bridge_contract_address = models . CharField ( max_length = 255 , verbose_name = \"contract_address\" ) bridge_contract_abi = models . CharField ( max_length = 256 , default = \"./abi/contract_abi.json\" , verbose_name = \"bridge_contract_abi\" ) decimal_digits = models . IntegerField ( verbose_name = \"decimal_digits\" , default = 18 ) active = models . BooleanField ( default = True , verbose_name = \"active\" ) for example: { name:fantom, symbl:ftm, chain_id:560, bridge_contract_address:our bridge contract address, bridge_contract_abi: abi address(defult=./abi/contract_abi.json), decimal_digits: native token decimal digits, active: True/False, } Connection model This model store networks connections and can store many rpc connections from network '''' model fields : name, path, network, type '''' example: {name: any name , path: rpc connection, network: network relation, type: connection type}","title":"Models"},{"location":"Services/Bridge/PaymentsService/base/#abstract-base-services","text":"","title":"Abstract base services"},{"location":"Services/Bridge/PaymentsService/base/#basecontractabc-class","text":"","title":"BaseContract(ABC) class"},{"location":"Services/Bridge/PaymentsService/base/#set_contract-method","text":"This method get a payment transaction and trying to connect network and set contract @staticmethod def set_contract ( payment_transaction ): try : contract = ContractTasks ( network_connections_list = payment_transaction . token . network . connections . all () ) contract . set_contract ( contract_address = payment_transaction . token . network . bridge_contract_address , contract_abi_path = payment_transaction . token . network . bridge_contract_abi ) return contract except Exception as e : print ( e )","title":"set_contract method"},{"location":"Services/Bridge/PaymentsService/base/#failed_transaction-method","text":"Update payment status to reverted @staticmethod def failed_transaction ( payment ): try : with transaction . atomic (): payment . transaction_des . status = TRANSACTION_TYPE_FAILED payment . transaction_des . save () payment . status = PAYMENT_TYPE_REVERTED payment . save () return payment except Exception as e : BaseLogger . log_error ( e )","title":"failed_transaction method"},{"location":"Services/Bridge/PaymentsService/base/#failed_service_fee-method","text":"If calculated service fee and estimated gas is more than destnation transaction amount payment update status to failed service fee (fail_service_fee) and service can not build transaction @staticmethod def failed_service_fee ( payment ): payment . status = PAYMENT_TYPE_FAIL_SERVICE_FEE payment . save ()","title":"failed_service_fee method"},{"location":"Services/Bridge/PaymentsService/base/#done_payment-method","text":"Update payment to done and stor contract transaction gas into payment info @staticmethod def done_payment ( payment , transaction_result , gas_price ): try : with transaction . atomic (): payment . transaction_des . status = TRANSACTION_TYPE_DONE token_gas = ( transaction_result . gasUsed * gas_price ) / 10 ** payment . transaction_des . token . network . decimal_digits real_gas = token_to_usd_convert ( token_gas , payment . transaction_des . token . network . symbol ) payment . transaction_des . gas_price = gas_price payment . transaction_des . real_gas = real_gas payment . transaction_des . save () payment . status = PAYMENT_TYPE_DONE payment . save () BaseLogger . log_info ( f \"payment_key: { payment . payment_key } ,\" f \"update payment status to done\" , ) return payment except Exception as e : BaseLogger . log_error ( e )","title":"done_payment method"},{"location":"Services/Bridge/PaymentsService/base/#save_gas_info_to_model-method","text":"Get gas info form des transaction and store in payment info @staticmethod def save_gas_info_to_model ( transaction_data , gas_price , estimate_gas , token_usdc_price , calculated_gas_fee_dollar , final_amount ): with transaction . atomic (): transaction_data . gas_price = gas_price transaction_data . estimate_gas = estimate_gas transaction_data . token_usdc_price = token_usdc_price transaction_data . calculated_gas_fee_dollar = calculated_gas_fee_dollar transaction_data . amount = final_amount transaction_data . save ()","title":"save_gas_info_to_model method"},{"location":"Services/Bridge/PaymentsService/base/#is_valid_payment_and_transaction_info-method","text":"Validate open payment with transaction info and evnet crawler @staticmethod def is_valid_payment_and_transaction_info ( payment , event ): from mongo.utiles import first_search_key if payment . salt [ 2 :] == event . get ( \"salt\" ): event_amount = first_search_key ( event , \"amount\" ) if event_amount is not None : event_amount = event_amount . to_decimal () if payment . amount == event_amount : if Web3 . toChecksumAddress ( payment . wallet . wallet_address ) == Web3 . toChecksumAddress ( first_search_key ( event , \"sender\" ) ): if Web3 . toChecksumAddress ( payment . transaction_src . token . token_contract_address ) == Web3 . toChecksumAddress ( first_search_key ( event , \"token\" ) ): return True if payment . transaction_src . token . is_native_token or payment . transaction_des . token . is_native_token : return True return False","title":"is_valid_payment_and_transaction_info method"},{"location":"Services/Bridge/PaymentsService/base/#token-fee-calculator","text":"","title":"Token fee calculator"},{"location":"Services/Bridge/PaymentsService/open_payment/","text":"Open payment service \u00b6 About \u00b6 This service is get open payments from the database and start to verify input transaction from the contract and event service and after the verifying input transaction trying sending out the transaction what transaction in(src_transaction) and transaction out(des_transaction)? \u00b6 When someone trying to bridge between two networks first of all, build a transaction with our frontend app and send a transaction to our contract on our contract in src network and after that event service tyring to get the contract event and verify source transaction and after verifying src transaction service try sending des transaction Flowchart diagram \u00b6 graph TD A[Open payment service] --> B(Get open payment) B --> C{Open payment create date < 1day} C -->|No| D[Update open payment status] C -->|Yes| F[Get contract events] F --> t{Can find any event?} t --> |No|B t --> |Yes|E{Transaction data is valid?} E --> |No| F E --> |Yes| H[Update open payment status to 'pending'] H --> R[Trying build trasction des] R --> G{Can build trasaction des ?} G --> |Yes| L[Update open payment status 'sending'] G --> |No| K[Log into mongo trasaction problem] K --> B L --> B Code logic \u00b6 CheckOpenPayment class inherit from BaseContract checking open payments \u00b6 this function get all open payments from database and check created dates def check ( self ): open_payment_list = self . get_open_payment_list () if len ( open_payment_list ) > 0 : for open_payment in open_payment_list : if open_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( days = 1 )): self . failed_transaction ( open_payment ) else : self . start_pay ( open_payment = open_payment ) self.get_open_payment_list() get all payments with \"open_payment\" status if open payment, open more than one day open payment update to fail If open payment open for 1 day it means user do not want to send transaction and reject the src transaction After that start checking payment def start_pay ( self , open_payment ): event_list = self . find_key_in_mongo ( open_payment . salt [ 2 :]) get the open payment and search in events from Mongo DB event crawler service payment salt : payment salt is a unique key generated when creating an open payment with payment nonce and random unique id payment key we generate salt to avoid duplicate payments and valid sending payments payment salt store in open payment on postgres db and contract events","title":"Open payment service"},{"location":"Services/Bridge/PaymentsService/open_payment/#open-payment-service","text":"","title":"Open payment service"},{"location":"Services/Bridge/PaymentsService/open_payment/#about","text":"This service is get open payments from the database and start to verify input transaction from the contract and event service and after the verifying input transaction trying sending out the transaction","title":"About"},{"location":"Services/Bridge/PaymentsService/open_payment/#what-transaction-insrc_transaction-and-transaction-outdes_transaction","text":"When someone trying to bridge between two networks first of all, build a transaction with our frontend app and send a transaction to our contract on our contract in src network and after that event service tyring to get the contract event and verify source transaction and after verifying src transaction service try sending des transaction","title":"what transaction in(src_transaction) and transaction out(des_transaction)?"},{"location":"Services/Bridge/PaymentsService/open_payment/#flowchart-diagram","text":"graph TD A[Open payment service] --> B(Get open payment) B --> C{Open payment create date < 1day} C -->|No| D[Update open payment status] C -->|Yes| F[Get contract events] F --> t{Can find any event?} t --> |No|B t --> |Yes|E{Transaction data is valid?} E --> |No| F E --> |Yes| H[Update open payment status to 'pending'] H --> R[Trying build trasction des] R --> G{Can build trasaction des ?} G --> |Yes| L[Update open payment status 'sending'] G --> |No| K[Log into mongo trasaction problem] K --> B L --> B","title":"Flowchart diagram"},{"location":"Services/Bridge/PaymentsService/open_payment/#code-logic","text":"CheckOpenPayment class inherit from BaseContract","title":"Code logic"},{"location":"Services/Bridge/PaymentsService/open_payment/#checking-open-payments","text":"this function get all open payments from database and check created dates def check ( self ): open_payment_list = self . get_open_payment_list () if len ( open_payment_list ) > 0 : for open_payment in open_payment_list : if open_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( days = 1 )): self . failed_transaction ( open_payment ) else : self . start_pay ( open_payment = open_payment ) self.get_open_payment_list() get all payments with \"open_payment\" status if open payment, open more than one day open payment update to fail If open payment open for 1 day it means user do not want to send transaction and reject the src transaction After that start checking payment def start_pay ( self , open_payment ): event_list = self . find_key_in_mongo ( open_payment . salt [ 2 :]) get the open payment and search in events from Mongo DB event crawler service payment salt : payment salt is a unique key generated when creating an open payment with payment nonce and random unique id payment key we generate salt to avoid duplicate payments and valid sending payments payment salt store in open payment on postgres db and contract events","title":"checking open payments"},{"location":"Services/Bridge/PaymentsService/pending_payment/","text":"Pending payment Service \u00b6 About \u00b6 This service is get pending payments from the database and start checking payment status Flowchart diagram \u00b6 graph TD A[Pending payment service] --> B(Get pending payment) B --> C{Pending payment create date < 6hours} C -->|No| D[Update pending payment status 'refund_open'] D --> B C -->|Yes| F{pending payment has transaction des hash address} F --> |No| G[Build transaction] F --> |Yes| K G --> I[Send transaction] I --> J[Update pending payment status 'sending'] J --> K[Check payment trasnaction hash form network] K --> L{Transaction status is done ?} L --> |Yes| M[update payment status to 'done'] L --> |No| N{Transaction is Failed} N --> |Yes| O[Update payment status 'refund_create'] N --> |No| B M --> B Code logic \u00b6 CheckPendingPayment class inherit from BaseContract check method \u00b6 Get all pending payments if pending more than 8 hours update pending payment to refund status and waiting for refund service and if it is pending mare than 300 seconds and the telegram bot is activ start sending message in telegram log channel def check ( self ): calculating_fee_initialize () pending_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_PENDING ) pending_pends_list : list = [] for pending_payment in pending_payment_list : if pending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( seconds = 300 ) ): pending_pends_list . append ( pending_payment . payment_key . __str__ ()) if pending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( hours = 8 ) ): self . refund_payment ( pending_payment ) else : self . start_pay ( pending_payment ) if len ( pending_pends_list ) > 0 : self . send_pending_telegram_message ( pending_pends_list ) start_pay method \u00b6 if created destination transaction and stored transaction hash first checking transaction hash and if the transaction is done in network update payment status to done and if failed in-network update payment status to failed and if the destination transaction is none try to create a destination transaction. def start_pay ( self , pending_payment ): contract_des = self . set_contract ( payment_transaction = pending_payment . transaction_des ) if contract_des is None : BaseLogger . log_error ( f \"payment_key: { pending_payment . payment_key } ,\" f \"can not set contract pending payment\" ) return BaseLogger . log_info ( f \"set contract success fully { pending_payment . payment_key } \" ) if pending_payment . transaction_des . transaction_hash is None : transaction_des = self . create_transaction_des ( contract_des = contract_des , transaction_data = pending_payment . transaction_des ) if transaction_des is not None : self . sending_payment ( pending_payment ) else : BaseLogger . log_error ( f \"payment_key: { pending_payment . payment_key } ,\" f \"can not set build transaction\" ) return transaction_result = contract_des . get_transaction ( pending_payment . transaction_des . transaction_hash ) gas_price = contract_des . get_gas_price ( pending_payment . transaction_des . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status self . done_payment ( payment = pending_payment , transaction_result = transaction_result , gas_price = gas_price ) if status == 0 : # fail transaction status self . refund_payment ( pending_payment )","title":"Pending payment Service"},{"location":"Services/Bridge/PaymentsService/pending_payment/#pending-payment-service","text":"","title":"Pending payment Service"},{"location":"Services/Bridge/PaymentsService/pending_payment/#about","text":"This service is get pending payments from the database and start checking payment status","title":"About"},{"location":"Services/Bridge/PaymentsService/pending_payment/#flowchart-diagram","text":"graph TD A[Pending payment service] --> B(Get pending payment) B --> C{Pending payment create date < 6hours} C -->|No| D[Update pending payment status 'refund_open'] D --> B C -->|Yes| F{pending payment has transaction des hash address} F --> |No| G[Build transaction] F --> |Yes| K G --> I[Send transaction] I --> J[Update pending payment status 'sending'] J --> K[Check payment trasnaction hash form network] K --> L{Transaction status is done ?} L --> |Yes| M[update payment status to 'done'] L --> |No| N{Transaction is Failed} N --> |Yes| O[Update payment status 'refund_create'] N --> |No| B M --> B","title":"Flowchart diagram"},{"location":"Services/Bridge/PaymentsService/pending_payment/#code-logic","text":"CheckPendingPayment class inherit from BaseContract","title":"Code logic"},{"location":"Services/Bridge/PaymentsService/pending_payment/#check-method","text":"Get all pending payments if pending more than 8 hours update pending payment to refund status and waiting for refund service and if it is pending mare than 300 seconds and the telegram bot is activ start sending message in telegram log channel def check ( self ): calculating_fee_initialize () pending_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_PENDING ) pending_pends_list : list = [] for pending_payment in pending_payment_list : if pending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( seconds = 300 ) ): pending_pends_list . append ( pending_payment . payment_key . __str__ ()) if pending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( hours = 8 ) ): self . refund_payment ( pending_payment ) else : self . start_pay ( pending_payment ) if len ( pending_pends_list ) > 0 : self . send_pending_telegram_message ( pending_pends_list )","title":"check method"},{"location":"Services/Bridge/PaymentsService/pending_payment/#start_pay-method","text":"if created destination transaction and stored transaction hash first checking transaction hash and if the transaction is done in network update payment status to done and if failed in-network update payment status to failed and if the destination transaction is none try to create a destination transaction. def start_pay ( self , pending_payment ): contract_des = self . set_contract ( payment_transaction = pending_payment . transaction_des ) if contract_des is None : BaseLogger . log_error ( f \"payment_key: { pending_payment . payment_key } ,\" f \"can not set contract pending payment\" ) return BaseLogger . log_info ( f \"set contract success fully { pending_payment . payment_key } \" ) if pending_payment . transaction_des . transaction_hash is None : transaction_des = self . create_transaction_des ( contract_des = contract_des , transaction_data = pending_payment . transaction_des ) if transaction_des is not None : self . sending_payment ( pending_payment ) else : BaseLogger . log_error ( f \"payment_key: { pending_payment . payment_key } ,\" f \"can not set build transaction\" ) return transaction_result = contract_des . get_transaction ( pending_payment . transaction_des . transaction_hash ) gas_price = contract_des . get_gas_price ( pending_payment . transaction_des . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status self . done_payment ( payment = pending_payment , transaction_result = transaction_result , gas_price = gas_price ) if status == 0 : # fail transaction status self . refund_payment ( pending_payment )","title":"start_pay method"},{"location":"Services/Bridge/PaymentsService/refund_payment/","text":"Refund payment service \u00b6 About \u00b6 When the destination transaction is failed , update payment status to refund and if the user send refunding request for pay back this service start paying back the user source transaction and pay back amount into source wallet address Refund open payment \u00b6 After user requested for pay back payment status updated to refund_create_payment and the service trying to pay back to user code logic \u00b6 check method \u00b6 Get all refund_create payments and if stay in this status more than 6 hours update payment status to operation for checking payment status and if is new payment trying build refund transaction and sending into network def check ( self ): \"\"\" start checking refunds payments service PAYMENT_TYPE_REFUND_CREATE and start refund payment flow :return: \"\"\" calculating_fee_initialize () refund_create_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_REFUND_CREATE ) for refund_create_payment in refund_create_payment_list : if refund_create_payment . modified_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( hours = 6 )): self . operation_payment ( refund_create_payment ) else : contract = self . set_contract ( payment_transaction = refund_create_payment . transaction_src ) transaction_result = contract . get_transaction ( refund_create_payment . transaction_src . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status refund_payment = self . create_transaction_refund ( refund_create_payment ) if refund_payment is not None : self . sending_payment ( refund_payment ) if status == 0 : # fail transaction status self . operation_payment ( refund_create_payment ) Refund sending \u00b6 Code logic \u00b6 If refund payment more than 6 hours update refund status tp operation status else try getting transaction status from the network and update payment status if transaction status is done payment def check ( self ): calculating_fee_initialize () refund_sending_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_REFUND_SENDING ) BaseLogger . log_info ( f \"refund sending payment list: { len ( refund_sending_payment_list ) } \" , ) for refund_sending_payment in refund_sending_payment_list : if refund_sending_payment . modified_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( hours = 6 )): self . operation_payment ( refund_sending_payment ) else : contract = self . set_contract ( payment_transaction = refund_sending_payment . transaction_src ) transaction_result = contract . get_transaction ( refund_sending_payment . transaction_src . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status self . done_payment ( refund_sending_payment , transaction_result ) if status == 0 : # fail transaction status self . operation_payment ( transaction_result )","title":"Refund payment service"},{"location":"Services/Bridge/PaymentsService/refund_payment/#refund-payment-service","text":"","title":"Refund payment service"},{"location":"Services/Bridge/PaymentsService/refund_payment/#about","text":"When the destination transaction is failed , update payment status to refund and if the user send refunding request for pay back this service start paying back the user source transaction and pay back amount into source wallet address","title":"About"},{"location":"Services/Bridge/PaymentsService/refund_payment/#refund-open-payment","text":"After user requested for pay back payment status updated to refund_create_payment and the service trying to pay back to user","title":"Refund open payment"},{"location":"Services/Bridge/PaymentsService/refund_payment/#code-logic","text":"","title":"code logic"},{"location":"Services/Bridge/PaymentsService/refund_payment/#check-method","text":"Get all refund_create payments and if stay in this status more than 6 hours update payment status to operation for checking payment status and if is new payment trying build refund transaction and sending into network def check ( self ): \"\"\" start checking refunds payments service PAYMENT_TYPE_REFUND_CREATE and start refund payment flow :return: \"\"\" calculating_fee_initialize () refund_create_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_REFUND_CREATE ) for refund_create_payment in refund_create_payment_list : if refund_create_payment . modified_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( hours = 6 )): self . operation_payment ( refund_create_payment ) else : contract = self . set_contract ( payment_transaction = refund_create_payment . transaction_src ) transaction_result = contract . get_transaction ( refund_create_payment . transaction_src . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status refund_payment = self . create_transaction_refund ( refund_create_payment ) if refund_payment is not None : self . sending_payment ( refund_payment ) if status == 0 : # fail transaction status self . operation_payment ( refund_create_payment )","title":"check method"},{"location":"Services/Bridge/PaymentsService/refund_payment/#refund-sending","text":"","title":"Refund sending"},{"location":"Services/Bridge/PaymentsService/refund_payment/#code-logic_1","text":"If refund payment more than 6 hours update refund status tp operation status else try getting transaction status from the network and update payment status if transaction status is done payment def check ( self ): calculating_fee_initialize () refund_sending_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_REFUND_SENDING ) BaseLogger . log_info ( f \"refund sending payment list: { len ( refund_sending_payment_list ) } \" , ) for refund_sending_payment in refund_sending_payment_list : if refund_sending_payment . modified_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( hours = 6 )): self . operation_payment ( refund_sending_payment ) else : contract = self . set_contract ( payment_transaction = refund_sending_payment . transaction_src ) transaction_result = contract . get_transaction ( refund_sending_payment . transaction_src . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status self . done_payment ( refund_sending_payment , transaction_result ) if status == 0 : # fail transaction status self . operation_payment ( transaction_result )","title":"Code logic"},{"location":"Services/Bridge/PaymentsService/sending_payment/","text":"Sending payment service \u00b6 About \u00b6 This service get sending payments status and trying get transaction status form network and update payment status Flowchart diagram \u00b6 graph TD A[Sending payment service] --> B(Get sending payment) B --> C{Sending payment create date < 1day} C -->|Yes| D[Update sending payment status 'operator'] D --> B C -->|No| K[Check payment trasnaction hash form network] K --> L{Transaction status is done ?} L --> |Yes| M[update payment status to 'done'] L --> |No| N{Transaction is Failed} N --> |Yes| O[Update payment status 'refund_create'] N --> |No| B M --> B Code logic \u00b6 CheckSendingPayment class inherit from BaseContract check method \u00b6 If sending payment to stay sending status more than one 1-day update status to operator status for operators are checking payment transaction status in the network or other issues else start to check destination transaction hash from the network and update payment status or waiting for a network valid response. If the destination transaction in the network is successful update payment to done status and get real estimated gas from transaction info and if destination transaction is failed update transaction status to refund for paying back the user payments price in the source transaction def check ( self ): calculating_fee_initialize () sending_pends_list : list = [] sending_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_SENDING ) BaseLogger . log_info ( f \"len sending payment { len ( sending_payment_list ) } \" , ) for sending_payment in sending_payment_list : if sending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( seconds = 300 ) ): sending_pends_list . append ( sending_payment . payment_key . __str__ ()) if sending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( days = 1 ) ): self . operation_payment ( sending_payment ) else : contract = self . set_contract ( payment_transaction = sending_payment . transaction_des ) transaction_result = contract . get_transaction ( sending_payment . transaction_des . transaction_hash ) gas_price = contract . get_gas_price ( sending_payment . transaction_des . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status self . done_payment ( payment = sending_payment , transaction_result = transaction_result , gas_price = gas_price ) if status == 0 : # fail transaction status self . refund_payment ( payment = sending_payment ) if len ( sending_pends_list ) > 0 : self . send_sending_telegram_message ( sending_pends_list )","title":"Sending payment service"},{"location":"Services/Bridge/PaymentsService/sending_payment/#sending-payment-service","text":"","title":"Sending payment service"},{"location":"Services/Bridge/PaymentsService/sending_payment/#about","text":"This service get sending payments status and trying get transaction status form network and update payment status","title":"About"},{"location":"Services/Bridge/PaymentsService/sending_payment/#flowchart-diagram","text":"graph TD A[Sending payment service] --> B(Get sending payment) B --> C{Sending payment create date < 1day} C -->|Yes| D[Update sending payment status 'operator'] D --> B C -->|No| K[Check payment trasnaction hash form network] K --> L{Transaction status is done ?} L --> |Yes| M[update payment status to 'done'] L --> |No| N{Transaction is Failed} N --> |Yes| O[Update payment status 'refund_create'] N --> |No| B M --> B","title":"Flowchart diagram"},{"location":"Services/Bridge/PaymentsService/sending_payment/#code-logic","text":"CheckSendingPayment class inherit from BaseContract","title":"Code logic"},{"location":"Services/Bridge/PaymentsService/sending_payment/#check-method","text":"If sending payment to stay sending status more than one 1-day update status to operator status for operators are checking payment transaction status in the network or other issues else start to check destination transaction hash from the network and update payment status or waiting for a network valid response. If the destination transaction in the network is successful update payment to done status and get real estimated gas from transaction info and if destination transaction is failed update transaction status to refund for paying back the user payments price in the source transaction def check ( self ): calculating_fee_initialize () sending_pends_list : list = [] sending_payment_list = self . payment_db . objects . filter ( status = PAYMENT_TYPE_SENDING ) BaseLogger . log_info ( f \"len sending payment { len ( sending_payment_list ) } \" , ) for sending_payment in sending_payment_list : if sending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( seconds = 300 ) ): sending_pends_list . append ( sending_payment . payment_key . __str__ ()) if sending_payment . created_time <= ( datetime . datetime . now ( datetime . timezone . utc ) - datetime . timedelta ( days = 1 ) ): self . operation_payment ( sending_payment ) else : contract = self . set_contract ( payment_transaction = sending_payment . transaction_des ) transaction_result = contract . get_transaction ( sending_payment . transaction_des . transaction_hash ) gas_price = contract . get_gas_price ( sending_payment . transaction_des . transaction_hash ) if transaction_result is not None : status = transaction_result . get ( \"status\" ) if status == 1 : # success transaction status self . done_payment ( payment = sending_payment , transaction_result = transaction_result , gas_price = gas_price ) if status == 0 : # fail transaction status self . refund_payment ( payment = sending_payment ) if len ( sending_pends_list ) > 0 : self . send_sending_telegram_message ( sending_pends_list )","title":"check method"},{"location":"Services/Buybacking/","text":"Buyback as a service \u00b6 This is a service to : Run Buyback as a standalone service Start Pause Kill See the status See the previous results Attention service does not have ability authentication. Please run it via vpn pritunl . How to start \u00b6 ABI","title":"Buyback as a service"},{"location":"Services/Buybacking/#buyback-as-a-service","text":"This is a service to : Run Buyback as a standalone service Start Pause Kill See the status See the previous results Attention service does not have ability authentication. Please run it via vpn pritunl .","title":"Buyback as a service"},{"location":"Services/Buybacking/#how-to-start","text":"ABI","title":"How to start"},{"location":"Services/Dex/Airdrop/","text":"Airdrop \u00b6 For distributing TCS tokens we used the same method uniswap used in their UNI token distribution. visit link Merkel Root \u00b6 A simple method to validate user data in contract! For more detailed explanation on this method visit this link . Implementation \u00b6 Steps: Create Me The idea behind this method was to calculate the merkle root of many hashed users data to check whether if user is verified to claim or not the requested amount. User's detail includes : User wallet addresses User airdrop amount User index (ordered by time they registered for initial airdrop) Which hash of following values is stored in Mongo Airdrop Database.","title":"Airdrop"},{"location":"Services/Dex/Airdrop/#airdrop","text":"For distributing TCS tokens we used the same method uniswap used in their UNI token distribution. visit link","title":"Airdrop"},{"location":"Services/Dex/Airdrop/#merkel-root","text":"A simple method to validate user data in contract! For more detailed explanation on this method visit this link .","title":"Merkel Root"},{"location":"Services/Dex/Airdrop/#implementation","text":"Steps: Create Me The idea behind this method was to calculate the merkle root of many hashed users data to check whether if user is verified to claim or not the requested amount. User's detail includes : User wallet addresses User airdrop amount User index (ordered by time they registered for initial airdrop) Which hash of following values is stored in Mongo Airdrop Database.","title":"Implementation"}]}